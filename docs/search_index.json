[["index.html", " Capítulo 1 Introdução 1.1 Importância da Bioestatística 1.2 Pílulas históricas da Estatística 1.3 História resumida do R 1.4 Sobre o autor", " Petrônio Fagundes de Oliveira Filho 2023-11-16 Capítulo 1 Introdução 1.1 Importância da Bioestatística Os indivíduos variam em relação as suas características biológicas, psicológicas e sociais na saúde e na doença. Esta variabilidade gera uma grande quantidade de incertezas. A Bioestatística, estatística aplicada às ciências biológicas e da saúde, é a ferramenta utilizada pelos pesquisadores para trabalhar com essas incertezas advindas da variabilidade. Várias definições foram escritas para a estatística, uma delas é a seguinte (1): Estatística é a disciplina interessada com o tratamento dos dados numéricos obtidos a partir de grupos de indivíduos A Bioestatística lida com a variabilidade humana utilizando técnicas estatísticas quantitativas (2) que ajudam a diminuir a ignorância em relação a esta diversidade. A compreensão da variabilidade humana torna a medicina mais ciência, diminuindo as incertezas, na tentativa de verificar se os resultados encontrados de fato existem ou são apenas obra do acaso. Na década de 1990, houve um acesso maior aos computadores. Os profissionais da saúde não estatísticos passaram a ter mais interesse no campo da bioestatística. Isto gerou uma onda que facilitou o aparecimento de novas ferramentas estatísticas de ponta. Apesar disso, o conhecimento da Bioestatística permanece restrito aos especialistas na área. Nos últimos anos, os pacotes de softwares foram aprimorados, tornando-se mais amigáveis e diminuindo significativamente o pânico ao se defrontar com uma série de números uma vez que a maioria deles exige apenas conhecimento básico de matemática. Para a tomada de decisão em saúde é fundamental o acúmulo de conhecimento adquirido através da prática clínica, geradora da experiência do profissional, do intercâmbio com os pares e da análise adequada das evidências científicas publicadas em periódicos de qualidade. Para atingir este objetivo, é fundamental o conhecimento de bioestatística, incluindo aqui que o pensamento que deve nortear os profissionais da saúde ao lidar com o ser humano é o pensamento probabilístico. 1.2 Pílulas históricas da Estatística A história deve começar em algum lugar, mas a história não tem começo (3) Entretanto, é natural, que se trace as raízes voltando ao passado, tanto quanto possível. Alguns referem-se à curiosidade em relação ao registro de dados à dinastia Shank, na China, possivelmente no século XIII a.c, com a realização de censos populacionais. Há relatos bíblicos de possíveis censos realizados por Moisés (1491 a.C.) e por Davi (1017 a.C.). Os romanos e os gregos já realizavam censos por volta do século VIII a IV a.C. Em 578-534 a.C., o imperador Servo Túlio mandou realizar um censo de população masculina adulta e suas propriedades que serviu para estabelecer o recrutamento para o exército, para o exercício dos direitos políticos e para o pagamento de impostos. Os romanos fizeram 72 censos entre 555 a.C. e 72 d.C. A punição para quem não respondia, geralmente era a morte! Na Idade Média, na Europa, existem registros de diversos censos: durante o domínio muçulmano, na Península Ibérica, nos séculos VII a XV; no reinado de Carlos Magno (712-814) e ainda o maior registro estatístico feito na época, o Domesday Book (Figura 1.1), realizado na Inglaterra, por Guilherme I (3) , o Conquistador, onde registravam nascimentos, mortes, batismos e casamentos. Houve, também, recenseamentos nas repúblicas italianas no século XII ao XIII (4). Figura1.1: Domesday Book John Graunt (24/04/1620 - 18/04/1674) foi um cientista britânico a quem se deve vários estudos demográficos ingleses. Foi o precursor da construção de Tábuas de Mortalidade. Realizou estudos com William Petty (1623 - 1687), economista britânico que propôs a aritmética política. Em 1791, Sir John Sinclair (1754 - 1835) concebeu um plano de uma pesquisa empírica na Escócia para fornecer informações estatísticas. Foi a primeira vez que o termo estatística foi usado em inglês. Girolamo Cardano (24/09/1501 - 21/09/1576) foi um médico, matemático, físico e filósofo italiano. É tido como o primeiro a introduzir ideias gerais da teoria das equações algébricas e as primeiras regras da probabilidade, descritas no livro Liber de Ludo Aleae, publicado em 1663. Descreveu pela primeira vez a clínica da febre tifoide. Foi amigo de Leonardo da Vinci. Pierre-Simon Laplace, Marquês Laplace (23/03/1749 - 05/03/1927) foi um matemático, astrônomo e físico francês. Embora conduzisse pesquisas substanciais sobre física, outro tema principal dos esforços de sua vida foi a teoria das probabilidades. Em seu Essai philosophique sur les probabilités, Laplace projetou um sistema matemático de raciocínio indutivo baseado em probabilidades, que hoje coincidem com as ideias bayesianas. Antoine Gombaud, conhecido como Chevalier de Méré (1607 - 1684) foi um nobre e jogador. Como não tinha mais sucesso nos jogos de azar, buscou ajuda de Blaise Pascal (19/06/1623 – 19/08/1662), matemático, físico francês, que se correspondeu com Pierre Fermat (matemático e cientista francês), nascendo desta colaboração a teoria matemática das probabilidades (1812). Blaise Pascal foi mais tarde chamado de o Pai da Teoria das Probabilidades. A moderna teoria das probabilidades foi atribuída a Abraham De Moivre (25/05/1667 – 27/11/1754), matemático francês, que adquiriu fama por seus estudos na trigonometria, teoria das probabilidades e pela equação da curva normal. Em 1742, Thomas Bayes (1701 – 07/04/1761, matemático e pastor presbiteriano, inglês, desenvolveu o Teorema de Bayes que descreve a probabilidade de um evento ocorrer, baseado em um conhecimento a priori. Adrien-Marie Legendre (18/09/1752 - 10/01/1833) foi um matemático francês. Em 1783, tornou-se membro adjunto da Academie des Sciences, instituição que esteve na vanguarda dos desenvolvimentos científicos dos séculos XVII e XVIII. Fez importantes contribuições à estatística, à teoria dos números e à álgebra abstrata. Johann Carl Friedrich Gauss (30/04/1777 - 23/02/1855) foi um matemático, astrônomo e físico alemão (Figura 1.2) que contribuiu em diversas áreas das ciências como teoria dos números, estatística, geometria diferencial, eletrostática, astronomia e ótica. Muitos referem-se a ele como o Príncipe da Matemática, o mais notável dos matemáticos. Descobriu o método dos mínimos quadrados e a lei de Gauss da distribuição normal de erros e sua curva em formato de sino, hoje tão familiar para todos que trabalham com estatística. Figura1.2: Johann Carl Friedrich Gauss Lambert Adolphe Jacques Quételet (22/02/1796 - 17/02/1874) foi um astrônomo, matemático, demógrafo e estatístico francês. Seu trabalho se concentrou em estatística social, criando regras de determinação de propensão ao crime Francis Galton (16/02/1822 – 17/01/1911) foi um antropólogo, matemático e estatístico inglês. Entre muitos artigos e livros, criou o conceito estatístico de correlação e da regressão à média. Ele foi o primeiro a aplicar métodos estatísticos para o estudo das diferenças e herança humanas de inteligência. Criou o conceito de eugenia e afirmava que era possível a melhoria da espécie por seleção artificial. Acreditava que a raça humana poderia ser melhorada caso fossem evitados relacionamentos indesejáveis. Isto acompanhava o pensamento burguês europeu da época. Criou a psicometria, onde desenvolveu testes de inteligência para selecionar homens e mulheres brilhantes. Esta teoria teve papel importante na formação do fascismo e nazismo (5). William Farr (30/11/1807 - 14/04/1883) foi um médico sanitarista e estatístico inglês, nascido na vila de Kenley, Shropshire. Foi o primeiro investigador a examinar séries temporais de morbimortalidade para longos períodos e, assim, considerado o criador da Estatística da Saúde Pública Moderna. Seus relatórios foram fundamentais para o desencadeamento das reformas sanitárias britânicas, em meados e final do século XIX (6). Florence Nightingale (12/05/1820 – 13/08/1910) foi uma enfermeira (Figura 1.3) que ficou famosa por ser pioneira no tratamento de feridos, durante a Guerra da Criméia (7). Ficou conhecida na história pelo apelido de “A dama da lâmpada”, pelo fato de servir-se de uma lamparina para auxiliar no cuidado aos feridos durante a noite. Também contribuiu no campo da Estatística, sendo pioneira na utilização de métodos de representação visual de informações, como por exemplo gráfico de setores (habitualmente conhecido como gráfico do tipo “pizza”) Figura1.3: Florence Nightingale John Snow (York, 15/03/1813 - Londres, 15/03/1858) foi um médico inglês (Figura 1.4), considerado pai da Epidemiologia Moderna. Recebeu, em 1853, o título de Sir após ter anestesiado a rainha Vitória no parto sem dor de seu oitavo filho, Leopoldo de Albany. Este fato ajudou a divulgar a técnica entre os médicos da época. Demonstrou que a cólera era causada pelo consumo de águas contaminadas com matérias fecais, ao comprovar que os casos dessa doença se agrupavam em determinados locais da cidade de Londres, em 1854, onde havia fontes dessas águas (6). Figura1.4: John Snow Karl Pearson (27/03/1857 - 27/04/1936) foi um importante estatístico inglês, fundador do Departamento de Estatística Aplicada da University College London em 1911. Juntamente com Weldon e Galton fundou, em 1901, a revista Biometrika com o objetivo era desenvolver as teorias estatísticas, editada até os dias de hoje. O trabalho de Pearson como estatístico fundamentou muitos métodos estatísticos de uso comum, nos dias atuais: regressão linear e o coeficiente de correlação, teste do qui-quadrado de Pearson, classificação das distribuições (8). Charles Edward Spearman (10/09/1863 - 17/09/1945) foi um psicólogo inglês conhecido pelo seu trabalho na área da estatística, como um pioneiro da análise fatorial e pelo coeficiente de correlação de postos de Spearman. Ele também fez bons trabalhos de modelos da inteligência humana. William Sealy Gosset (13/07/1876 - 16/10/1937) foi um químico e estatístico inglês (Figura 1.5). Em 1907, enquanto trabalhava químico da cervejaria experimental de Arthur Guinness &amp; Son, criou a distribuição t que usou para identificar a melhor variedade de cevada, trabalhando com pequenas amostras. A cervejaria Guinness tinha uma política que proibia que seus empregados publicassem suas descobertas em seu próprio nome. Ele, então, usou o pseudônimo “Student” e o teste é chamado “t de Student” em sua homenagem (9). Figura1.5: William Sealy Gosset Ronald Aylmer Fisher (17/02/1890 - 29/07/1962) foi um estatístico, biólogo e geneticista inglês. Em 1919, Fisher se envolveu com pesquisa agrícola no centro de experimentos de Rothamsted Research, em Harpenden, Inglaterra, e desenvolveu novas metodologias e teoria no ramo de experimentos (10). Durante sua vida, Fisher (Figura 1.6) escreveu 7 livros e publicou cerca de 400 artigos acadêmicos em estatística e genética . Em um dos seus livros, The design of Experiments (1935), Fisher relata um experimento que surgiu de uma pergunta curiosa: o gosto do chá muda de acordo com a ordem em que as ervas e o leite são colocados? Essa simples questão resultou em um estudo pioneiro na área e serviu de sustentação para análise da aleatorização de dados experimentais (9). Ronald A. Fisher foi descrito (11) como “um gênio que criou praticamente sozinho os fundamentos para o moderno pensamento estatístico”. Era muito temperamental. Seus atritos com outros estatísticos ficaram famosos, entre eles encontra-se ninguém menos do que Karl Pearson, outro notável estatístico. Figura1.6: Ronald A. Fisher Austin Bradford Hill (08/07/1897 - 18 /04/1991) foi um epidemiologista e estatístico inglês (Figura 1.7), pioneiro no estudo do acaso nos ensaios clínicos e, juntamente com Richard Doll, foi o primeiro a demonstrar a ligação entre o uso do cigarro e o câncer de pulmão. Hill é amplamente conhecido pelos Critérios de Hill, conjunto de critérios para a determinação de uma associação causal (12). Figura1.7: Bradford Hill John Wilder Tukey (16/06/1915 - 26/07/2000) foi um estatístico norte-americano. Desenvolveu uma filosofia para a análise de dados que mudou a maneira de pensar dos estatísticos, sugerindo que se faça uma visualização dos dados, interpretando o formato, centro, dispersão, presença de valores atípicos, sumarizar numericamente e por fim escolher um modelo matemático. Foi o criador do boxplot e introduziu a palavra “bit” como uma contração do termo binary digit. Douglas G. Altman (12 /07/1948 - 03/06/2018) foi um estatístico inglês (Figura 1.8), conhecido por seu trabalho em melhorar a confiabilidade dos artigos de pesquisa médica (13) e por artigos altamente citados sobre metodologia estatística. Ele foi professor de estatística em medicina na Universidade de Oxford. Há praticamente 30 anos, Altman (14) escreveu um artigo sobre problema da qualidade da pesquisa em medicina que causou um grande impacto e permanece válido até hoje. Nesta publicação ele afirma: A má qualidade de muitas pesquisas médicas é amplamente reconhecida, mas, de forma perturbadora, os líderes da profissão médica parecem apenas minimamente preocupados com o problema e não fazem nenhum esforço aparente para encontrar uma solução. Figura1.8: Douglas G. Altman 1.3 História resumida do R O R é uma linguagem e um ambiente de desenvolvimento voltado fundamentalmente para a computação estatística. Foi inspirado em duas linguagens: S (John Chambers, do Bell Labs) que forneceu a sintaxe e Scheme (Hal Abelson e Gerald Sussman) implementou e forneceu a semântica. O nome R provém em parte das iniciais dos criadores, George Ross Ihaka e Robert Gentleman (Figura 1.9), e também de um jogo figurado com a linguagem S. Em 29 de Fevereiro de 2000, o software foi considerado com funcionalidades e estável o suficiente para a versão 1.0. O R é um projeto GNU 1. Software Livre significa que os usuários têm liberdade para executar, copiar, distribuir, estudar, alterar e melhorar o software. Foi desenvolvido em um esforço colaborativo de pessoas em vários locais do mundo (15). O projeto R fornece uma grande variedade de técnicas estatísticas e gráficas. É uma linguagem e um ambiente similar ao S. A linguagem do S que também é uma linguagem de computador voltada para cálculos estatísticos. Um dos pontos fortes de R é a facilidade com que produções gráficas de qualidade podem ser produzidas. O R é também altamente expansível com o uso dos pacotes, que são bibliotecas para sub-rotinas específicas ou áreas de estudo específicas. Um conjunto de pacotes é incluído com a instalação de R e muito outros estão disponíveis na rede de distribuição do R - Comprehensive R Archive Network (CRAN) (16). Figura1.9: Robert Gentlemen (E) e George Ross (D) A linguagem R é largamente usada entre estatísticos e analistas de dados para desenvolver softwares de estatística e análise de dados. Pesquisas e levantamentos com profissionais da área da saúde mostram que a popularidade do R aumentou substancialmente nos últimos anos (17). 1.4 Sobre o autor Petrônio Fagundes de Oliveira Filho nasceu em 04/10/1947, em Porto Alegre, Rio Grande do Sul, Brasil. Estudou no Ensino Médio do Colégio do Rosário, em Porto Alegre. Possui graduação em Medicina pela Universidade de Caxias do Sul (UCS), em 1973, residência em Pediatria no Hospital da Criança Conceição, Porto Alegre (1975) e mestrado em Saúde Pública Materno Infantil, Universidade de São Paulo (1998). Em 1980, obteve o Título de Especialista em Pediatria (TEP) e, em 2009, o título de especialista em Estatística Aplicada (UCS). Atuou como Pediatra no INAMPS até 2002 e em consultório privado até hoje. Aposentou-se como professor da Universidade de Caxias do Sul (UCS), em 2019, onde atuou desde 1975, nas áreas de Pediatria, Epidemiologia e Bioestatística, foi coordenador do Serviço e da Residência Médica em Pediatria, chefe de Departamento, coordenador do curso de Medicina e diretor de Ensino do Hospital Geral de Caxias do Sul (Hospital de Ensino da Universidade de Caxias do Sul) e membro de Conselho de Ética em Pesquisa da Universidade de Caxias do Sul, ligado ao CONEP (Conselho Nacional de pesquisa). Durante mais de 20 anos fez parte do Núcleo de Consultoria e Epidemiologia do Centro de Ciência da Saúde (UCS). É autor de dois livros: Epidemiologia e Bioestatística: Fundamentos para a leitura crítica (Editora Rubio, 2015 e 2018) e SPSS - Análise de Dados Biomédicos, em coautoria com Valter Motta (MedBook, 2009). Além disso, participou de dezenas publicações e de capítulos de outros livros. Desde 1976, é casado com Lena Maria Cantergiani Fagundes de Oliveira. Tem duas filhas, Nathalia e Andressa, e dois lindos netos, Gabriel e Felix. Ah, tem um cão shitzu branco, marrom claro e com algumas manchas pretas, Floquinho, que ao acompanhar seus estudos e análises estatísticas, late toda vez que ele menciona o nome de Ronald Fisher. email: petronioliveira@gmail.com WhatsApp: (54)999715499 Esta sigla está associada ao animal gnu africano, símbolo de software de distribuição livre, quer dizer is Not Unix, sigla recursiva muito comum entre nerds!↩︎ "],["natureza-dos-dados.html", "Capítulo 2 Natureza dos Dados 2.1 Variáveis e Dados 2.2 População e Amostra 2.3 Estatística e Parâmetro 2.4 Escalas de medição 2.5 Tipos de Variáveis", " Capítulo 2 Natureza dos Dados 2.1 Variáveis e Dados As pesquisas manuseiam dados referentes às variáveis que estão sendo estudadas. Variável é toda característica ou condição de interesse que pode de ser mensurada ou observada em cada elemento de uma amostra ou população. Como o próprio nome diz, seus valores são passíveis variar de um indivíduo a outro ou no mesmo indivíduo. Em contraste com a variável, o valor de uma constante é fixo. As variáveis podem ter valores numéricos ou não numéricos. O resultado da mensuração ou observação de uma variável é denominado dado. A Tabela 2.1 mostra um conjunto de variáveis e suas medidas (dados) de um grupo de pacientes internados em uma determinada UTI. O termo medida deve ser entendido num sentido amplo, pois não é possível “medir” o sexo (observação) ou o estado geral (critérios) de alguém, ao contrário do peso e da pressão arterial que podem ser mensurados com instrumentos. Tabela2.1: Variáveis e dados. Id Nome Idade Sexo PAS PAD Estado Geral 1 45 João masculino 140 90 bom 2 32 Maria feminino 110 70 regular 3 27 Pedro masculino 120 80 grave 4 18 Teresa feminino 100 60 bom 2.2 População e Amostra Na pesquisa em saúde, a não ser quando se realiza um censo, coleta-se dados de um subconjunto de indivíduos denominado de amostra, pertencente a um grupo maior, conhecido como população. A população de interesse é, geralmente, chamada de população-alvo. A amostra para ser representativa da população deve ter as mesmas características desta. A partir dos dados encontrados na amostra, presume-se o resultado é condizente com a população. Este processo é denominado de inferência estatística. O interesse na amostra não está propriamente nela, mas na informação que ela fornece ao investigador sobre a população de onde ela provém. A amostra fornece estimativas (estatísticas) da população (Figura 2.1). População ou população-alvo consiste em todos os elementos (indivíduos, itens, objetos) cujas características estão sendo estudadas. Amostra é a parte, subconjunto, da população selecionada para estudo. Em decorrência do acaso, diferentes amostras de uma mesma população fornecem resultados diferentes. Este fato deve ser levado em consideração ao usar uma amostra para fazer inferência sobre uma população. Este fenômeno é denominado de variação amostral ou erro amostral2 e é a essência da estatística. O grau de certeza na inferência estatística depende da representatividade da amostra. O processo de obtenção da amostra é chamado de amostragem. Mesmo que este processo seja adequado, a amostra nunca será uma cópia perfeita da população de onde ela foi extraída. Desta forma, em qualquer conclusão baseada em dados de uma amostra, sempre haverá o que é conhecido como erro amostral. Este erro deve ser tratado estatisticamente tendo em mente a teoria da amostragem, baseada em probabilidades. Figura2.1: População, amostra e inferência estatística 2.3 Estatística e Parâmetro Estatística é uma característica que resume os dados de uma amostra e o parâmetro é uma característica estabelecida para toda a população. Os valores dos parâmetros são normalmente desconhecidos, porque é inviável medir uma população inteira. A estatística é um valor aproximado, uma estimativa, do parâmetro. As estatísticas são representadas por letras romanas3 e os parâmetros por letras gregas. Por exemplo, a media da população é representada por \\(\\mu\\) e a média da amostra por \\(\\bar{x}\\); o desvio padrão da população é denotado \\(\\sigma\\) e o desvio padrão da amostra por s. Na maioria dos estudos, são utilizadas amostras que fornecem estimativas que, para serem representativas da população, devem ser probabilísticas. Ou seja, a amostra deve ser recrutada de forma aleatória, permitindo que cada um dos membros da população tenha a mesma probabilidade de ser incluído na amostra. Além disso, uma amostra deve ter um tamanho adequado para permitir inferências válidas. 2.4 Escalas de medição Em um estudo científico, há necessidade de registrar os dados para que eles representem acuradamente as variáveis observadas. Este registro de valores necessita de escalas de medição. Mensuração ou medição é o processo de atribuir números ou rótulos a objetos, pessoas, estados ou eventos de acordo com regras específicas para representar quantidades ou qualidades dos dados. Para a mensuração das variáveis são usadas as escalas nominal, ordinal, intervalar e de razão (18). 2.4.1 Escala Nominal As escalas nominais são meramente classificativas, permitindo descrever as variáveis ou designar os sujeitos, sem recurso à quantificação. É o nível mais elementar de representação. São usados nomes, números ou outros símbolos para designar a variável. Os números, quando usados, representam códigos e como tal não permitem operações matemáticas. As variáveis nominais não podem ser ordenadas. Podem apenas ser comparadas utilizando as relações de igualdade ou de diferença, através de contagens. Os números atribuídos às variáveis servem como identificação, ou para associá-la a uma dada categoria. As categorias de uma escala nominal são exaustivas e mutuamente exclusivas. Quando existem duas categorias, a variável é dita dicotômica e com três ou mais categorias, politômicas. Os nomes e símbolos que designam as categorias podem ser intercambiáveis sem alterar a informação essencial. Exemplos: Tipos sanguíneos: A, B, AB, O; variáveis dicotômicas: morto/vivo, homem/mulher, sim/não; cor dos olhos, etc. 2.4.2 Escala Ordinal As variáveis são medidas em uma escala ordinal quando ocorre uma ordem, crescente ou decrescente, inerente entre as categorias, estabelecida sob determinado critério. A diferença entre as categorias não é necessariamente igual e nem sempre mensuráveis. Geralmente, designam-se os valores de uma escala ordinal em termos de numerais ou postos (ranks), sendo estes apenas modos diferentes de expressar o mesmo tipo de dados. Também não faz sentido realizar operações matemática com variáveis ordinais. Pode-se continuar a usar contagem. Exemplos: classe social (baixa, média, alta); estado geral do paciente: bom, regular, mau; estágios do câncer: 0, 1, 2, 3 e 4; escore de Apgar: 0, 1, 2… 10. 2.4.3 Escala Intervalar Uma escala intervalar contém todas as características das escalas ordinais com a diferença de que se conhece as distâncias entre quaisquer números. Em outras palavras, existe um espectro ordenado com intervalos quantificáveis. Este tipo de escala permite que se verifique a ordem e a diferença entre as variáveis, porém não tem um zero verdadeiro, o zero é arbitrário. O exemplo clássico é a mensuração da temperatura, usando as escalas de: Celsius ou Fahrenheit. Aqui é legítimo ordenar, fazer soma ou médias. No entanto, 0ºC não significa ausência de temperatura, portanto a operação divisão não é possível. Uma temperatura de 40ºC não é o dobro de 20ºC. Se 40ºC e 20ºC forem transformados para a escala Fahrenheit, passarão, respectivamente, para 104ºF e 68ºF e, sem dúvida, 104 não é o dobro de 68! 2.4.4 Escala de Razão Há um espectro ordenado com intervalos quantificáveis como na escala intervalar. Entretanto, as medidas iniciam a partir de um zero verdadeiro e a escala tem intervalos iguais, permitindo as comparações de magnitude entre os valores. Refletem a quantidade real de uma variável, permitindo qualquer operação matemática. Os dados tanto na escala intervalar como na de razão, podem ser contínuos ou discretos. Dados contínuos necessitam de instrumentos para a sua mensuração e assumem qualquer valor em um certo intervalo. Por exemplo, o tempo para terminar qualquer tarefa pode assumir qualquer valor, 10 min, 20 min, 35 min, etc., de acordo com o tipo de tarefa. Outros exemplos: peso, dosagem de colesterol, glicemia. Dados discretos possuem valores iguais a números inteiros, não existindo valores intermediários. A mensuração é feita através da contagem. Por exemplo: número de filhos, número de fraturas, número de pessoas. 2.5 Tipos de Variáveis A primeira etapa na descrição e análise dos dados é classificar as variáveis, pois a apresentação dos dados e os métodos estatísticos variam de acordo com os seus tipos. As variáveis, primariamente, podem ser divididas em dois tipos: numéricas ou quantitativas e categóricas ou qualitativas (19). 2.5.1 Variáveis Numéricas As variáveis numéricas são classificadas em dois tipos de acordo com a escala de mensuração: continuas e discretas. As variáveis contínuas são aquelas cujos dados foram mensurados em uma escala intervalar ou de razão, podendo assumir, como visto, qualquer valor dentro de um intervalo de números reais, dependendo da precisão do instrumento de medição. O tratamento estatístico tanto para variável intervalar como de a razão é o mesmo. A diferença entre elas está na presença do zero absoluto. As variáveis numéricas contínuas têm unidade de medida. Por exemplo, um menino de 4 anos tem 104 cm. Uma variável numérica é considerada discreta quando é apenas possível quantificar os resultados possíveis através do processo de contagem. Também têm unidade de medida – número de elementos. Por exemplo, o número de fraturas, o número de acidentes, etc. 2.5.2 Variáveis Categóricas As variáveis categóricas ou qualitativas são de dois tipos: nominal e ordinal, de acordo com a escala de mensuração. Um tipo particularmente comum é uma variável binária (ou variável dicotômica), que tem apenas dois valores possíveis. Por exemplo, o sexo é masculino ou feminino. Este tipo de variável é bastante utilizado na área da saúde, em Epidemiologia. As variáveis nominais não têm quaisquer unidades de medida e a nominação das categorias é completamente arbitrária e pertencer a uma categoria não significa ter maior importância do que pertencer à outra. Uma variável ordinal tem uma ordem inerente ou hierarquia entre as categorias. Do mesmo modo que as variáveis nominais, as variáveis ordinais não têm unidades de medida. Entretanto, a ordenação das categorias não é arbitrária. Assim, é possível ordená-las de modo lógico. Um exemplo comum de uma variável categórica ordinal é a classe social, que tem um ordenamento natural da maioria dos mais desfavorecidos para os mais ricos. As escalas, como a escore de Apgar e a escala de coma de Glasgow (20), também são variáveis ordinais. Mesmo que pareçam numéricas, elas apenas mostram uma ordem no estado dos pacientes. O escore de Apgar (21) é uma escala, desenvolvida para a avaliação clínica do recém-nascido imediatamente após o nascimento. Originalmente, a escala foi usada para avaliar a adaptação imediata do recém-nascido à vida extrauterina. A pontuação pode variar de zero a 10. Uma pontuação igual ou maior do que oito, indica um recém-nascido normal. Uma pontuação de sete ou menos pode significar depressão do sistema nervoso e abaixo de quatro, depressão grave. As variáveis ordinais, da mesma forma que as nominais, não são números reais e não convém aplicar as regras da aritmética básica para estes tipos de dados. Este fato gera uma limitação na análise dos dados. 2.5.3 Como identificar o tipo da variável? A maneira mais fácil de dizer se os dados são numéricos é verificar se eles têm unidades ligadas a eles, tais como: g, mm, ºC, ml, número de úlceras de pressão, número de mortes e assim por diante. Se não, podem ser ordinais ou nominais – ordinais se os valores podem ser colocados em ordem. A Figura @ref{fig:caminho} é uma ajuda para o reconhecimento do tipo de variável (22). Figura2.2: Caminho para identificar o tipo de variável 2.5.4 Variáveis Dependentes e Independentes De um modo geral as pesquisas são realizadas para testar as hipóteses dos pesquisadores e, para isso, eles medem variáveis com a finalidade de compará-las. A maioria das hipóteses podem ser expressas por duas variáveis: uma variável explicativa ou preditora e uma variável desfecho (19). A variável preditora ou explanatória é a que se acredita ser a causa e também é conhecida como variável independente, porque o seu valor não depende de outras variáveis. Em Epidemiologia, é com frequência referida como exposição ou fator de risco. A variável desfecho é aquela que é o efeito, consequência ou resultado da ação de outra variável, por isso, também chamada de variável dependente. Em um estudo que tenta verificar se o tabagismo, durante a gestação, pode interferir no peso do recém-nascido, tem o fumo (variável categórica) como variável preditora (exposição ou fator de risco) e o peso do recém-nascido (variável numérica contínua) como variável desfecho Consulte também o capítulo 9↩︎ Também podem ser representadas pela letra grega correspondente ao respectivo parâmetro com um acento circunflexo, por exemplo, a média amostral é \\(\\hat{\\mu}\\), dita (mü chapéu).↩︎ "],["produção-dos-dados.html", "Capítulo 3 Produção dos Dados 3.1 Processo de Pesquisa 3.2 Processo de Amostragem 3.3 Principais Delineamentos de Pesquisa 3.4 Estudos Observacionais 3.5 Ensaios Clínicos", " Capítulo 3 Produção dos Dados 3.1 Processo de Pesquisa A pesquisa é um processo de construção do conhecimento. O objetivo deste processo é gerar um novo conhecimento e/ou confirmar ou refutar algum conhecimento prévio. A pesquisa é um processo de aprendizagem tanto do pesquisador quanto da sociedade que se beneficiará deste novo conhecimento. Para ser chamada de científica, a pesquisa deve obedecer aos princípios consagrados pela ciência (23). A pesquisa nasce de uma dúvida do pesquisador, de algum questionamento que ele considerou interessante sobre o mundo, ou seja, de algo que se costuma chamar de pergunta ou questão da pesquisa. Existem vários motivos que geram questões de pesquisa: Avaliação crítica de pesquisas realizadas por outros pesquisadores. Condução de uma pesquisa primária com a finalidade de responder uma questão (ou questões), gerando um novo conhecimento ou ampliação do conhecimento existente. Para obter habilidades de pesquisa ou experiência, com frequência como parte de um programa educacional. Testar a viabilidade de um projeto ou técnica de pesquisa. 3.1.1 Questão de Pesquisa A pesquisa visa estabelecer novos conhecimentos em torno de um tema específico. O tema de pesquisa pode surgir do próprio interesse ou experiência do pesquisador, ou partir da encomenda de alguma instituição financiadora. Algumas vezes, a pesquisa se origina de outros estudos realizados pelo próprio pesquisador ou outros pesquisadores. À medida que a ideia da pesquisa cresce, o pesquisador estabelece uma pergunta de pesquisa específica ou um conjunto de questões que ele deseja responder. Algumas vezes, o tema da pesquisa é tão amplo que o pesquisador tem que ter cuidado para não se perder do seu objetivo. Este objetivo é que vai guiá-lo no estabelecimento da pergunta ou perguntas a serem respondidas no estudo. Estes questionamentos são conhecidos como questão de pesquisa ou pergunta de partida. O foco da questão de pesquisa pode ser na descrição de um fenômeno clínico. Neste caso a pergunta é dita descritiva, por exemplo, pesquisa de prevalência de uma enfermidade, proporção de utilização de um serviço de saúde, características de um teste, etc. Quando a pergunta busca a explicação para um fenômeno, ela é dita analítica, por exemplo, comparação entre dois fenômenos. Em geral, perguntas analíticas são mais interessantes. Entretanto, as perguntas descritivas são fundamentais no início de um estudo analítico. Uma boa pergunta de pesquisa deve ter as seguintes características (24): Factível: o pesquisador deve conhecer desde o início os limites e problemas práticos que podem interferir na pesquisa. A viabilidade está relacionada com o tamanho amostral, com o domínio técnico adequado, com o tempo e custos envolvidos e com um foco dirigido estritamente aos objetivos mais importantes. Interessante: a questão de pesquisa deve despertar o interesse não apenas do pesquisador, mas também de seus pares e agentes financiadores. Nova: a pesquisa deve ser inovadora, original, em algum sentido, para que o estudo seja uma contribuição ao conhecimento ou amplie um conhecimento existente; Ética: se o estudo impõe riscos físicos ou invasão de privacidade ou não traz nenhuma informação nova, o pesquisador deve suspendê-lo. É importante discutir previamente com pesquisadores mais experientes ou com algum representante do Comitê de Ética em Pesquisa da instituição. Relevante: nenhuma das características da questão de pesquisa é mais importante do que a sua relevância. Para isto basta pensar nos benefícios que os resultados da pesquisa trarão à Medicina atual. Ou seja, antes de dedicar tempo e esforço para escrever um projeto de pesquisa deve-se avaliar se a questão de pesquisa é FINER (Factível, Interessante, Nova, Ética e Relevante). 3.1.2 Hipótese de Pesquisa Uma vez estabelecida a(s) pergunta(s) de pesquisa adequada(s), os pesquisadores formulam hipóteses para serem testadas. Enquanto a pergunta de pesquisa possa ser um pouco vaga em sua natureza como: “existe uma relação entre o tipo psicológico e a capacidade de parar de usar drogas?” Uma hipótese de pesquisa, necessita ser precisa. Há necessidade de especificar qual o tipo psicológico está relacionado à habilidade de parar de usar drogas. A precisão da hipótese é fundamental em um projeto de pesquisa, pois ela determinará o delineamento de pesquisa a ser seguido pelo pesquisador e as técnicas estatísticas apropriadas para a análise dos dados. A fonte e o tipo de dados são determinados pela característica do delineamento recomendado pela hipótese de pesquisa. O objetivo da pesquisa, usando o método científico, é refutar ou não as hipóteses de pesquisa. Se a hipótese do pesquisador não for rejeitada, houve a geração de um novo conhecimento. 3.2 Processo de Amostragem Após o estabelecimento das hipóteses a serem testadas, há necessidade de coletar os dados. Uma vez que é praticamente impossível analisar toda a população que constitui a população-alvo, extrai-se uma amostra desta população. Este processo é denominado de amostragem (25). Uma amostra deve ser representativa da população, ou seja, deve ter características semelhantes às da população e ser fidedigna. A fidedignidade está relacionada à precisão dos dados que sofrem influência dos instrumentos de aferição, questionários não validados e falhas humanas. Uma amostra inadequada ameaça a validade da pesquisa. Os dados coletados de maneira não aleatória são chamados de evidência anedótica. O nível de confiança nos resultados de uma pesquisa está diretamente relacionado à qualidade da amostra. A amostra deve ser representativa. Uma amostra deve conter apenas dados úteis que permitam a resposta da pergunta de pesquisa, evitando desperdício e fuga dos objetivos traçados. A aleatoriedade provoca uma diferença entre o resultado da amostra e o verdadeiro valor da população que é denominada erro amostral. Não importa quão bem a amostra seja coletada, os erros amostrais irão sempre ocorrer. Entretanto, não existe técnica estatística que salve amostras coletadas incorretamente, tendenciosas! 3.2.1 Amostras probabilística Para evitar vieses, erros sistemáticos, que favorecem determinados desfechos, o ideal é coletar uma amostra probabilística. A amostra probabilística adota o princípio da equiprobabilidade, isto é, “todos os sujeitos da população têm a mesma probabilidade de fazerem parte da amostra”. Esta probabilidade é conhecida e diferente de zero. As amostras probabilísticas têm o potencial de ser possível a generalização para a população; ser imparcial e com menor erro amostral. Amostra aleatória simples: é a mais utilizada pois garante representatividade da amostra junto à população. A amostra aleatória simples não emprega nenhum critério particular para a definição da amostra. O mecanismo mais comum de obter este tipo de amostra é por um simples sorteio, em geral, usando programas de computador. Amostra aleatória estratificada: quando a população é constituída por subpopulações ou estratos e é razoável supor que a variável de interesse apresenta comportamento diferente nos diferentes estratos, pode-se usar este tipo de amostragem. Neste caso, a amostra deve ter a mesma estratificação da população para ser representativa. Um exemplo comum de estratificação é o nível socioeconômico. A partir do momento que os estratos estão definidos se procede uma amostra aleatória simples de cada estrato. Amostra aleatória sistemática: as unidades amostrais são selecionadas a partir de um esquema rígido preestabelecido de sistematização que tem o propósito de abranger toda a população-alvo. Para isso, ordena-se os indivíduos da população (por exemplo, um grande arquivo com 20000 fichas) e calcula-se uma constante conveniente, \\(c = N/n\\), onde \\(N\\) é tamanho da população e \\(n\\) é o tamanho da amostra. Se \\(n = 500\\), a constante será \\(40\\), ou seja, será selecionado aleatoriamente o primeiro membro da amostra (\\(k\\)), de maneira que \\(k\\) seja menor do que a constante e maior do que \\(1\\). A partir daí os sucessivos membros serão: \\(k + c\\); \\(k + 2c\\); \\(k + 3c\\); … até atingir \\(n\\). Amostra aleatória por conglomerados (clusters): este tipo de amostra é utilizada quando dentro da população são identificados agrupamentos (clusters) naturais, por exemplo, espaços, vilas, etc. Neste tipo de amostragem o elemento focal não é o sujeito, mas o cluster. Identificados estes, sorteiam-se os conglomerados e se analisa todos os indivíduos dos conglomerados sorteados. 3.2.2 Amostras não probabilísticas Na amostragem não aleatória ou intencionada há uma escolha deliberada da amostra, subordinada a objetivos específicos do pesquisador. Não há garantia de representatividade da população. É importante averiguar, neste tipo de amostragem, a presença de conflitos de interesse. Amostra de conveniência: é uma técnica comum onde é selecionada uma mostra que esteja acessível. Em outras palavras, os indivíduos são recrutados porque eles estão prontamente disponíveis. Neste tipo de amostra há incapacidade de fazer afirmações gerais com rigor estatístico sobre a população. Amostra por cotas: é uma versão não probabilística da amostra estratificada. Tem três etapas: Segmentação, onde se divide em grupos, por exemplo, sexo, classe social, região, etc.; Definição do tamanho das cotas; Seleção por meio de amostras de conveniência. Amostra de resposta voluntária: o pesquisador solicita aos membros de uma população-alvo para que eles participem da amostra e as pessoas decidem se entram ou não. Esses tipos de amostras são enviesados porque as pessoas podem ter interesses particulares ou opiniões negativas e tendem a querer participar. 3.2.3 Tamanho amostral A determinação do tamanho de uma amostra é de suma importância, pois amostras desnecessariamente grandes acarretam desperdício de tempo e de dinheiro e amostras muito pequenas podem levar a resultados não confiáveis, ameaçando a validade da pesquisa. Não existe um número estabelecido para o tamanho da amostra. Há uma solução para cada caso. O tamanho da amostra depende (26): do tipo de problema; do tipo de variável; da magnitude do erro estatístico aceito pelo pesquisador; da diferença minimamente importante entre os grupos; da probabilidade de que a amostra identifique uma diferença verdadeira: Poder estatístico; do tempo, dinheiro e pessoal disponível, bem como da dificuldade em se obterem dados e da complexidade da pesquisa. O tamanho amostral mínimo é determinado por fórmulas estatísticas complexas. Os cálculos são muito pesados, mas agora, felizmente, existem programas de computador disponíveis que realizam este trabalho, por exemplo o G-Power3 (27). Além disso, é possível acessar um site que fornece informações e ferramentas para o cálculo amostral em pesquisas da área da saúde 4. Existem tabelas extensas para calcular o número de participantes (28) para um determinado nível de poder (e vice-versa). 3.3 Principais Delineamentos de Pesquisa Em geral, a pesquisa clínica, é dividida em dois tipos de investigação. O primeiro é aquele em que o observador apenas observa o doente, as características da sua doença e sua evolução, sem atuar de modo a modificar qualquer aspecto que esteja estudando. Trata-se de estudo observacional. O segundo corresponde aos estudos experimentais, onde o pesquisador não se limita a observar, mas promove uma intervenção com o objetivo de conhecer os efeitos dessa sobre os participantes da pesquisa. A intervenção pode ser a prescrição de um medicamento, uma dieta, atividade física ou repouso, ou simplesmente, o estabelecimento de um programa de atenção à saúde. Os estudos podem ser também classificados em primários ou secundários ou integrativos (29). Estudos primários correspondem a pesquisas originais que constituem a maioria das publicações encontradas nas revistas médicas. Estudos secundários são aqueles que procuram sumarizar e extrair conclusões de estudos primários Estudos Primários Estudos Observacionais Relato de Caso e Série de Casos Estudo Transversal Estudo Caso-controle Estudo de Coorte Estudos Experimentais Experimento laboratorial Ensaio Clínico Estudos Secundários Revisões não sistemáticas Revisões Sistemáticas Direrizes (Guidelines) Análise de decisão Análise Econômica 3.3.1 Elementos básicos de um delineamento de pesquisa Os estudos contêm três elementos básicos: Variáveis componentes: Nas investigações das relações entre as variáveis identificam-se pelo menos duas variáveis nos estudos epidemiológicos. Desfecho: Aquilo que vai acontecer durante uma investigação na mensuração da condição de saúde-doença. Sinônimo: variável dependente. Exposição: O fator que precede o desfecho. Sinônimos: fator em estudo, variável preditora, variável independente. Temporalidade: Quanto ao tempo os estudos podem ser contemporâneos, retrospectivos e prospectivos, de acordo como os dados são obtidos em relação ao momento atual. Enfoque: Um estudo pode ter vários enfoques. Na maioria deles, na área médica, eles relacionam-se à prevenção, ao diagnóstico, à terapêutica e ao prognóstico. 3.4 Estudos Observacionais 3.4.1 Relato de Caso ou Série de casos No relato de caso, descrevem-se casos raros, eventos não comuns ou inesperados, doenças desconhecidas ou raras. Um evento notável deve ser identificado. Um relato de caso tem a descrição de até dez casos. Acima deste número tem-se uma série de casos (30). Metodologicamente, faz-se um relato descritivo simples de características interessantes observadas em um paciente ou grupo de pacientes. Os indivíduos são acompanhados em um espaço de tempo curto e não possuem participantes-controles. A coleta dos dados é, na maioria das vezes, retrospectiva. Uma série de casos não é planejada e não envolve quaisquer hipóteses investigativas. Pode ser empregada como precursor de outros estudos. 3.4.2 Estudos Transversais ou Seccionais Os estudos transversais são também conhecidos como estudos seccionais. Este tipo de estudo fornece a informação sobre a prevalência, ou seja, a proporção dos indivíduos que tem a doença ou condição clínica em um determinado momento. Por este motivo são também conhecidos como estudos de prevalência (31). Observam dados coletados em um grupo de indivíduos em um único momento, sem um período de seguimento. O desfecho e exposição são avaliados no mesmo momento no tempo. Os dados são coletados apenas uma vez para cada indivíduo, podendo ser em dias diferentes em diferentes sujeitos. As informações são, em geral, obtidas em um curto espaço de tempo. É um estudo estático, representa a “fotografia” de um momento. Entretanto, se as variáveis preditora e de desfecho são definidas apenas com base nas hipóteses causa-efeito do investigador e não no delineamento do estudo, é possível também examinar associações. Os estudos de corte transversal, de um modo geral, são desenhados para determinar “O que está acontecendo?”. São usados para: Determinar a prevalência de uma doença, como a prevalência de HIV em gestantes. Pesquisar atitudes ou opiniões em relação a um determinado assunto (pesquisa de satisfação) Verificar interrelações entre variáveis, como observação das características de fumantes pesados em relação ao sexo, idade, etc. Enquetes Cuidados na interpretação de dados de estudos transversais Efeito temporal Como os dados (exposição e desfecho) são coletados no mesmo momento, fica difícil estabelecer qualquer relação temporal entre eles (dilema ovo/galinha). Por exemplo, não é possível estabelecer uma relação de causalidade entre hipertensão e doença cardíaca se os dados são coletados de forma a ficar impossível saber que surgiu em primeiro lugar. Estudos transversais repetidos Os estudos transversais, algumas vezes, são repetidos em outro momento ou em outros locais com a finalidade de verificar variabilidade nos achados. Por exemplo, medir a prevalência de uma doença em momentos diferentes ou em diferentes locais. Os indivíduos serão um pouco diferentes, devendo-se interpretar as diferenças destes resultados com cautela. Estudos transversais que parecem longitudinais Uma armadilha comum é confundir um estudo seccional com um longitudinal porque os dados foram coletados através do tempo até completar o tamanho amostral previsto. O importante é que os dados (variável preditora e desfecho) foram coletados somente uma vez para cada indivíduo e no mesmo momento. Isto gera uma interpretação errônea se analisarmos como um estudo longitudinal. Análise dos Estudos Transversais Quando se compara a prevalência de doença em expostos e não expostos, a medida de associação usada é a Razão de Prevalência Pontual (RPP). 3.4.3 Estudos Caso-Controle Para examinar a possível associação de uma exposição a uma determinada doença, identifica-se um grupo de doentes (casos) e, com a finalidade de comparação, um grupo de pessoas sem a doença (controles) e determina-se a chance (odds) de exposição e não exposição entre casos e entre controles. Os estudos caso-controle, portanto, partem da presença ou ausência de um desfecho e após olham para trás no tempo (retrospectivamente) para detectar possíveis fatores de risco (Figura 3.1)(32). Analisam o que aconteceu e são usados para investigar fatores de risco de doenças raras onde um estudo prospectivo seria muito longo para identificar uma quantidade suficiente de casos. É útil também para investigar surtos agudos (infecção alimentar) para identificar se existe ou não associação entre a exposição e o desfecho investigado. Com frequência, os estudos caso-controle são o primeiro passo na busca de uma etiologia quando há suspeita de que alguma de várias exposições esteja associada a uma determinada doença. Figura3.1: Desenho de um estudo caso controle. Seleção dos casos Os casos podem ser selecionados de várias fontes, incluindo indivíduos hospitalizados, de consultórios ou clínicas, principalmente quando registros adequados são mantidos. Muitos problemas podem ocorrer na seleção de casos, neste tipo de estudo. Se os casos forem selecionados de um único hospital, quaisquer fatores de risco identificados podem ser apenas daquele hospital, em decorrência do padrão de referência e nível de atendimento (um hospital terciário que apenas atende um determinado convênio, por exemplo, o Sistema Único de Saúde). Por isso, devem ser utilizados casos procedentes de vários hospitais da comunidade, pois aí os casos pertenceriam a diferentes grupos sociais e diferentes graus de gravidade da doença. Casos incidentes ou prevalentes Os casos usados nos estudos caso-controle podem ser casos incidentes (recém-diagnosticados) ou casos prevalentes da doença (pessoas que apresentaram a doença em algum período). O problema do uso de casos incidentes é que há necessidade de se esperar que novos casos sejam diagnosticados e isto pode requerer muito tempo. Enquanto os casos prevalentes já estão disponíveis havendo um maior número disponível para o estudo. Em ambos os modelos existem problemas, pois nos casos prevalentes algumas pessoas podem morrer logo após o diagnóstico e estarem pouco representadas no estudo. Por outro lado, nos casos incidentes, serão excluídos os pacientes que morreram antes do diagnóstico ser feito. Não existe uma solução fácil para este problema, mas é importante lembrar-se destas questões ao interpretar os resultados e tirar conclusões do estudo. Seleção dos controles Da mesma forma do que nos estudos experimentais, a escolha dos controles afeta a comparação com os casos (33). A escolha dos controles inclui: Pacientes do mesmo hospital, mas com condições ou doenças não relacionadas; Pacientes pareados um a um em relação a fatores prognósticos, tais como sexo e idade; Uma amostra aleatória originária da mesma população de onde provêm os casos. Sem dúvida, o melhor grupo controle é a terceira opção, mas esta é raramente possível. Por este motivo, alguns estudos caso-controle incluem mais de um grupo controle para tornar o estudo mais robusto Controles pareados O emparelhamento é definido como processo de seleção dos controles para que sejam semelhantes aos casos em algumas características como, por exemplo, idade, gênero, raça, condição socioeconômica e ocupação. Controles emparelhados são bastante comuns. O autor deve ter o cuidado de especificar cuidadosamente o modo como houve o pareamento. Por exemplo, “emparelhado por idade dentro de dois anos” mostra a amplitude do pareamento. É difícil realizar o emparelhamento para muitos fatores, pois um pareamento seguro não existe. Em um delineamento pareado, a análise estatística deve levar em conta o emparelhamento e os fatores usados por ele. Onde um indivíduo em um par tiver um dado perdido, ambos devem ser omitidos da análise estatística. Estudos caso-controle aninhados Um delineamento do tipo caso-controle aninhado é um estudo de caso-controle ’’aninhado” em um estudo de coorte (34). É um excelente desenho para variáveis preditoras que são caras para medir e que podem ser avaliadas no final do estudo em indivíduos que desenvolvem o resultado durante o estudo (casos) e em uma amostra daqueles que não o fazem (controles). O investigador começa com uma coorte adequada (Figura 3.2) (35) com casos suficientes ao final do acompanhamento para fornecer poder adequado para responder à pergunta de pesquisa. No final do estudo, aplica critérios que definem o resultado de interesse para identificar todos aqueles que desenvolveram o resultado (casos). Em seguida, seleciona uma amostra aleatória dos indivíduos que não desenvolveram o resultado (controles). A principal razão para usar delineamentos caso-controle aninhado é reduzir o trabalho e o custo na coleta de dados. A principal desvantagem desse projeto é que muitas questões e circunstâncias da pesquisa não são passíveis de armazenamento para posterior análise. Figura3.2: Desenho de um estudo caso-controle aninhado. Estudo caso-controle de base populacional São os estudos caso-controle onde os casos e controles são uma amostra completa ou probabilística de uma população definida. Limitações dos estudos caso-controle Várias limitações podem afetar os estudos caso-controle: A escolha do grupo controle afeta as comparações entre casos e controles; Os dados da exposição ao fator de risco são coletados retrospectivamente e dependem da memória dos participantes, registros médicos e, portanto, podem ser incompletos, sem acurácia ou enviesados (viés de memória); Se o processo que conduz à identificação dos casos está relacionado a um possível fator de risco, a interpretação dos resultados será difícil (viés averiguação). Por exemplo: suponha que os casos sejam mulheres jovens com hipertensão selecionadas de uma clínica de contracepção. Nesta situação, um possível fator de risco, o anticoncepcional oral (ACO), estará vinculado à seleção dos casos e, desta forma, o uso de ACO será mais comum entre os casos do que entre os controles populacionais. Análise dos Estudos Caso-controle A principal estratégia de análise é o cálculo da odds ratio (Razão de Chances), que pode ser interpretado como uma estimativa do Risco Relativo. O Risco Relativo somente pode ser calculado quando é possível o cálculo da incidência (ver seção 18.5.2). Nos estudos caso-controle, isso não é possível, pois aqui o estudo começa com casos e controles em vez de indivíduos expostos e não expostos ao fator de risco. Desta maneira, se comparam as odds (chance) de uma exposição passada a um fator de risco suspeitado em indivíduos doentes e em controles não doentes. Esta relação é denominada de odds ratio (ver seção 18.5.1). 3.4.4 Estudos de Coorte Os estudos de coorte são considerados o padrão-ouro dos estudos observacionais. Seu nome se originou das coortes dos soldados romanos, cada uma delas constituída por 480 a 600 legionários. As coortes romanas eram distintas entre si e tinham sua identidade determinada por, ao menos, uma característica comum entre os indivíduos de cada grupo. Podia ser por características estratégicas no campo de batalha, por uma cor presente na indumentária, ou outras. Em Epidemiologia, o termo coorte permaneceu com significado semelhante. Em um estudo de coorte, um grupo de pacientes sadios (coorte), expostos ou não a um suspeitado fator de risco, é seguido através do tempo para determinar a incidência da doença em questão em cada um dos grupos (36). Neste modelo de estudo, a característica comum aos dois grupos é a exposição. Tem-se uma coorte de expostos e uma coorte de não expostos que são acompanhadas por um período de tempo que permita o aparecimento do desfecho. No final do estudo, compara-se a incidência do desfecho (doença) entre os expostos com a incidência do desfecho entre os não expostos. Se existe uma associação positiva entre a exposição e o desfecho, se espera que a incidência do desfecho entre os expostos seja maior do que a incidência de desfecho entre não expostos. Um esquema simplificado de um estudo de coorte é mostrado na Figura 3.3(37). Figura3.3: Desenho de um estudo de coorte sobre risco. Observar que como se identifica novos casos (incidência) à medida que eles ocorrem, é possível determinar uma relação temporal entre a exposição e a doença, isto é, se a exposição precedeu o início da doença. Isto é fundamental para estabelecer uma relação causal entre a exposição e a doença. Os estudos de coorte têm semelhança com os ensaios clínicos randomizados. Ambos os estudos comparam grupos expostos a grupos não expostos. Não havendo possibilidade de realizar a randomização, por exemplo, por motivos éticos quando a exposição é sabidamente prejudicial, é indicado um estudo de coorte. A diferença fundamental, portanto, é a ausência de randomização nos estudos de coorte. Existem duas maneiras básicas para formar os grupos: Seleciona-se a população-alvo baseado no fato dos indivíduos estarem expostos ou não ao fator em estudo (Figura 3.3); Ou seleciona-se a população-alvo antes que qualquer um dos seus membros se torne exposto, ou antes, que a exposição seja identificada (Figura 3.4). Um exemplo típico deste modelo é o clássico Estudo de Framingham (38). Figura3.4: Desenho de uma coorte com grupos expostos e não expostos. (39). Tipos de estudo de coorte De acordo com as características do seguimento, as coortes podem ser: Estudo de Coorte Prospectivo (Coorte Concorrente ou Longitudinal), onde os grupos são montados no presente, coletados os dados basais deles e continua-se a coletar dados com o passar do tempo até a doença se desenvolver ou não. Estudo de Coorte Retrospectivo ou Histórico (Coorte não concorrente), onde a exposição é avaliada em dados passados e o desfecho (doença ou não) é verificado no momento do início do estudo. O problema aqui é que a averiguação da exposição depende dos registros pregressos. Estudo de Coorte Misto (Prospectivo e Retrospectivo), onde a exposição é verificada em registros objetivos no passado (como em uma coorte histórica) e o seguimento e a medida do desfecho se fazem no futuro. Vieses em estudos de coorte Os potenciais vieses nos estudos de coorte são os seguintes: Viés de confusão – é a grande ameaça dos estudos observacionais. O confundimento causa um erro sistemático na inferência, podendo aumentar ou diminuir uma associação observada entre exposição e doença. Uma variável funciona como fator de confusão quando ela está associada com a exposição e ao mesmo tempo com a doença. Ela não deve fazer parte da cadeia causal da exposição à doença. Por exemplo, num estudo sobre fatores de risco, uma associação entre o hábito de beber café e a doença coronária é detectada. Porém, se não for considerado o fato de que os fumantes bebem mais café do que os não-fumantes, pode-se chegar à errônea conclusão de que o café é um fator de risco independente para doença coronária, o que não corresponde à realidade. Neste caso, o café é um fator de confusão e não um fator causal independente para a doença coronária (40). Viés na avaliação dos desfechos – este viés pode ocorrer quando o pesquisador que avalia o desfecho também sabe sobre o status de exposição dos sujeitos da pesquisa. Evita-se este problema “cegando” a pessoa que faz a avaliação da doença. Viés de informação – ocorrem principalmente em estudos históricos onde as informações dependem de registros passados e podem ser diferentes entre as pessoas expostas e não expostas. Viés de não resposta e perdas de acompanhamento – a não participação e as perdas podem introduzir um grande viés, alterando o cálculo da incidência nos expostos e entre os não expostos. Viés de análise – se os estatísticos tiverem alguma hipótese em relação aos dados que estão analisando, eles podem introduzir vieses em suas análises. Análise dos estudos de coorte Para verificar se existe associação entre certo desfecho (doença) e uma determinada exposição calcula-se o Risco Relativo (RR). Este é definido como a razão entre a incidência (risco) em expostos e a incidência (risco) em não expostos (ver seção 18.5.2). Vantagens e desvantagens dos estudos de coorte Vantagens Adequado para exposições raras Bom poder para testar hipóteses Importante em estudos etiológicos e prognósticos Salienta os múltiplos desfechos de uma exposição Desvantagens Inadequado em desfechos raros Perdas no seguimento levam a viés de seleção Demorado/elevado custo 3.5 Ensaios Clínicos Experimentos são estudos nos quais o pesquisador manipula a variável preditora (intervenção) e observa o efeito no desfecho que está sendo avaliado ao longo do tempo. A abordagem experimental, especificamente, o ensaio clínico randomizado controlado é a ferramenta de escolha para comparar terapêuticas ou intervenções. Os estudos experimentais podem também comparar os cuidados prestados por serviços de saúde, programas de educação em saúde e estratégias administrativas. Os estudos experimentais realizados com seres humanos são denominados de ensaios clínicos. Nos ensaios clínicos não controlados os indivíduos servem como seus próprios controles (antes-e-depois). Os resultados destes estudos estão sujeitos vários problemas: Melhora previsível. Paciente melhora espontaneamente e não pelo tratamento. Flutuação na gravidade da doença. Efeito Hawthorne: o indivíduo melhora pela atenção e não pela terapêutica (41). Regressão à média: uma limitação importante surge quando se quer avaliar a evolução de um grupo que tenha sido selecionado por estar no extremo de uma distribuição sem que haja um grupo controle. Empiricamente, observa-se que indivíduos que se encontrem num determinado momento, em um dos extremos de uma distribuição, tendem a estarem menos distantes da média em um momento posterior, sem que qualquer intervenção tenha sido desenvolvida. Este fenômeno é conhecido como efeito de regressão à média. Por exemplo: uma pessoa com uma doença crônica tem dias piores e outros melhores. Se ela é medicada com gotas homeopáticas ou faz uso de florais nos dias em que se sente excepcionalmente mal vai notar que é frequente uma melhora, seguindo estes “tratamentos”. Não que eles funcionem, mas pela regressão à média (42). 3.5.1 Características de um ensaio clínico Um ensaio clínico deve ter algumas características fundamentais (Figura 3.5)(43): Os indivíduos devem ser designados por randomização para os grupos de comparação. A randomização é a melhor abordagem no delineamento de um ensaio clínico (44). Randomizar significa sortear (por meio de computadores, tábua de números aleatórios) os indivíduos para decidir a alocação dos mesmos em um dos grupos de estudo. O elemento decisivo da randomização é a imprevisibilidade da próxima alocação. O pesquisador compara o grupo de estudo com um grupo controle apropriado. O investigador manipula a variável independente (preditora). Figura3.5: Estrutura de um ensaio clínico randomizado. 3.5.2 Elementos básicos de um ensaio clínico Seleção dos participantes Os pesquisadores devem determinar e explicar detalhadamente os critérios de inclusão e de exclusão: Objetivos dos critérios de inclusão e exclusão Restringir a heterogeneidade da amostra Diminuir o número de variáveis independentes Fazer com que exista uma chance maior de que as diferenças nos desfechos estejam relacionadas aos tratamentos Melhorar a validade interna, ou seja, o grau em que os resultados do estudo são consistentes para aquela amostra particular de indivíduos. Esta validade depende basicamente do rigor metodológico usado para delinear o ensaio clínico, podendo ser ameaçada por dois tipos de erros: sistemático ou aleatório. Tornar a generalização (validade externa) mais precisa. Entretanto deve-se ter cuidado com critérios de inclusão e exclusão muito rígidos, pois podem diminuir esta capacidade de generalização O grau de detalhamento deve ser suficientemente preciso para permitir que outros reproduzam o estudo. O tamanho da amostra deve ser claramente determinado pelo poder do teste estatístico. Poder é a habilidade de o teste estatístico detectar diferenças entre os grupos, dado que tais diferenças existam na população em estudo. Lembrar que resultados não significativos podem ser apenas uma evidência para um inadequado tamanho amostral. O grupo controle deve ser selecionado utilizando-se os mesmos critérios do grupo experimental. Prestar atenção em possíveis armadilhas que podem gerar vieses: Uso de grupo controle histórico (não concorrente); Grupo controle selecionado de outros locais (outras clínicas, outros hospitais). O grupo controle adequado é um grupo controle concorrente, tratado no mesmo momento e no mesmo local do grupo experimental. O característico é o grupo controle não receber tratamento. Mais comumente recebem um placebo, indistinguível do tratamento experimental, mas sem componente ativo. Mesmo assim, pode haver melhora dos participantes do grupo controle (Efeito Placebo ) (45). Quando não for ético suspender o tratamento e administrar placebo, o grupo controle pode ser constituído por indivíduos que recebem o tratamento padrão. Alocação A alocação deve ser aleatória. A randomização é a principal técnica para reduzir o viés, criando grupos homogêneos. Como foi visto, é uma das características fundamentais dos ensaios clínicos. O poder da randomização depende da ocultação da sequência de alocação. A randomização pode ser: Completa: os indivíduos que obedecem ao critério de inclusão e exclusão são randomizados de modo que todos têm a mesma probabilidade de pertencer a cada um dos grupos. Isto maximiza o poder. Pode ser feita por blocos para assegurar a igualdade numérica dos grupos (estudos multicêntricos). Estratificada: os participantes são estratificados de acordo com possíveis variáveis de confusão (gravidade da doença, idade, sexo, etc.) e a randomização é realizada dentro de cada estrato. Randomização e alocação desigual: os sujeitos têm uma maior probabilidade de ser randomizados em um grupo (em geral, grupo experimental) do que o outro (comparação). Este tipo de estudo tem menor poder. Condução/Seguimento/Avaliação Em um ensaio clínico deve estar assegurado de que o estudo tenha um tempo de seguimento adequado, pois nem todos os indivíduos participam conforme o plano original. Podem ocorrer perdas de alguns pacientes durante o acompanhamento, seja porque com o tempo se constata que eles não têm a doença em estudo ou porque não aderiram ao tratamento ou intervenção e abandonaram o estudo. Quanto maior o número de pacientes perdidos e menos informações sobre eles, menos confiança pode ser colocada nos resultados do estudo. De um modo geral, não se deve tolerar perdas que sejam maiores que a incidência do desfecho no estudo. Uma regra simples é que perdas menores que 5% produzem pouco viés e perdas maiores que 20% são uma ameaça importante à validade do estudo. As perdas entre 5 e 20% devem ser avaliadas com cuidado, se possível utilizando-se uma análise de sensibilidade (pior cenário), principalmente se as perdas forem diferentes nos grupos pelo maior risco de viés. Neste tipo de análise, nos estudos com resultado positivo, todos os pacientes perdidos no grupo experimental, inicialmente, são considerados como tendo o desfecho. Posteriormente, analisa-se como se nenhum dos indivíduos perdidos no grupo controle atingiu o desfecho. Se o resultado permanecer positivo, as perdas não afetaram a validade do estudo. Estudos sem relato adequado ou nenhum relato de perdas ou exclusões devem ser avaliados com muito cuidado. Outro aspecto importante, no seguimento dos sujeitos da pesquisa, é o tratamento igual de todos os grupos. Para garantir este princípio, utiliza-se da técnica de cegamento ou mascaramento (46). Esta técnica impede que os participantes da pesquisa (pesquisadores, avaliadores e participantes) tomem conhecimento de qual grupo de tratamento o participante se encontra. Este conhecimento antecipado pode influenciar as expectativas, as opiniões e as crenças em relação aos resultados do estudo. O cegamento tem como principal finalidade a eliminação do viés de aferição, além de melhorar a adesão ao tratamento, reduzir as perdas de seguimento e diminuir o viés causado por co-intervenções (assistência suplementar maior para um dos grupos). Quando o cegamento ocorre nos pacientes e nos pesquisadores, diz-se que o estudo é duplo-cego. Se ele também incluir os avaliadores do estudo, ele é triplo cego. Um ensaio clínico em que não há cegamento é dito aberto (open label, no caso de estudos com fármacos). A avaliação dos desfechos também pode afetar os resultados. É importante garantir-se que aqueles que registram os desfechos estejam cegados em relação a que grupo o sujeito da pesquisa pertence. Os autores devem estabelecer regras cuidadosas para decidir se um desfecho ocorreu ou não e despender esforços iguais para identificar desfechos para todos os pacientes no estudo. Intenção de tratar Os pesquisadores violam a randomização se omitirem da análise os pacientes que não receberam a intervenção designada ou, pior ainda, contarem eventos que ocorreram nos sujeitos não aderentes que foram designados para a intervenção contra o grupo controle. Os sujeitos de uma pesquisa, para evitar tal viés, devem ser analisados dentro do grupo para o qual eles foram alocados pela randomização (47). Este princípio é denominado intenção de tratar. Análise da magnitude do efeito Calcula-se uma série de estimativas quantitativas para analisar a magnitude do efeito da intervenção em um ensaio clínico. Entre elas, destacam-se o Risco Relativo, Redução Relativa do Risco, Número Necessário para Tratar que serão estudados no capítulo 18. Outro método para avaliar resultados de um ensaio clínico para dados de tempo até o evento é a análise de sobrevida. Esta fornece informação sobre a rapidez com que os eventos ocorrem. A curva de sobrevida pode utilizar dados de pacientes acompanhados por diferentes períodos de tempo. 3.5.3 Ensaios clínicos de equivalência e não inferioridade Ensaios clínicos controlados com placebo são ideais para avaliar a eficácia de um tratamento. Eles permitem o controle do efeito placebo e são mais eficientes, exigindo um menor número de pacientes para detectar um efeito do tratamento. Um ensaio clínico placebo controlado é eticamente justificado se não existe tratamento padrão, se o tratamento padrão não se mostrou eficaz, não há riscos associados com o retardo no tratamento e se a possiblidade de se retirar do estudo está incluída no protocolo. Sempre que possível e justificado, os ensaios clínicos placebo controlados devem ser a primeira escolha para avaliação de um tratamento. Dado que um grande número de tratamentos eficazes comprovados está disponível, ensaios clínicos controlados por placebo são, muitas vezes, antiéticos. Nestas situações, ensaios clínicos com controle ativo são geralmente apropriados. Se o objetivo do ensaio clínico é testar se um novo tratamento é similar em eficácia a um tratamento já existente, ele é denominado de Estudo de Equivalência. O Ensaio Clínico é delineado de maneira que possa demonstrar que, dentro limites aceitáveis, os dois tratamentos são igualmente eficazes. Existe equivalência quando a diferença observada entre os dois tratamentos for menor que a máxima diferença aceitável, determinada previamente. Estes limites devem ser clinicamente apropriados. Se condição em investigação for muito grave, os limites para a equivalência devem ser estreitados. Quanto menor forem os limites de equivalência, maior o tamanho amostral. Este delineamento é útil se o novo tratamento trouxer benefícios, tais como menores efeitos colaterais, facilidade no uso e ser mais barato. Em muitos estudos com controle ativo, os pesquisadores desejam comprovar que o tratamento em estudo, no mínimo, não é substancialmente pior que o tratamento controle. Estes estudos são chamados de Estudos de Não Inferioridade. Um aspecto importante do delineamento e da interpretação desses estudos é a determinação da margem de não inferioridade. Os estudos de não inferioridade devem demonstrar, pelo menos, que o tratamento em estudo tem alguma eficácia, não inferior ao tratamento padrão. A análise dos estudos de não inferioridade é, por natureza, unidirecional. Quando um ensaio clínico busca evidenciar que um tratamento é melhor do que outro ele é denominado Estudos de Superioridade. Quando o ensaio clínico é delineado, ele deve ter uma hipótese bilateral e o tamanho da amostra definido de maneira que haja alto poder estatístico para detectar uma diferença clinicamente significativa entre os dois tratamentos. Os ensaios clínicos clássicos têm esta característica. Entretanto, nos dias atuais, este desenho de estudo pode não ser eticamente possível, uma vez que é pouco provável que não exista um tratamento com algum benefício comprovado. A comparação, portanto, deverá ser feita com o tratamento já existente, provando que o tratamento em estudo é similar ou, pelo menos, não seja inferior (48). 3.5.4 Outros tipos de ensaios clínicos Ensaio clínico com delineamento cruzado No delineamento cruzado (crossover design), os sujeitos da pesquisa são randomizados para um grupo e depois mudados para o outro grupo (Figura 3.6). Cada sujeito serve como seu próprio controle, diminuindo a variabilidade intragrupo, aumentando o poder e consequentemente, reduzindo o erro \\(\\beta\\) (erro que ocorre quando a análise estatística dos dados não consegue rejeitar uma hipótese, no caso desta hipótese ser falsa). É um tipo de delineamento bastante atrativo e útil (49). A maior desvantagem é o efeito residual (carryover), por isso os estudos cruzados devem ter um período de washout, período sem nenhum tratamento. Este período de tempo deve ser suficiente para a eliminação da droga para se ter certeza de que nenhum efeito da terapia permaneceu. Também pode haver um viés de acordo com a ordem de administração das terapias, pois os pacientes podem reagir de modo diferente como resultado do entusiasmo no início do tratamento que pode diminuir com o tempo. Figura3.6: Ensaio clínico randomizado com delineamento cruzado. Delineamento Fatorial Uma variação interessante de ensaio clínico é o delineamento fatorial. Este tipo de estudo permite que sejam testadas duas drogas em apenas um estudo, assumindo que os desfechos antecipados para as duas são diferentes e que seus modos de ação são independentes. Este desenho de estudo gera economia. Um exemplo de delineamento fatorial é observado no Physician’s Health Study onde usando um delineamento fatorial 2 x 2 foi testada a aspirina para a prevenção primária de doença cardiovascular (50), e betacaroteno para a prevenção primária de câncer. No estudo da prevenção primária do câncer, os autores concluíram, após 12 anos de suplementação de betacaroteno, que o mesmo não produziu nem benefícios e nem prejuízos em termos de incidência de câncer (51). 3.5.5 Fases de um ensaio clínico Para a realização de um ensaio clínico, a intervenção deve passar por várias fases (52). Fase Não Clínica Antes de começar a testar novos tratamentos em seres humanos, os cientistas testam as substâncias em laboratórios (in vitro) e em animais de experimentação. O objetivo principal desta fase é verificar como esta substância se comporta em um organismo. Assim, após esta fase se pode verificar se o medicamento é seguro para ser testado em seres humanos. Todo este processo é regido por leis da bioética em pesquisa em animais. Fase Clínica A fase clínica é a fase de testes em seres humanos. Esta etapa é constituída por quatro fases consecutivas e somente depois de finalizadas todas as fases, a droga poderá ser autorizada para comercialização e disponibilizada para uso em seres humanos. As sucessivas fases dentro da fase clínica são: Fase I - Um estudo de fase I testa a droga pela primeira vez. O objetivo principal é avaliar a segurança do produto investigado. Nesta fase, o medicamento é testado em pequenos grupos (10 – 30 pessoas), geralmente, de voluntários sadios. Podemos ter exceções se estivermos avaliando medicamentos para câncer ou portadores de HIV-AIDS. Se a droga se mostrar segura, é possível ir para a Fase II. Fase II - Nesta fase, o número de pacientes é maior (70 - 100). O objetivo é avaliar a eficácia da medicação, isto é, se ela funciona para tratar determinada doença, e também conseguir informações mais detalhadas sobre a segurança (toxicidade). Somente se os resultados forem bons é que o medicamento será estudado como um estudo clínico fase III. Fase III - Nesta fase, o novo tratamento é comparado com o tratamento padrão existente. São os ensaios clínicos. O número de pacientes aumenta e depende da hipótese (em geral, 100 a 1.000). Devem de preferência utilizar desfechos clínicos, grupo controle, além de serem randomizados e duplo-cegos. Fase IV - Estes estudos são realizados para se confirmar que os resultados obtidos na fase III são aplicáveis a grande parte dos doentes. Nesta fase, o medicamento já foi aprovado para ser comercializado. A vantagem dos estudos fase IV é que eles permitem acompanhar os efeitos dos medicamentos em longo prazo. É uma fase de vigilância pós-comercialização. http://calculoamostral.bauru.usp.br/calculoamostral/index.php↩︎ "],["ambiente-do-r.html", "Capítulo 4 Ambiente do R 4.1 Instalação do R básico 4.2 RStudio 4.3 Pacotes 4.4 Diretório de trabalho 4.5 Projeto 4.6 O R como calculadora 4.7 Objetos 4.8 Funções 4.9 Classes 4.10 Vetores 4.11 Dataframes 4.12 Fatores", " Capítulo 4 Ambiente do R 4.1 Instalação do R básico Para usar o R, há necessidade de carregar o programa básico que contém a sua linguagem de programação. O sistema é formado por um programa básico, Graphical User Interface (R-Gui) e muitos pacotes com procedimentos adicionais. O site oficial do R fornece as versões atualizadas do software e informações sobre este sofisticado projeto de computação estatística. Para baixar o R, usa-se um “CRAN Mirror”, clicando em CRAN (Comprehensive R Archive Network) na margem esquerda, abaixo de Download. O CRAN é central no uso do R: é o local de onde se carrega o software e todos os pacotes necessários para instalar e para expandir o R. Em vez de ter um único local, o CRAN é “espelhado” em diferentes locais do mundo. “Espelhado” significa simplesmente que existem versões idênticas do CRAN distribuídas por todo o mundo. É possível baixar o R diretamente da nuvem ou escolher uma origem mais próxima do seu local de atuação. No Brasil, encontram-se várias opções, como a Universidade Federal do Paraná, Fundação Oswaldo Cruz, RJ, Universidade de São Paulo, São Paulo e Universidade de São Paulo, Piracicaba Após escolher uma das alternativas acima (pode ser qualquer uma delas) surgirá a página The Comprehensive R Archive Network com as opções para escolher o sistema operacional. Escolha o sitema de acordo com o seu computador (Windows, macOS ou Linux). Ao clicar em uma dessas opções, se o sistema operacional escolhido é o Windows, aparecerá a página R for Windows. Nesta, deve-se clicar em base. No caso de outros sistemas operacionais, seguir as orientações mostradas no site do R. Clicando em base, haverá um redirecionamento para a a página onde aparece a versão do R para o Windows mais atual. Clique no link que diz Download R-…for Window para baixar o instalador em um diretório do computador, em geral Downloads. Para instalar o programa básico, basta executar o instalador R-…-win.exe baixado no diretório. Ao fazer isso, aparece na tela do computador,no canto esquerdo, em baixo, o arquivo salvo. Execute este arquivo com um clique sobre ele. Aparecerá u,a janela perguntando “Deseja permitir que este aplicativo faça alterações no seu dispositivo?”. Clique em Sim. A seguir o instalador pedirá para escolher o Idioma. Selecione Português Brasileiro. Em sequência aparecerão informações sobre o diretório no qual o R será instalado em seu computador. Recomenda-se aceitar a configuração padrão sugerida pelo instalador do software. A próxima janela pedirá para personalizar os componentes que serão instalados. Recomenda-se usar as configurações sugeridas pelo instalador que irá reconhecer automaticamente a arquitetura do seu sistema Windows (32 e/ou 64 bits). A partir daqui, siga as recomendações padrão propostas pelo instalador até completar a instalação, clicando em Concluir. O R não precisa ser iniciado, pois o software que será usado, neste livro, é o RStudio. Este, para ser executado, necessita ter o R instalado no computador. Ou seja, o R é o programa “cérebro” necessário para as análises de dados que serão realizadas. Ele precisa estar instalado para permitir o funcionamento do RStudio. 4.2 RStudio O RStudio é um membro ativo da comunidade R. Foi fundado em 2009 por Joseph J. Allaire, engenheiro de software americano. O RStudio, inspirado pelas inovações dos usuários de R em ciência, educação e indústria, desenvolveu ferramentas gratuitas e abertas para facilitar o uso do R. O RStudio é um projeto filiado à Foundation for Open Access Statistics (FOAS). A FOAS trabalha para garantir o sucesso do projeto R. Eles promovem o uso e o desenvolvimento de software livre para estatísticas, como a linguagem R e o ambiente para estatísticas computacionais. Junto está o R Consortium que é uma colaboração entre a Fundação R, RStudio , Microsoft, TIBCO, Google, Oracle, HP e outros. O RStudio é patrocinado para financiar e inspirar ideias que permitirão que o R se torne uma plataforma ainda melhor para a ciência. 4.2.1 Instalação do R Studio Para instalar o RStudio , acessar o site e clicar em Download para obter a versão desejada. Recomenda-se a versão RStudio Desktop – Open Source License que é gratuita. Esta versão entrega as ferramentas integradas para o R. A seguir, aparecerão os instaladores disponíveis, conforme a plataforma suportada pelo seu computador. As mais utilizadas são Windows e Mac OS X. Neste livro, como base, serão mostrados os passos para a plataforma Windows 5. Em sequência, executar o instalador baixado RStudio-2023.03.0-386.exe 6 e seguir as suas instruções. 4.2.2 Iniciando o RStudio Para iniciar o RStudio basta clicar no ícone indicativo (Figura 4.1) que se encontra no menu Iniciar do Windows. Figura4.1: Ícone do RStudio O RStudio abre como mostrado na Figura 4.2. O RStudio é uma interface mais funcional e amigável para o R. Contém um conjunto de ferramentas integradas projetadas para ajudá-lo a ser mais produtivo com o R. Figura4.2: Tela inicial do RStudio Inclui o Console , editor que suporta execução direta de códigos e uma variedade de ferramentas robustas para plotagem, exibição de histórico, depuração e gerenciamento de seu espaço de trabalho incluídos em uma interface que está, inicialmente, dividida em 3 paineis: Console Environment, History, Connections, Tutorial Files, Plots, Packages, Help Console e R Script Do lado esquerdo fica o Console (Figura 4.2, em vermelho), onde os comandos podem ser digitados e onde aparecem os resultados da execução dos comandos. Ao abrir o RStudio , aparece no Console uma série de informações sobre o R, como versão em uso e, por último, o diretório onde está armazenado o espaço de trabalho (workspace). Estas informações podem ser facilmente apagadas, clicando na barra de ferramentas, no menu Edit, e após em Clear Console ou, usando as teclas Ctrl+L. O Console é a principal parte do R. Aqui é onde o R realmente executa o comando. No início do Console, existe um caractere (&gt;). Este é um prompt que informa que o R está pronto para receber um novo código. Pode-se digitar o código diretamente no Console após o prompt e obter uma resposta imediata. Por exemplo, se for digitado 1 + 1 e pressionado Enter, o R imediatamente gera uma saída de 2 (Figura 4.3). Figura4.3: Console Recomenda-se que a maior parte dos comandos sejam digitados no bloco de notas do RStudio , o R Script. Reservar o Console apenas para depurar ou fazer análises e cálculos rápidos. A razão para isso é simples: se o comando for digitado diretamente no Console, ele não será salvo e se for cometido um erro na digitação, haverá necessidade de digitar tudo novamente. Portanto, é melhor escrever os comandos no R Script e, quando estiver pronto para executar, enviar para o Console. O R Script é o quarto painel do RStudio e seu bloco de notas. Ele é criado através do menu File &gt; New File &gt; R Script ou clicando no botão verde com o sinal (+), na barra de ferramentas de acesso rápido, na parte superior à esquerda. Ao criar um novo R Script será aberto o painel do bloco de notas (Figura 4.4, em verde). Figura4.4: R Script Um diferencial do RStudio é que os comandos são autocompletáveis. Basta começar a escrever o comando, inserindo 3 ou mais caracteres, por exemplo, summ referente a função summary (), usada para sumarizar um conjunto de dados, e surge um menu de opções, facilitando a digitação (Figura 4.5). Figura4.5: Menu autocompletável Após digitar no Console, para que seja executado o comando há necessidade de clicar na tecla Enter; no RScript, clicar em Run, acima, na barra, no lado direito, ou usar o atalho Ctrl + Enter. Textos podem ser copiados e colados no script e linhas em branco podem ser inseridas. Além disso, no final da sua sessão, é possível salvar o arquivo, que poderá ser recarregado no futuro, se precisar refazer a análise. Os scripts do R são apenas arquivos de texto com a extensão (.R). Quando se cria um R Script, aparece como Sem título (Untitled). Antes de começar a digitar um novo script no R Sem título, recomenda-se salvar o arquivo com um novo nome de arquivo. Dessa forma, se algo no computador falhar durante o trabalho, o R terá o código protegido. Ao digitar o código em um script, o R não executa o código enquanto se digita. Para que o R realmente avalie o código digitado, há necessidade de primeiro enviar o código para o Console, clicando no botão Run ou usando a tecla de atalho Crtl+Enter. Cada linha é marcada no início por um número em sequência. Além da digitação de comandos, o R Script permite fazer comentários onde tudo que for escrito após o símbolo \\(\\#\\) não é considerado, é apenas uma explicação, um esclarecimento. Os comentários são literais, escritos diretamente para explicar o comando executado. São repetidos na saída do Console sem não aparecer nos resultados. Ambiente, História, Conexão e Tutorial No lado superior direito há um painel com quatro abas (Figura 4.2, em azul): Ambiente (Environment) - onde ficam armazenados os objetos criados, as bases de dados importadas, etc., na sessão ativa. É possível visualizar informações como o número de observações e linhas dos bancos de dados ativos. A guia também tem algumas ações clicáveis, como Import Dataset, que permite importar arquivos csv, Excel, SPSS, etc. História (History) - onde fica o histórico dos comandos executados no Console. Estes comandos podem ser pesquisados nesta guia. Os comandos são exibidos em ordem (mais recentes na parte inferior) e agrupados por bloco de tempo. Conexões (Connections) - mostra todas as conexões feitas com fontes de dados suportadas e permite saber quais conexões estão ativas no momento. O RStudio suporta múltiplas conexões de banco de dados simultâneas. Tutorial - a partir da versão 1.3, o R Script ganhou um painel Tutorial dedicado, usado para executar tutoriais que ajudarão você a aprender e dominar a linguagem de programação R. Na primeira vez que se abre o programa, clicando nesta aba, o RStudio solicita que seja instalado o pacote learnr (Figura 4.6). Isto permite acesso a vários tutoriais úteis que merecem ser explorados Figura4.6: Tutoriais do RStudio Arquivos, Gráficos, Pacotes, Ajuda e Apresentação No lado direito, abaixo, existem outras abas muito úteis (Figura 4.2, em amarelo): Arquivos (Files) - esta guia dá acesso ao diretório onde se encontram os seus arquivos. Um bom recurso do painel Files é que se pode usá-lo para definir seu diretório de trabalho. Para isso, clique em More e depois em Set As Working Directory. Gráficos (Plots) - local onde ficam os gráficos gerados. Existem botões para abrir o gráfico em uma janela separada e exportar o gráfico como um .pdf ou .jpeg. Pacotes (Packages) - mostra uma lista de todos os pacotes R instalados no seu computador e indica se eles estão atualmente carregados ou não. Pacotes que estão sendo executados na sessão atual, estão marcados, enquanto aqueles que estão instalados, mas ainda inativos, estão desmarcados. Ajuda (Help) - menu de ajuda para as funções R. Você pode digitar o nome de uma função na janela de pesquisa (por exemplo, histogram ou usar o ?hist), no Console ou no R Script, para procurar ajuda sobre uma função (Figura 4.7). A Ajuda no R Studio pode também ser acessada no menu Help da barra de ferramentas onde existem várias opções. Para complementar, alguns livros são muito uteis, como o R Cookbook (53) ou Using R* for introductory statistics* (54). No entanto, na maioria das vezes a forma mais prática de conseguir ajuda com uma dúvida específica é a busca em fóruns na internet, como o Stack Overflow: https://stackoverflow.com/. Apresentação (Presentation) – é visualizador de apresentações. Nas últimas versões do Rstudio, é possível com o Quarto, editar um código em R Markdown para construir uma apresentação. Não faz parte do objetivo deste livro desenvolver este assunto. É possível encontrar um tutorial em https://quarto.org/docs/get-started/hello/rstudio.html. Figura4.7: Ajuda do RStudio 4.3 Pacotes Para que o R cumpra a sua função de dialogar com o usuário para realizar análises estatística e construir gráficos, ele necessita ter instalado pacotes. Quando se instala o R básico, ele vem com vários pacotes que permitem uma grande quantidade de análises. Entretanto, à medida que se utiliza o R, torna-se necessário instalar novos pacotes criados pela comunidade do R. Esses novos pacotes contêm novas funções e novos comandos que aumentarão a funcionalidade do R. Um pacote é uma coleção de funções, dados e documentação que expande os recursos do R base. O uso dos pacotes é a chave para o uso bem-sucedido do R. Eles são instalados à medida que o trabalho com o R exigir. 4.3.1 Repositório de pacotes Quando se identifica a necessidade de um novo pacote, é fundamental saber onde ele se encontra. O principal repositório de pacotes é o CRAN (Comprehensible R* Archive Network), já comentado anteriormente. Para acessar este repositório, use o link e escolha um espelho (0-Cloud* ou o mais próximo geograficamente). Depois que o pacote for instalado, ele será mantido em sua biblioteca (library) R associada à sua versão principal atual do R. Haverá necessidade de atualizar e reinstalar os pacotes sempre que atualizar uma versão principal do R. Estando na página do CRAN, no menu, à esquerda, clique em Packages . Isto o colocará na página dos Contributed Packages, onde a maioria dos pacotes podem ser encontrados em Table of available packages, sorted by name . Também é possível clicar em CRAN Task Views , onde estão os pacotes separados por tópicos. 4.3.2 Instalação de um pacote novo Instalar um pacote significa simplesmente baixar o código do pacote em um computador pessoal. Existem duas maneiras principais de instalar novos pacotes. O método mais comum é baixá-los do CRAN, usando a função install.packages (). Dentro dos parênteses, como argumento, coloca-se entre aspas (duplas ou simples) o nome do pacote. Como visto, deve-se, de preferência, digitar o comando no R Script. Por exemplo, será instalado o pacote ggplot2 que contém múltiplas funções gráficas como abaixo: install.packages(&quot;ggplot2&quot;) library(ggplot2) Para carregar o pacote, isto é, para fazer com que suas funções se tornem ativas para uso na na sessão, deve-se usar a função library(), como mostrado no comando acima. Se o RStudio for fechado e reaberto, o o pacote deverá ser novamente ativado. Observe que a função library() não requer que o nome do pacote seja digitado entre aspas. Isto acontece porque antes de o pacote ser instalado o R não o reconhece , portanto, há necessidade de indicar o nome (caracteres), para que o R procure na internet, por exemplo, o que ele deve baixar. Já, depois de instalado, o pacote é um objeto conhecido pelo R, logo as aspas não são mais necessárias. Uma outra maneira de instalar pacotes no R, é usar o botão Install, localizado na aba Packages, no painel inferior, à direita. Clicando em Install, abre-se a caixa de diálogo da Figura 4.8. Digitar em Packages o nome do pacote (ggplot2) e o RStudio completará com opções para achar o pacote. Clicar em ggplot2 e verifique se Install dependencies foi selecionado. A seguir clicar em Install e aguardar aparecer no Console a mensagem que o pacote foi instalado com sucesso. Figura4.8: Instalação do pacote ‘ggplot2’ usando a caixa de diálogo ‘Install Packages’ 4.3.3 Atualização dos pacotes Periodicamente, há necessidade de atualizar os pacotes instalados. Essa necessidade advém do fato que, com o tempo, os autores de pacotes lançarão novas versões com correções de defeitos e novos recursos e, geralmente, é uma boa ideia manter-se atualizado. Para realizar a atualização proceda da seguinte maneira: # atualiza todos os pacotes disponíveis, solicitando permissão update.packages() # atualiza, sem solicitações de permissão/esclarecimento update.packages(ask = FALSE) # atualiza um pacote específico update.packages(&quot;ggplot2&quot;) 4.3.4 Instalando e carregando mais de um pacote Para carregar mais de um pacote simultaneamente, pode-se usar uma das funções: libraries() ou packages() do pacote easypackages. Em primeiro lugar, instalar e carregar o pacote: install.packages(&quot;easypackages&quot;) library(easypackages) Posteriormente, basta usar uma das funções do easypackages: libraries(&quot;readxl&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;car&quot;) Outro pacote que gerencia pacotes do R é o pacman (55). Este pacote tem a função p_load() que instala e carrega um ou mais pacotes. Usar esta função, escrevendo o nome dos pacotes sem necessidade de aspas: install.packages(&quot;pacman&quot;) library(pacman) p_load(readxl, dplyr, ggplot2, car) Ou, escrever diretamente: pacman::p_load(readxl, dplyr, ggplot2, car) O pacote pacman tem outas funções, entre elas a função p_update() que atualiza o pacote e , se usada sem especificar o pacote , atualiza todos. Para saber mais sobre o pacote pacman, use a ajuda. p_update(readxl, dplyr, ggplot2, car) 4.3.5 Citação de pacotes em publicações No R existe um comando que mostra como citar o R ou um de seus pacotes. Basta digitar a função citation() no Console ou no R Script e observar a saída. Para um pacote específico, basta colocar o nome do pacote entre aspas, na função. citation() ## To cite R in publications use: ## ## R Core Team (2023). _R: A Language and Environment for Statistical ## Computing_. R Foundation for Statistical Computing, Vienna, Austria. ## &lt;https://www.R-project.org/&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2023}, ## url = {https://www.R-project.org/}, ## } ## ## We have invested a lot of time and effort in creating R, please cite it ## when using it for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for ## citing R packages. citation (&quot;ggplot2&quot;) ## To cite ggplot2 in publications, please use ## ## H. Wickham. ggplot2: Elegant Graphics for Data Analysis. ## Springer-Verlag New York, 2016. ## ## A BibTeX entry for LaTeX users is ## ## @Book{, ## author = {Hadley Wickham}, ## title = {ggplot2: Elegant Graphics for Data Analysis}, ## publisher = {Springer-Verlag New York}, ## year = {2016}, ## isbn = {978-3-319-24277-4}, ## url = {https://ggplot2.tidyverse.org}, ## } 4.4 Diretório de trabalho O diretório de trabalho (Working Directory) é uma pasta onde o R lê e salva arquivos. Deve-se criar um diretório de trabalho para a sessão . Para isso, no RStudio siga o caminho: Session &gt; Set Working Directory &gt; Choose Directory ou use o atalho Ctrl + Shift + H e escolha o diretório desejado ou crie um novo. Ao finalizar, aparecerá no Console (Figura 4.9): Figura4.9: Diretório de trabalho Note que o R usou a função setwd() que significa “definir diretório de trabalho”. Também é possível usar esta função diretamente no R Script ou no Console, digitando conforme o caminho do diretório. Para saber qual é o diretório de trabalho que está sendo usado pelo R pode-se executar a função getwd(). A saída no Console mostrará o diretório de trabalho usado, portanto é recomendado que se faça isso no início da sessão para verificar se há ou não necessidade de modificar o diretório. 4.5 Projeto Uma funcionalidade importante do RStudio é a possibilidade de se criar projetos. Um projeto nada mais é do que uma pasta no seu computador. Nessa pasta, estarão todos os arquivos que serão usados ou criados na sua análise. A principal razão de se utilizar projetos é simplesmente organização. Com eles, fica muito mais fácil importar conjunto de dados para dentro do R, criar análises reprodutíveis e compartilhar o trabalho realizado. Ao se começar uma nova análise, é interessante criar um Novo Projeto. Para isso, clicar File &gt; New Project ou clicar no menu que está na parte superior, à direita, Project (none) &gt; New Project…. Abrirá a janela da Figura 4.10. Figura4.10: Assistente de novo projeto. Clique em New Directory para criar um novo diretório. Por exemplo, para as aulas de Bioestatística, pode-se criar um diretório com o nome de bioestatistica (evite usar acentos, maiúsculas ou caracteres especiais) ou qualquer outro nome. Quaisquer documentos Excel ou arquivos de texto associados podem ser salvos nesta nova pasta e facilmente acessados R, indo ao menu Project (none) &gt; Open Project…. A partir daí, é possível realizar análises de dados ou produzir visualizações com seus dados importados. Quando um projeto estiver aberto no RStudio, o seu nome aparecerá no canto superior direito da tela. Na aba Files, aparecerão todos os arquivos contidos no projeto. Quando se clica no nome do projeto, abre um menu que torna muito fácil a navegação pelos projetos existentes. Basta clicar em qualquer um deles para trocar de projeto, isto é, deixar de trabalhar em uma análise e começar a trabalhar em outra. 4.6 O R como calculadora O R pode ser utilizado para uma série de operações matemáticas desde as mais simples às mais complexas. Para isso, basta digitar no Console ou no R Script, usando os operadores. 4.6.1 Operadores Operadores são usados para realizar operações com variáveis e valores. Operadores aritméticos No R, você pode usar operadores aritméticos para realizar operações matemáticas comuns. 10 + 5 # Adição ## [1] 15 10 - 5 # Subtração ## [1] 5 10 * 5 # Multiplicação ## [1] 50 10 / 5 # Divisão ## [1] 2 10 ^ 5 # Potência ## [1] 1e+05 10 %% 3 # Divisão modular (divisão com resto) ## [1] 1 10 %/% 3 # Divisão inteiro ## [1] 3 Observe que o R repete a operação e coloca em baixo o resultado precedido por [1]. O resultado da operação de exponenciação é exibido como notação científica, onde \\(e+05\\) significa \\(10^5\\). Operadores de atribuição Operadores de atribuição são usados para atribuir valores a variáveis, como será visto na seção 4.7, adiante. Operadores de comparação São usados para comparar dois valores. # Igualdade 3 == 3 ## [1] TRUE 3 == 4 ## [1] FALSE # Não igual (diferente) 3 != 4 ## [1] TRUE # Maior 6 &gt; 3 ## [1] TRUE # Menor 3 &lt; 4 ## [1] TRUE # Maior ou igual 5 &gt;= 3 ## [1] TRUE # Menor ou igual 3 &lt;= 4 ## [1] TRUE Observe que, na linguagem R, o sinal de igualdade é escrito com duplo \\(=\\). Operadores lógicos Operadores lógicos são usados para combinar declarações condicionais: # Conjunção lógica E, retorna TRUE se ambos elementos são verdadeiros 6 == 6 &amp; 7 == 8 ## [1] FALSE # Conjunção lógica E, retorna TRUE se ambos elementos são verdadeiros 2 * 3 &amp;&amp; 1 * 6 ## [1] TRUE # Conjunção lógica OU, retorna TRUE se um dos elementos é verdadeiro (2 * 2) | sqrt(16) ## [1] TRUE 6 == 6 | 7 == 8 ## [1] TRUE # Conjunção lógica NÃO, retorna FALSE se o elemento é verdadeiro !6==6 ## [1] FALSE !2==4 ## [1] TRUE # Operador lógico que verifica se um elemento pertence a um conjunto (%in%) pares &lt;- c(0, 2, 4, 6, 8, 10) 5 %in% pares ## [1] FALSE Outros operadores # Logarítmo natural (base e) log (10) ## [1] 2.302585 # Logarítmo base 10 log10 (10) ## [1] 1 # Raiz quadrada sqrt (81) ## [1] 9 # Resultado absoluto abs (3 - 6) ## [1] 3 4.7 Objetos O R permite salvar valores dentro de um objeto. Os objetos são criados utilizando o operador de atribuição (&lt;-). Para digitar este operador, basta teclar o sinal menor que (&lt;), seguido de hífen (-) , sem espaços. Existe um atalho que é pressionar (Alt) \\(+\\) (-). O símbolo \\(=\\) pode ser usado no lugar de &lt;-, mas não é recomendado. Objeto é um pequeno espaço na memória do computador onde o R armazenará um valor ou o resultado de um comando, utilizando um nome arbitrariamente definido. Tudo criado pelo R pode se constituir em um objeto, por exemplo: uma variável, uma operação aritmética, um gráfico, uma matriz ou um modelo estatístico. Através de um objeto torna-se simples acessar os dados armazenados na memória. Ao criar um objeto, se faz uma declaração. Isto significa que se está afirmando que uma determinada operação aritmética irá, agora, tornar-se um objeto que irá armazenar um determinado valor. As declarações são feitas uma em cada linha do R Script. Os objetos devem receber um nome e é obrigatório que ele comece por uma letra (ou um ponto) e não é permitido o uso do hífen. Pode-se usar o ponto e underlines para separar palavras. Deve ser evitado o uso de nomes que sejam de objetos do sistema, ou outros objetos já criados, funções ou constantes. Por exemplo, não deve ser utilizado: c, q, r, s, t, C, D, F, I, T, diff, exp, log, mean, pi, range, rank, var, NA, NaN, NULL, FALSE, TRUE, break, else, if, break, function, in, while que devem ser reservados, pois têm significados especiais. Quando se usa um objeto com o nome pi, ele assumirá outro valor diferente de 3,141593. Preservando este nome, toda vez que usarmos a palavra pi, o R assume o valor pré-estabelecido. Além disso, o R faz a diferença entre letras maiúsculas e minúsculas. Ou seja, soma é um objeto diferente de Soma e ambos são diferentes de SOMA. Para exibir o conteúdo de um objeto, basta digitar seu nome no R Script ou no Console e executar. Em análises mais extensas, verificar se já há um objeto com o mesmo nome, pois seus valores serão substituídos ao executar o novo objeto. Para saber se já existe um objeto com o nome definido, digite as primeiras letras do objeto criado e o R Studio listará, usando a sua função de autocompletar, tudo que começar com essas letras no arquivo. Assim ficará fácil verificar se já existe um objeto com o nome desejado. No comando abaixo, é criado um objeto que receberá a soma de dez números, utilizando a função sum(). O objeto foi denominado de soma. Para exibir o valor contido no objeto soma, é necessário digitar soma no R Script ou Console e executar: soma &lt;- sum (2, 3, 12, 15, 21, 4, 8, 7, 13, 21) soma ## [1] 106 4.8 Funções A função é uma orientação aoRpara que ele execute algum procedimento específico, por isso, em geral, têm nomes sugestivos do que elas realizam. Por exemplo, a função mean () realiza a média aritmética de uma série de números colocados entre parênteses. O resultado, como regra geral, deve ser colocado em um objeto que será armazenado na memória do computador. Esta série de números pode antes ser armazenada por um objeto, nomeado dadose, posteriormente, se usa a função mean()com este objeto dados. O resultado da função mean, exibido no Console, será recebido por outro objeto media_dados que será colocado na memória do computador. dados &lt;- c(3, 5, 7, 9, 6, 7) media_dados &lt;- mean(dados) media_dados ## [1] 6.166667 As funções podem ser criadas pelo pesquisador, de acordo com as suas necessidades. Entretanto, na maioria das vezes, elas são encontradas prontas, fazendo parte de um pacote. Pacotes contêm muitas funções que para serem executadas necessitam que estes estejam instalados e carregados. As funções para exercerem a sua ação devem receber dentro delas (entre parênteses) os argumentos que elas exigem. Os argumentos de uma função são sempre separados por vírgulas. Para se saber quais argumentos necessários para uma determinada função basta consultar a ajuda, onde se encontrará a documentação da mesma. Para isso basta digitar no Console, no caso da função mean(), help(mean) ou ?mean: help(mean) O resultado deste comando aparecerá na aba Help, na parte inferior, à direita (Figura 4.11: Figura4.11: Ajuda para Média Aritmética. Os principais argumentos da função mean() são: x \\(\\longrightarrow\\) vetor numérico trim \\(\\longrightarrow\\) fração das observações (varia de 0 a 0,5) extraída de cada extremidade de x para calcular a média aparada na.rm \\(\\longrightarrow\\) valor lógico (TRUE ou FALSE) que indicam se os valores ausentes (NA) devem ser removidos antes que o cálculo continue Este último argumento é muito importante quando, na sequência de valores existe algum não informado ou inexistente. No R, els são denominados de valores ausentes (missing values) e denotados por NA (Not Available). Por exemplo, em uma coleta de uma série de valores, correspondentes ao peso de 15 recém-nascidos, havendo a “falta” de um dos registros, ao calcular a média com a função mean(), ela retornará NA. pesoRN &lt;- c (3340,3345,3750,3650,3220,4070,NA,3970,3060,3180, 2865,2815,3245,2051,2630) mean (pesoRN) ## [1] NA Colocando o argumento na.rm = TRUE, para remover os valores faltantes, a função retornará a média aritmética sem este valor: mean (pesoRN, na.rm = TRUE) ## [1] 3227.929 4.8.1 Criando funções No R, é possível criar funções pessoais que podem simplificar um código e, eventualmente, diminuir o tempo de execução das análises. Fórmula geral As funções têm uma fórmula geral: nome_da_funcao &lt;- function (x){transformar x} Por exemplo, a área de um circulo é igual a \\(\\pi\\times raio^2\\). Para calcular a área do círculo, pode-se criar uma função que faça este trabalho: area.circ &lt;- function(r){ area &lt;- pi*r^2 return(area) } Ao executar essa função, é possível usá-la para calcular a área de um círculo, cujo raio é igual a 5 cm: r = 5 area.circ(5) ## [1] 78.53982 Outros exemplos O Indice de Massa Corporal é igual ao peso (kg) dividido pela \\(altura^2\\), em metros. Uma função para fazer este cálculo é: imc &lt;- function(peso, altura){ res &lt;- peso/altura^2 return(res) } Logo, o IMC de um indivíduo que tenha 67 kg e 1,7 m é: peso &lt;- 67 altura &lt;- 1.70 imc(67, 1.70) ## [1] 23.18339 Ativação de uma função criada Para ativar uma função previamente criada, usa-se a função nativa source (). O argumento desta função é o caminho (no exemplo, é o diretório do autor) onde se encontra a função buscada, por exemplo, a função imc() criada acima: source(&#39;C:/Users/petro/Dropbox/Estatistica/Bioestatistica_usando_R/Funcoes/imc.R&#39;) 4.9 Classes São os atributos de um objeto e o seu conhecimento é de suma importância. A partir do conhecimento do tipo de classe que as funções sabem o que extamente fazer com um objeto. Por exemplo, não é possivel somar duas letras e se for feita a tentativa de somar “a” e “b”, ORretorna um erro: Error in “a” + “b”: non-numeric argument to binary operator . No R, os textos são escritos entre aspas simples ou duplas. As aspas servem para diferenciar nomes (objetos, funções, pacotes) de textos (letras e palavras). Os textos são muito comuns em variáveis categóricas e são popularmente chamados de strings ou character. Alé desta classe, o R tem outras classes básicas que são a numeric e a logical. Um objeto de qualquer uma dessas classes é chamado de objeto atômico. Esse nome se deve ao fato de essas classes não se misturarem (56). Para saber qual o tipo de classe que um objeto pertence, basta usar a função class (). idade &lt;- c(3, 5, 7, 9, 6, 7) class (idade) ## [1] &quot;numeric&quot; nome &lt;- c(&quot;Pedro&quot;, &quot;Maria&quot;, &quot;Margarida&quot;, &quot;Alice&quot;, &quot;João&quot;, &quot;Luís&quot;) class(nome) ## [1] &quot;character&quot; 4.10 Vetores Um vetor é uma variável com um ou mais valores do mesmo tipo. Por exemplo, o número de filhos em 10 famílias foi 4, 5, 3, 2, 2, 1, 2, 1, 3 e 2. O vetor nomeado de n.filhos é um objeto numérico de comprimento = 10. A maneira mais fácil de criar um vetor em R é concatenar (ligar) os 10 valores, usando a função concatenar c() assim: n.filhos &lt;- c(4, 5, 3, 2, 2, 1, 2, 1, 3, 2) n.filhos ## [1] 4 5 3 2 2 1 2 1 3 2 Como os vetores são conjuntos indexados, pode-se dizer que cada valor dentro de um vetor tem uma posição. Essa posição é dada pela ordem em que os elementos foram colocados no momento em que o vetor foi criado. Isso nos permite acessar individualmente cada valor de um vetor (56). Para acessar um determinado valor, basta colocar a posição do mesmo entre colchetes [ ]. Se há interesse em conhecer o número de filhos da quinta família, procede-se da seguinte forma: n.filhos[5] ## [1] 2 Se houver tentativa de acessar um valor inexixtente, o R retorna NA. n.filhos[11] ## [1] NA Se houver necessidade de excluir um dos elementos, basta colocar entre colchetes a posição do mesmo com sinal negativo. Por exemplo, para excluir o valor correspondente a sexta família, usa-se: n.filhos[-6] ## [1] 4 5 3 2 2 2 1 3 2 Observa-se que o valor 1 foi excluído da série de elementos. Quando são colocados elementos em um vetor que pertençam a classes diferentes, o R promove o que se denomina de coerção, pois o vetor pode ter apenas uma classe de objeto. Dessa forma, as classes mais fortes reprimem as mais fracas. Por exemplo, sempre que for misturado números e texto em um vetor, os números serão considerados como texto: vetor &lt;- c(12, 15, 4, 6, &quot;A&quot;, &quot;D&quot;) vetor ## [1] &quot;12&quot; &quot;15&quot; &quot;4&quot; &quot;6&quot; &quot;A&quot; &quot;D&quot; Observe que, agora, todos os elementos do vetor passaram a ser textos e, porisso, estão entre aspas. 4.10.1 Tipos de vetores Dado um vetor, pode-se determinar seu tipo com typeof(), ou verificar se é um tipo específico com uma das funções: is.character(), ’is.double(),is.integer(),is.logical( )`. n.filhos &lt;- c(4, 5, 3, 2, 2, 1, 2, 1, 3, 2) typeof(n.filhos) ## [1] &quot;double&quot; is.numeric(n.filhos) ## [1] TRUE As expressões do tipo character devem aparecer entre aspas duplas ou simples. Os números no R são geralmente tratados como objetos numéricos (números reais de dupla precisão). Mesmo números inteiros são tratados como numéricos. Para fazer um número inteiro ser tratado como objeto inteiro, deve-se utilizar a letra L após o número. Os valores lógicos (ou booleanos) são TRUE ou FALSE. T ou F também são aceitos. n.filhos &lt;- c(4L, 5L, 3L, 2L, 2L, 1L, 2L, 1L, 3L, 2L) typeof(n.filhos) ## [1] &quot;integer&quot; is.numeric(n.filhos) ## [1] TRUE is.double(n.filhos) ## [1] FALSE nomes &lt;- c(&#39;Maria&#39;, &#39;João&#39;, &#39;Manuel&#39;, &#39;Petronio&#39;, &#39;José&#39;) typeof(nomes) ## [1] &quot;character&quot; is.numeric(nomes) ## [1] FALSE is.double(nomes) ## [1] FALSE altura &lt;- c(1.60, 1.78, 1.55, 1.67, 1.69) typeof(altura) ## [1] &quot;double&quot; is.numeric(altura) ## [1] TRUE is.double(altura) ## [1] TRUE 4.11 Dataframes Dataframes são objetos de dados genéricos de R, usados para armazenar os dados tabulares, onde os dados são organizados de maneira lógica em um formato de linha-e-coluna semelhante ao de uma planilha do Excel. O data frame é uma estrutura bidimensional. Estas dimensões podem ser encontradas com a função dim(). Os Data frames podem ser formados com objetos criados previamente, desde que tenham o mesmo comprimento (57). Abaixo serão criadas algumas variáveis, todas relacionadas ao nascimento de 15 bebês: id &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15) pesoRN &lt;- c (3340,3345,3750,3650,3220,4070,3380,3970,3060,3180, 2865,2815,3245,2051,2630) compRN &lt;- c (50,48,52,48,50,51,50,51,47,47,47,49,51,50,44) sexo &lt;- c (2,2,2,1,1,1,2,1,1,1,2,2,1,1,2) tipoParto &lt;- c (1,1,2,1,2,2,1,2,1,1,1,2,1,1,1) idadeMae &lt;- c (40,19,26,19,32,24,27,20,21,19,23,36,21,23,23) Tem-se um grupo de variáveis isoladas. Seria útil reuni-las em um só objeto, usando a função data.frame(). Este novo objeto receberá o nome de dadosNeonatos. dadosNeonatos &lt;- data.frame (id, pesoRN, compRN, sexo, tipoParto, idadeMae) Ao ser executado o comando retornará um novo objeto da classe data.frame: class (dadosNeonatos) ## [1] &quot;data.frame&quot; Havendo necessidade de acrescentar outra variável no banco de dados dadosNeonatos, por exemplo, os dados da ida ou não dos recém-nascidos para a UTI. Para isso, será atribuido a um vetor, contendo a situação dos 15 recém-nascidos, o nome de utiNeo e para relacioná-lo a uma coluna do dataframe dadosNeonatos, será usado o símbolo $, como mostrado abaixo 7: dadosNeonatos$utiNeo &lt;- c (2,2,2,2,1,2,1,2,2,2,2,1,2,2,2) Para observar o novo banco de dados, pode-se usar a função str() do R base. Digitar no R Script: str (dadosNeonatos) ## &#39;data.frame&#39;: 15 obs. of 7 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## $ pesoRN : num 3340 3345 3750 3650 3220 ... ## $ compRN : num 50 48 52 48 50 51 50 51 47 47 ... ## $ sexo : num 2 2 2 1 1 1 2 1 1 1 ... ## $ tipoParto: num 1 1 2 1 2 2 1 2 1 1 ... ## $ idadeMae : num 40 19 26 19 32 24 27 20 21 19 ... ## $ utiNeo : num 2 2 2 2 1 2 1 2 2 2 ... Observando a saida da função, verifica-se que o dataframe contém 15 linhas e 6 colunas e que todas as variáveis estão como variáveis numéricas , mas as variáveis sexo, tipoParto são variáveis categóricas, bem como a variável utiNeo, acrescentada depois. Há necessidade de fazer uma transformação dessas variáveis. 4.12 Fatores Os fatores, no R, são usados para trabalhar com variáveis categóricas. São variáveis usadas para categorizar e armazenar os dados, tendo um número limitado de valores diferentes. Um fator armazena os dados como um vetor de valores inteiros. O fator em R também é conhecido como uma variável categórica que armazena valores de dados de string e inteiros como níveis. O fator é usado principalmente em modelagem estatística e análise exploratória de dados com R (58). 4.12.1 Criando fatores No data frame dadosNeonatos, criado anteriormente, contém três variáveis (sexo, tipoParto e utiNeo) que estão como variáveis numéricas. É possível, desta forma, realizar operações aritméticas com elas. Isto, obviamente, seria um absurdo. Assim, é necessário transformá-las em fatores. Para isso, é usada a função factor(), nativa do R. Os principais argumentos desta função são: x \\(\\longrightarrow\\) vetor numérico levels \\(\\longrightarrow\\) vetor opcional dos valores que x pode assumir labels \\(\\longrightarrow\\) vetor de caracteres dos rótulos para os níveis, na mesma ordem ordered \\(\\longrightarrow\\) vetor lógico (TRUE ou FALSE). Se TRUE, os níveis dos fatores são assumidos como ordenados No exemplo, as variáveis não têm uma ordem lógica, então, o argumento ordered não será usado. dadosNeonatos$utiNeo &lt;- factor (dadosNeonatos$utiNeo, levels = c(1,2), labels = c(&#39;sim&#39;,&#39;não&#39;)) dadosNeonatos$tipoParto &lt;- factor(dadosNeonatos$tipoParto, levels = c(1,2), labels = c(&quot;normal&quot;,&quot;cesareo&quot;)) dadosNeonatos$sexo &lt;- factor (dadosNeonatos$sexo, levels = c(1,2), labels = c(&quot;M&quot;,&quot;F&quot;)) Após a transformação, executa-se novamente a função str() para ver como ficou o dataframe: str(dadosNeonatos) ## &#39;data.frame&#39;: 15 obs. of 7 variables: ## $ id : num 1 2 3 4 5 6 7 8 9 10 ... ## $ pesoRN : num 3340 3345 3750 3650 3220 ... ## $ compRN : num 50 48 52 48 50 51 50 51 47 47 ... ## $ sexo : Factor w/ 2 levels &quot;M&quot;,&quot;F&quot;: 2 2 2 1 1 1 2 1 1 1 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 1 1 2 1 2 2 1 2 1 1 ... ## $ idadeMae : num 40 19 26 19 32 24 27 20 21 19 ... ## $ utiNeo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 2 2 1 2 1 2 2 2 ... Agora, as três varáveis passaram a ser fatores e as outras mantiveram-se numéricas. Desta forma, é possível trabalhar com ela fazendo, por exemplo, uma contagem da frequência do tipo de parto, usando a função table(): table(dadosNeonatos$tipoParto) ## ## normal cesareo ## 10 5 Ou seja, aproximadamente 70% dos partos desta amostra são normais. 4.12.2 Salvando o dataframe criado O data frame, criado e modificado anteriormente, pode ser salvo para uso posterior no diretório de trabalho. Para isso existe a função save (), fornecendo como argumentos o data frame a ser salvo e o nome do arquivo (file =) entre aspas. Por convenção, esta função salva com a extensão .RData que deve ser digitada, pois o R não a adiciona automaticamente. save(dadosNeonatos, file = &quot;dadosNeonatos.RData&quot;) Este comando colocará o arquivo no diretório de trabalho em uso. Portanto, se o objetivo é salvar em outro local, deve ser informado ao R qual o novo diretório. Para carregar o objeto salvo anteriormente com o comando save (), usa-se a função load (). Se o arquivo a ser lido não estiver no diretório de trabalho da sessão, há necessidade de especificar o caminho até o arquivo: load(&quot;dadosNeonatos.RData&quot;) Ou, indicando o diretório onde está o arquivo: load(&quot;C:/Users/petro/Dropbox/Estatistica/Meus_Livros/Bioestatistica_R/Book/dadosNeonatos.RData&quot;) É possível salvar em outro tipo de extensão como Excel (.xlsx), Valores Separados por Vírgula (.csv), etc. O procedimento é o mesmo, mudando a função. Para salvar em uma extensão .xlsx,utiliza-se a função write_xlsx () do pacote writexl (59): writexl::write_xlsx(dadosNeonatos, &quot;dadosNeonatos.xlsx&quot;) Para salvar com a extensão .csv, usar a função write.csv() ou write.csv2() que faz parte do pacote utils, incluido no R base. A primeira função, usa \".\" para a separação dos decimais e \",\" para separar as variáveis; a segunda função usa \",\" para os decimais e \";\" para separar as variáveis, convenção do Excel para algumas localidades, como o Brasil (60). Portanto, uma maneira de salvar o arquivo é: write.csv2 (dadosNeonatos, &quot;dadosNeonatos.csv&quot;) A instalação para Mac OS X pode ser facilmente obtida em busca do Google. Depois de instalado, o uso do RStudio não difere do Windows↩︎ Versão disponível em 10/04/2023↩︎ A variável criada, utiNeo, possui dois níveis: 1 = sim; 2 = não, referente se o bebê foi ou não para a UTI.↩︎ "],["manipulando-os-dados-no-r-studio.html", "Capítulo 5 Manipulando os dados no R Studio 5.1 Importando dados de outros softwares 5.2 Tibble 5.3 Pacote dplyr 5.4 Manipulação de datas", " Capítulo 5 Manipulando os dados no R Studio 5.1 Importando dados de outros softwares Foi visto. quando estudou-se os dataframes, que é possível inserir dados diretamente no R. Entretanto, se o conjunto de dados for muito extenso, torna-se complicado. Desta forma, é melhor importar os dados de outro software, como o Excel, SPSS, etc. A recomendação é que se construa o banco de dados, por exemplo, no Excel, e, depois, exporte o arquivo em um formato que oRreconheça – .xlsx, .csv, .sav. 5.1.1 Importando dados de um arquivo CSV O formato CSV significa Comma Separated Values, ou seja, é um arquivo de valores separados por vírgula. Esse formato de armazenamento é simples e agrupa informações de arquivos de texto em planilhas. É possível gerar um arquivo .csv, a partir de uma planilha do Excel, usando o menu salvar como e escolher CSV. As funções read.csv() e read.csv2(), incluídas noRbase, podem ser utilizadas para importar arquivos CSV. Existe uma pequena diferença entre elas. Dois argumentos dessas funções têm padrão diferentes em cada uma. São eles: sep (separador de colunas) e dec (separador de decimais). Em read.csv(), o padrão é sep = ”,” e dec = ”.” e em read.csv2() o padrão é sep = “;” e dec = ”,”. Portanto, quando se importa um arquivo .csv, é importante saber qual a sua estrutura. Verificar se os decimais estão separados por ponto ou por vírgula e se as colunas (variáveis), por vírgula ou ponto e vírgula. Para ver isso, basta abrir o arquivo em um bloco de notas (por exemplo, Bloco de Notas do Windows, Notepad ++). Quando se usa o read.csv() há necessidade de informar o separador e o decimal, pois senão ele usará o padrão inglês e o arquivo não será lido. Já com read.csv2(), que usa o padrão brasileiro, não há necessidade de informar aoRqual o separador de colunas e nem o separador dos decimais. Além disso, é necessário saber em que diretório do computador está o arquivo para informar ao comando. Recomenda-se colocar o arquivo na pasta do diretório de trabalho, pois assim basta apenas colocar o nome do arquivo na função de leitura dos dados. Caso contrário, tem-se que se usar todo o caminho (path). Como exemplo, será importado o arquivo dadosNeonatos.csv que se encontra no diretório de trabalho do autor, salvo anteriormente. Para obter o arquivo, clique no link e salve em seu diretório de trabalho. A estrutura deste arquivo mostra que as colunas estão separadas por ponto-e-virgula e, portanto, a leitura dos dados será feita com a função read.csv2() e, como o arquivo está no diretório de trabalho, não há necessidade de informar o diretório completo. Os dados serão colocados em um objeto de nome neonatos 8: neonatos &lt;- read.csv2(&quot;./Arquivos/dadosNeonatos.csv&quot;) Use a função str() para visualizar o conjunto de dados:9 str(neonatos) ## &#39;data.frame&#39;: 15 obs. of 7 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ pesoRN : int 3340 3345 3750 3650 3220 4070 3380 3970 3060 3180 ... ## $ compRN : int 50 48 52 48 50 51 50 51 47 47 ... ## $ sexo : chr &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; ... ## $ tipoParto: chr &quot;normal&quot; &quot;normal&quot; &quot;cesareo&quot; &quot;normal&quot; ... ## $ idadeMae : int 40 19 26 19 32 24 27 20 21 19 ... ## $ utiNeo : chr &quot;não&quot; &quot;não&quot; &quot;não&quot; &quot;não&quot; ... Recentemente, foi desenvolvido o pacote readr, incluído no conjunto de pacotes tidyverse(61), para lidar rapidamente com a leitura de grandes arquivos. O pacote fornece substituições para funções como read.csv(). As funções read_csv() e read_csv2() oferecidas pelo readr são análogas às doRbase. Entretanto, são muito mais rápidas e fornecem mais recursos, como um método compacto para especificar tipos de coluna. Além disso, produzem tibbles (ver adiante, neste mesmo capítulo) que são mais reproduzíveis, pois as funções básicas do R herdam alguns comportamentos do sistema operacional e das variáveis de ambiente, portanto, o código de importação que funciona no seu computador pode não funcionar no de outra pessoa. Para usar a função é necessário instalar e ativar o pacote readr. A função read_csv2() será utilizada para criar um outro objeto de nome recemNascidos, mas o conjunto de dados a ser ativado é o mesmo (dadosNeonatos): library(readr) recemNascidos &lt;- read_csv2(&quot;Arquivos/dadosNeonatos.csv&quot;) ## ℹ Using &quot;&#39;,&#39;&quot; as decimal and &quot;&#39;.&#39;&quot; as grouping mark. Use `read_delim()` for more control. ## Rows: 15 Columns: 7 ## ── Column specification ───────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (3): sexo, tipoParto, utiNeo ## dbl (4): id, pesoRN, compRN, idadeMae ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Quando você executa read_csv2(), ele imprime uma especificação de coluna que fornece o nome e o tipo de cada coluna. Novamente, a função str() mostrará a estrutura do arquivo 10: str(recemNascidos) ## spc_tbl_ [15 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:15] 1 2 3 4 5 6 7 8 9 10 ... ## $ pesoRN : num [1:15] 3340 3345 3750 3650 3220 ... ## $ compRN : num [1:15] 50 48 52 48 50 51 50 51 47 47 ... ## $ sexo : chr [1:15] &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; ... ## $ tipoParto: chr [1:15] &quot;normal&quot; &quot;normal&quot; &quot;cesareo&quot; &quot;normal&quot; ... ## $ idadeMae : num [1:15] 40 19 26 19 32 24 27 20 21 19 ... ## $ utiNeo : chr [1:15] &quot;não&quot; &quot;não&quot; &quot;não&quot; &quot;não&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. pesoRN = col_double(), ## .. compRN = col_double(), ## .. sexo = col_character(), ## .. tipoParto = col_character(), ## .. idadeMae = col_double(), ## .. utiNeo = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 5.1.2 Importando um arquivo do Excel O pacote readxl, pertencente ao conjunto de pacotes do tidyverse, facilita a obtenção de dados do Excel para o R, através da função read_excel(). esta função tem o argumento sheet = , que deve ser usado indicando o número ou o nome da planilha, colocado entre aspas. Este argumento é importante se houver mais de uma planilha, caso contrário, ele é opcional. Para saber os outros argumentos da função, colque o cursor dentro da função e aperte a tecla Tab (Figura 5.1). Isto abrirá um menu com os argumentos: Figura5.1: Argumentos da função para importar arquivos xlsx Será feita a leitura dos mesmos dados, usados na leitura de dados csv, apenas o arquivo agora está no formato .xlsx. Para obter o arquivo, siga os mesmos passos, usados anteriormente. Clique no link e salve em seu diretório de trabalho. Os dados serão atribuídos a um objeto com outro nome (recemNatos): library(readxl) recemNatos &lt;- read_excel(&quot;Arquivos/dadosNeonatos.xlsx&quot;) str(recemNatos) ## tibble [15 × 7] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:15] 1 2 3 4 5 6 7 8 9 10 ... ## $ pesoRN : num [1:15] 3340 3345 3750 3650 3220 ... ## $ compRN : num [1:15] 50 48 52 48 50 51 50 51 47 47 ... ## $ sexo : chr [1:15] &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; ... ## $ tipoParto: chr [1:15] &quot;normal&quot; &quot;normal&quot; &quot;cesareo&quot; &quot;normal&quot; ... ## $ idadeMae : num [1:15] 40 19 26 19 32 24 27 20 21 19 ... ## $ utiNeo : chr [1:15] &quot;não&quot; &quot;não&quot; &quot;não&quot; &quot;não&quot; ... Na Figura 5.1, o duplo dois pontos (::) precedido do nome pacote, no caso readr, especifica a procedência da função usada. Nesta situação, não há necessidade de usar a função library() para carregar o pacote já instalado em um diretório (biblioteca) previamente. 5.1.3 Importando arquivos com o RStudio O RStudio permite importar arquivos sem a necessidade de digitar comandos, que, para alguns podem ser tediosos. Na tela inicial do RStudio, à direita, na parte superior, clique na aba Environment e em Import Dataset. Esta ação abre um menu que permite importar arquivos .csv, Excel, SPSS, etc. Por exemplo, para importar o arquivo dadosNeonatos.xlsx, clicar em From Excel... Abre uma janela com uma caixa de diálogo. Clicar no botão Browse..., localizado em cima à direita, para buscar o arquivo dadosNeonatos.xlsx. Assim que o arquivo for aberto, ele mostra uma preview do arquivo e, em baixo, à direita mostra uma preview do código (Figura 5.2), igual ao digitado anteriormente, que cria um objeto denominado dadosNeonatos, nome do objeto escolhido pelo R, mas pode ser modificado na janela, à esquerda, Import Option em Name, onde pode-se digitar qualquer nome. Após encerrar as escolhas, clicar em Import. É um caminho diferente para fazer o mesmo. Este é um dos fascínios do R! Figura5.2: Importando arquivos do excel com o RStudio. 5.2 Tibble A maneira mais comum de armazenar dados no R é usar data.frames ou tibble. Tibble é um novo tipo de dataframe. É como se fosse um dataframe mais moderno. Ele mantém muitos recursos importantes do data frame original, mas remove muitos dos recursos desatualizados. Os tibbles são outro recurso incrível adicionado ao R por Hadley Wickham, através do tidyverse, conjunto de pacotes que formam um conjunto básico de funções que facilitam a manipulação e representação gráfica dos dados (61). Para saber mais sobre tibble, veja vignette(‘tibbles’). A maioria dos pacotes do R usa dataframes tradicionais, entretanto é possível transformá-los para tibble, usando a função as_tibble(), incluída no pacote tidyr (62). O único propósito deste pacote é simplificar o processo de criação de tidy data(dados organizados). O conceito de tidy data, introduzido por Wickman (63), se refere à estrutura dos dados organizados de maneira que cada linha é uma observação, cada coluna representa variáveis e cada entrada nas células do dataframe são os valores. A transformação de um dataframe tradicional em um tibble, é um procedimento rescomendável, em função da maior flexibilidade destes. Como exemplo deste procedimento, será usado o famoso conjunto de dados da flor iris (64) que fornece as medidas em centímetros das variáveis comprimento e largura da sepala e comprimento e largura da pétala, repectivamente, para 50 flores de cada uma das 3 espécies de íris (Iris setosa, versicolor e virginica). Este conjunto de dados encontra-se no pacote datasets no R base. Para visualizar os dados, será usado a função str(), também do R base, que mostra a estrutura interna de um objeto: str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Observa-se que é um conjunto de dados da classe data.frame, contendo 150 observações de 5 variáveis (colunas). Fazendo a coerção para um tibble, tem-se: library(tidyr) as_tibble(iris) ## # A tibble: 150 × 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ℹ 140 more rows Verifica-se que não houve grandes mudanças, apenas o conjunto de dados está estruturalmente mais organizado, mais flexível. 5.3 Pacote dplyr O pacote dpylr é comumente usado para limpar e trabalhar com dados (65). No nível mais básico, as funções do pacote referem-se a “verbos” de manipulação de dados, como select, filter, mutate, arrange, summarize, entre outros, que permitem encadear várias etapas em algumas linhas de código, como será visto adiante. O pacote dplyr é adequado para trabalhar com um único conjunto de dados, bem como para obter resultados complexos em grandes conjuntos de dados. As funções dplyr são processadas mais rápido do que as funções R base. Para trabalhar na manipulação dos dados serão usados alguns pacotes, já mencionados anteriormente, readxl(66) e dplyr, e o conjunto de dados dadosMater.xlsx. Para obter estes dados, clique aqui e faça o download para o seu diretório de trabalho, como orientado anteriormente. library(readxl) library(dplyr) mater &lt;- read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) A função read_excel() carrega o arquivo e o atribui a um objeto , arbitrariamente, denominado de mater11. as_tibble(mater) ## # A tibble: 1,368 × 30 ## id idadeMae altura peso ganhoPeso anosEst cor eCivil renda fumo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 42 1.65 69.9 3.9 3 2 1 1.45 2 ## 2 2 29 1.66 78 16.5 11 1 2 2.41 2 ## 3 3 19 1.72 81 5 9 2 1 1.93 2 ## 4 4 31 1.55 74 43 5 2 2 1.45 2 ## 5 5 34 1.6 60 15 7 2 2 0.48 2 ## 6 6 29 1.5 60 11.4 8 2 2 0.96 1 ## 7 7 30 1.54 75.5 10.5 4 1 2 1.2 1 ## 8 8 34 1.63 61 9 6 1 2 2.41 2 ## 9 9 17 1.68 57 15 10 1 2 2.17 2 ## 10 10 32 1.5 70 11.4 1 2 2 0.72 2 ## # ℹ 1,358 more rows ## # ℹ 20 more variables: quantFumo &lt;dbl&gt;, prenatal &lt;dbl&gt;, para &lt;dbl&gt;, ## # droga &lt;dbl&gt;, ig &lt;dbl&gt;, tipoParto &lt;dbl&gt;, pesoPla &lt;dbl&gt;, sexo &lt;dbl&gt;, ## # pesoRN &lt;dbl&gt;, compRN &lt;dbl&gt;, pcRN &lt;dbl&gt;, apgar1 &lt;dbl&gt;, apgar5 &lt;dbl&gt;, ## # utiNeo &lt;dbl&gt;, obito &lt;dbl&gt;, hiv &lt;dbl&gt;, sifilis &lt;dbl&gt;, rubeola &lt;dbl&gt;, ## # toxo &lt;dbl&gt;, infCong &lt;dbl&gt; Por padrão, a função retorna as dez primeiras linhas. Além disso, colunas que não couberem na largura da tela serão omitidas. Também são apresentadas a dimensão da tabela e as classes de cada coluna. Observa-se que ele tem 1368 linhas (observações) e 30 colunas (variáveis). Além disso, verifica-se que todas as variáveis estão como numéricas (dbl) e, certamente, algumas, dependendo do objetivo na análise, precisarão ser transformadas. O significado de cada uma das variáveis do arquivo dadosMater.xlsx 12 é mostrado abaixo. id \\(\\longrightarrow\\) identificação do participante idadeMae \\(\\longrightarrow\\) idade da parturiente em anos altura \\(\\longrightarrow\\) altura da parturiente em metros peso \\(\\longrightarrow\\) peso da parturiente em kg ganhoPeso \\(\\longrightarrow\\) aumento de peso durante a gestação anosEst \\(\\longrightarrow\\) anos de estudo completos cor \\(\\longrightarrow\\) cor declarada pela parturiente: 1 = branca; 2 = não branca eCivil \\(\\longrightarrow\\) estado civil: 1 = solteira; 2 = casada ou companheira renda \\(\\longrightarrow\\) renda familiar em salários minimos fumo \\(\\longrightarrow\\) tabagismo: 1 = sim; 2 = não quantFumo \\(\\longrightarrow\\) quantidade de cigarros fumados diariamente prenatal \\(\\longrightarrow\\) realizou pelo menos 6 consultas no pré-natal? 1 = sim; 2 = não para \\(\\longrightarrow\\) número de filhos paridos droga \\(\\longrightarrow\\) drogadição? 1 = sim; 2 = não ig \\(\\longrightarrow\\) idade gestacional em semanas tipoParto \\(\\longrightarrow\\) tipo de parto: 1 = normal; 2 = cesareana pesoPla \\(\\longrightarrow\\) peso da placenta em gramas sexo \\(\\longrightarrow\\) sexo do recém-nascido (RN): 1 = masc; 2 = fem pesoRN \\(\\longrightarrow\\) peso do RN em gramas compRN \\(\\longrightarrow\\) comprimento do RN em cm pcRN \\(\\longrightarrow\\) perímetro cefálico dorecém-nascido em cm apgar1 \\(\\longrightarrow\\) escore de Apgar no primeiro minuto apgar5 \\(\\longrightarrow\\) escore de Apgar no quinto minuto utiNeo \\(\\longrightarrow\\) RN necessitou de terapia intesiva? 1 = sim; 2 = não obito \\(\\longrightarrow\\) obito no período neonatal? 1 = sim; 2 = não hiv \\(\\longrightarrow\\) parturiente portadora de HIV? 1 = sim; 2 = não sifilis \\(\\longrightarrow\\) paruriente portadora de sífilis? 1 = sim; 2 = não rubeola \\(\\longrightarrow\\) paruriente portadora de rubéola? 1 = sim; 2 = não toxo \\(\\longrightarrow\\) paruriente portadora de toxoplasmose? 1 = sim; 2 = não infCong \\(\\longrightarrow\\) paruriente portadora de alguma infecção congênita? 1 = sim; 2 = não 5.3.1 Função select() A função select () é usada para escolher com quais colunas (variáveis) entrarão na análise. Ela recebe os nomes das colunas como argumentos e cria um novo banco de dados usando as colunas selecionadas. A função select () pode ser combinada com outras funções, como filter (). Por exemplo, um novo banco de dados será criado (mater1), contendo as mesmas 1368 linhas, mas apenas com as variáveis idadeMae, altura, peso, anosEst, renda, ig, fumo, pesoRN, sexo. Consulte a ajuda (?select()) para obter maiores informações em relação aos argumentos da função: mater1 &lt;- select(mater, idadeMae, altura, peso, anosEst, renda, ig, tipoParto, fumo, pesoRN, sexo) Para visualizar este novo banco de dados, pode-se usar a função str(): str(mater1) ## tibble [1,368 × 10] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ... ## $ altura : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ... ## $ peso : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ... ## $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ... ## $ renda : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ... ## $ ig : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ... ## $ tipoParto: num [1:1368] 2 2 1 1 2 1 2 1 1 2 ... ## $ fumo : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ... ## $ pesoRN : num [1:1368] 1035 2300 1580 1840 2475 ... ## $ sexo : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ... Como mostrado anteriormente, muitas variáveis numéricas do mater, na realidade, são fatores e necessitam de serem modificadas. Entretanto, das selecionadas, para constituir o novo banco de dados, apenas tipoParto, fumo e sexo necessitam serem transformadas para fator: mater1$tipoParto &lt;- factor(mater1$tipoParto, levels = c(1,2), labels = c(&quot;normal&quot;,&quot;cesareo&quot;)) mater1$fumo &lt;- factor (mater1$fumo, levels = c(1,2), labels = c(&#39;sim&#39;,&#39;não&#39;)) mater1$sexo &lt;- factor (mater1$sexo, levels = c(1,2), labels = c(&quot;masc&quot;,&quot;fem&quot;)) Usando, de novo, a função str(), é possível observar a transformação: str(mater1) ## tibble [1,368 × 10] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ... ## $ altura : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ... ## $ peso : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ... ## $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ... ## $ renda : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ... ## $ ig : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 2 2 1 1 2 1 2 1 1 2 ... ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 2 2 2 1 1 2 2 2 ... ## $ pesoRN : num [1:1368] 1035 2300 1580 1840 2475 ... ## $ sexo : Factor w/ 2 levels &quot;masc&quot;,&quot;fem&quot;: 2 2 2 2 2 2 2 2 2 2 ... Se houver necessidade de se excluir alguma variável (coluna), basta colocar o sinal de subtração (-) antes do nome da variável: mater2 &lt;- select(mater1, -altura) str(mater2) ## tibble [1,368 × 9] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ... ## $ peso : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ... ## $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ... ## $ renda : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ... ## $ ig : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 2 2 1 1 2 1 2 1 1 2 ... ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 2 2 2 1 1 2 2 2 ... ## $ pesoRN : num [1:1368] 1035 2300 1580 1840 2475 ... ## $ sexo : Factor w/ 2 levels &quot;masc&quot;,&quot;fem&quot;: 2 2 2 2 2 2 2 2 2 2 ... 5.3.2 Função filter() A função filter() é usada para criar um subconjunto de dados que obedeçam determinadas condições lógicas: &amp; (e), | (ou) e ! (não). Por exemplo: y &amp; !x \\(\\longrightarrow\\) seleciona y e não x x &amp; !y \\(\\longrightarrow\\) seleciona x e não y x | !x \\(\\longrightarrow\\) seleciona x ou y x &amp; !x \\(\\longrightarrow\\) seleciona x e y Um recém-nascido é dito a termo quando a duração da gestação é igual a 37 a 42 semanas incompletas. Se quisermos extrair do banco de dados mater1 os recém-nascidos a termo, pode-se usar a função filter(): mater3 &lt;- filter (mater1, ig&gt;=37 &amp; ig&lt;42) Para exibir o resultado, execute a função str(): str(mater3) ## tibble [1,085 × 10] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:1085] 28 31 27 28 18 28 22 28 25 14 ... ## $ altura : num [1:1085] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ... ## $ peso : num [1:1085] 48.5 65 60 47 65.5 72 65 74 70 56.7 ... ## $ anosEst : num [1:1085] 6 5 8 8 7 11 6 5 9 6 ... ## $ renda : num [1:1085] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ... ## $ ig : num [1:1085] 37 37 37 38 39 39 39 39 39 39 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 1 2 2 1 1 2 2 1 1 1 ... ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 1 2 1 1 2 2 2 2 ... ## $ pesoRN : num [1:1085] 3285 3100 3100 2800 3270 ... ## $ sexo : Factor w/ 2 levels &quot;masc&quot;,&quot;fem&quot;: 1 1 1 1 1 1 1 1 1 1 ... Observe que, agora, o conjunto de dados mater3 tem 1085 linhas, número de recém-nascidos a termo do banco de dados original mater (1368). Logo, os recém nascidos a termo correspondem a 79.3% dos nascimentos, nesta maternidade. Outro exemplo Para selecionar apenas os meninos, nascidos a termo, codificados como \"masc\", procede-se da seguinte maneira13: meninos &lt;- filter (mater3, sexo == &#39;masc&#39;) str(meninos) ## tibble [592 × 10] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:592] 28 31 27 28 18 28 22 28 25 14 ... ## $ altura : num [1:592] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ... ## $ peso : num [1:592] 48.5 65 60 47 65.5 72 65 74 70 56.7 ... ## $ anosEst : num [1:592] 6 5 8 8 7 11 6 5 9 6 ... ## $ renda : num [1:592] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ... ## $ ig : num [1:592] 37 37 37 38 39 39 39 39 39 39 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 1 2 2 1 1 2 2 1 1 1 ... ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 1 2 1 1 2 2 2 2 ... ## $ pesoRN : num [1:592] 3285 3100 3100 2800 3270 ... ## $ sexo : Factor w/ 2 levels &quot;masc&quot;,&quot;fem&quot;: 1 1 1 1 1 1 1 1 1 1 ... O banco de dados meninos é constituídos por 592 meninos. Isto representa 43.3% dos nascimentos. Uma outra maneira de se fazer o mesmo é usar a função grepl(), dentro da função filter (). Ela é usada para pesquisar a correspondência de padrões. No código a seguir, pesquisa-se os registros em que a variável sexo contém “fem”, correspondentes às meninas. meninas &lt;- filter (mater3, grepl(&quot;fem&quot;, sexo)) str(meninas) ## tibble [493 × 10] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:493] 17 30 27 28 17 21 28 19 24 43 ... ## $ altura : num [1:493] 1.65 1.6 1.53 1.4 1.55 1.52 1.58 1.55 1.72 1.6 ... ## $ peso : num [1:493] 60 54 43.5 60 78 52 50 60.5 60 53 ... ## $ anosEst : num [1:493] 7 5 11 8 10 11 11 8 8 4 ... ## $ renda : num [1:493] 1.92 1.92 1.93 2.17 4.82 0.96 4.82 1.69 0.96 1.92 ... ## $ ig : num [1:493] 38 39 39 38 40 38 37 38 40 39 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 1 2 1 1 2 1 1 1 1 1 ... ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 2 2 1 2 1 1 1 2 ... ## $ pesoRN : num [1:493] 3165 3150 2980 3095 3020 ... ## $ sexo : Factor w/ 2 levels &quot;masc&quot;,&quot;fem&quot;: 2 2 2 2 2 2 2 2 2 2 ... 5.3.3 Função mutate() Esta função tem a finalidade de computar ou anexar uma ou mais colunas (variáveis) novas. O Índice de Massa Corporal (IMC) é igual a \\[IMC = \\frac {peso}{altura ^2}\\]. O peso deve ser expreso em kg e a altura em metros. Para acrescentar este indicador no banco de dados mater1 , se fará uso da função mutate(), nomeando a nova variável de imc: Será acrescentado a variável imc, no banco de dados mater1, usando a função mutate(), nomenando essa variável de imc: mater1 &lt;- mutate(mater1, imc = peso/altura^2) str (mater1) ## tibble [1,368 × 11] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ... ## $ altura : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ... ## $ peso : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ... ## $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ... ## $ renda : num [1:1368] 1.45 2.41 1.93 1.45 0.48 0.96 1.2 2.41 2.17 0.72 ... ## $ ig : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 2 2 1 1 2 1 2 1 1 2 ... ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 2 2 2 1 1 2 2 2 ... ## $ pesoRN : num [1:1368] 1035 2300 1580 1840 2475 ... ## $ sexo : Factor w/ 2 levels &quot;masc&quot;,&quot;fem&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ imc : num [1:1368] 25.7 28.3 27.4 30.8 23.4 ... 5.3.4 Função sample_n() Função usada para selecionar de forma aleatória linhas de um dataframe. Sempre consulte a ajuda (?sample_n) para obter informações das funções. Os seus argumentos básicos são: tbl \\(\\longrightarrow\\) dataframe size \\(\\longrightarrow\\) número de linhas para selecionar replace \\(\\longrightarrow\\) amostra com ou sem reposição?. Padrão = FALSE Uma mostra de 20 neonatos do banco de dados meninos, pode ser selecionada, usando a função sample_n: meninos1 &lt;- sample_n(meninos, 20) Assim, selecionou-se uma amostra de 20 meninos do banco de dados de recém-nascidos a termo. Pode-se, fazer um resumo da variável peso do recém-nascido (meninos1$pesoRN) para ver como ela se comporta, usando a função summary(): summary(meninos1$pesoRN) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2720 3074 3295 3357 3524 4315 É importante mencionar que toda vez que esta função for executada ela irá gerar uma amostra diferente. Então, por exemplo, não se deve esperar que a média dos pesos dos recém-nascidos de amostras diferentes sejam iguais. meninos2 &lt;- sample_n(meninos, 20) summary(meninos2$pesoRN) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2745 3155 3405 3345 3510 3980 No capítulo 9 sobre Distribuições Amostrais, este assunto voltará à cena. As funções sample_n() estão com os dias contados, pois foram substituídas por slice_sample() do conjunto de funções que acompanham a função slice() 5.3.5 Função slice() Esta função é usada para selecionar um subconjunto linhas com base em seus locais inteiros. Permite selecionar, remover e duplicar linhas. Para os exemplos, será usado o conjunto de dados meninos, criado acima. Os argumentos básicos da função slice() são .data ⟶ dataframe .by ⟶ seleciona por grupo n, prop ⟶ fornecer n, o número de linhas, ou prop, a proporção de linhas a serem selecionadas. … Exemplpos: # Selecionando a linha 10 slice(.data = meninos, 10) ## # A tibble: 1 × 10 ## idadeMae altura peso anosEst renda ig tipoParto fumo pesoRN sexo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 14 1.51 56.7 6 1.92 39 normal não 3200 masc # Selecionando varias linhas, por exemplo, de 1 a 5 slice(.data = meninos, 1:5) ## # A tibble: 5 × 10 ## idadeMae altura peso anosEst renda ig tipoParto fumo pesoRN sexo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 28 1.5 48.5 6 3.13 37 normal não 3285 masc ## 2 31 1.55 65 5 0.72 37 cesareo não 3100 masc ## 3 27 1.6 60 8 2.41 37 cesareo sim 3100 masc ## 4 28 1.58 47 8 1.69 38 normal não 2800 masc ## 5 18 1.76 65.5 7 1.93 39 normal sim 3270 masc É possível também selecionar linhas de acordo com determinado grupo. Como grupo, será usada a variável fumo que tem dois níveis (sim e não). Será selecionada uma linha de cada grupo: slice(.data = meninos, .by = fumo, 1) ## # A tibble: 2 × 10 ## idadeMae altura peso anosEst renda ig tipoParto fumo pesoRN sexo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 28 1.5 48.5 6 3.13 37 normal não 3285 masc ## 2 27 1.6 60 8 2.41 37 cesareo sim 3100 masc Para separa por grupo , é possível usar a função group_by(), incluída no pacote dplyr. A melhor solução, neste caso, para aplicar diversas funções de manipulação em um dataframe é aplicar o operador pipe: %&gt;%. No final desta seção, será discutido com mais detalhes este operador. meninos %&gt;% group_by(fumo) %&gt;% slice (1) ## # A tibble: 2 × 10 ## # Groups: fumo [2] ## idadeMae altura peso anosEst renda ig tipoParto fumo pesoRN sexo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 27 1.6 60 8 2.41 37 cesareo sim 3100 masc ## 2 28 1.5 48.5 6 3.13 37 normal não 3285 masc A saída desse código é a mesma vista anteriormente. A vantagem é termos escrito o código na ordem em que as funções são aplicadas. Portanto, é um código mais legível. 5.3.5.1 Funções auxiliares da função slice() A função slice() é acompanhada por várias funções auxiliares para casos de uso comuns: * slice_head() e slice_tail() selecionam a primeira ou a última linha; * slice_sample() seleciona linhas aleatoriamente, substitui a sample_n(); * slice_min() e slice_max() selecionam linhas com valores mais altos ou mais baixos de uma variável. Por exemplo, para selecionar uma amostra aleatória de 20 meninos do conjunto de dados meninos, pode-se usar a função slice_sample(): meninos3 &lt;- slice_sample(.data = meninos, n = 20) str (meninos3) ## tibble [20 × 10] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:20] 18 27 24 28 24 25 23 28 20 31 ... ## $ altura : num [1:20] 1.65 1.63 1.45 1.68 1.66 1.55 1.62 1.55 1.67 1.76 ... ## $ peso : num [1:20] 60 71 48 50 76.5 53 45 53 69 77 ... ## $ anosEst : num [1:20] 6 8 5 9 11 11 9 11 10 8 ... ## $ renda : num [1:20] 1.92 1.2 1.92 2.41 4.1 1.92 1.33 2.41 1.93 1.69 ... ## $ ig : num [1:20] 40 39 39 39 40 40 39 37 40 39 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 1 2 1 1 2 2 1 1 2 2 ... ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 2 1 2 2 2 1 1 2 ... ## $ pesoRN : num [1:20] 3150 2925 3175 3240 3340 ... ## $ sexo : Factor w/ 2 levels &quot;masc&quot;,&quot;fem&quot;: 1 1 1 1 1 1 1 1 1 1 ... Como se observa no código, o argumento n da função deve ser nomeado explicitamente (n = 20). É possível também usar o argumento prop, colocando a proporção de linhas que se deseja selecionar. Se o objetivo é selecionar 10% da amostra, coloca-se o argumento como prop = 0.10. Como o dataframe meninos contém 592 casos, serão selecionados 59. Além disso, a função permite selecionar por grupos com o argumento by =. meninos4 &lt;- slice_sample(.data = meninos, prop = 0.10) str (meninos4) ## tibble [59 × 10] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:59] 37 23 30 14 39 22 34 24 31 19 ... ## $ altura : num [1:59] 1.55 1.7 1.53 1.51 1.73 1.62 1.55 1.66 1.55 1.69 ... ## $ peso : num [1:59] 70 60 57 56.7 72 69 75 76.5 53 65 ... ## $ anosEst : num [1:59] 8 7 8 6 6 8 1 11 6 8 ... ## $ renda : num [1:59] 1.93 1.92 1.69 1.92 3.13 2.17 1.92 4.1 1.2 2.89 ... ## $ ig : num [1:59] 37 40 40 39 37 39 41 40 38 40 ... ## $ tipoParto: Factor w/ 2 levels &quot;normal&quot;,&quot;cesareo&quot;: 1 2 2 1 1 1 1 2 1 2 ... ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 2 2 2 1 2 2 1 2 ... ## $ pesoRN : num [1:59] 3840 3110 3275 3200 3355 ... ## $ sexo : Factor w/ 2 levels &quot;masc&quot;,&quot;fem&quot;: 1 1 1 1 1 1 1 1 1 1 ... Para maiores informações em relação a estas funções consulte a ajuda (?slice()). 5.3.6 Função arrange() Ordena as linhas pelos valores de uma coluna de forma ascendente ou descentente. Voltando a amostra meninos1, será colocado em ordem crescente a variável pesoRN: arrange(meninos1, pesoRN) ## # A tibble: 20 × 10 ## idadeMae altura peso anosEst renda ig tipoParto fumo pesoRN sexo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 31 1.55 44 5 1.76 39 cesareo não 2720 masc ## 2 37 1.5 60 3 1.45 39 normal não 2795 masc ## 3 16 1.61 59 8 2.41 38 normal não 2980 masc ## 4 22 1.65 59.5 4 1.92 38 cesareo não 2980 masc ## 5 19 1.5 49 8 2.41 40 normal sim 3055 masc ## 6 27 1.68 50 8 1.93 41 normal não 3080 masc ## 7 31 1.48 58 8 3.61 39 cesareo não 3130 masc ## 8 43 1.65 60 8 1.45 39 normal não 3205 masc ## 9 25 1.48 54 6 1.2 39 cesareo sim 3290 masc ## 10 20 1.6 49 8 2.41 41 normal não 3290 masc ## 11 43 1.65 63 8 4.82 40 normal não 3300 masc ## 12 22 1.55 46 6 1.92 39 normal não 3315 masc ## 13 34 1.67 70 10 2.89 38 cesareo não 3320 masc ## 14 29 1.63 66 3 2.07 40 normal não 3340 masc ## 15 14 1.68 64 6 0.96 40 normal sim 3455 masc ## 16 19 1.68 64 11 4.1 39 cesareo não 3730 masc ## 17 19 1.62 62 8 2.17 40 normal não 3780 masc ## 18 26 1.55 116 4 2.41 40 cesareo não 3840 masc ## 19 27 1.57 60 9 3.61 39 cesareo não 4215 masc ## 20 22 1.6 63 11 2.17 40 cesareo não 4315 masc Para a ordem decrescente, colocar a função desc(), dentro da função arrange() arrange(meninos1, desc(pesoRN)) ## # A tibble: 20 × 10 ## idadeMae altura peso anosEst renda ig tipoParto fumo pesoRN sexo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 22 1.6 63 11 2.17 40 cesareo não 4315 masc ## 2 27 1.57 60 9 3.61 39 cesareo não 4215 masc ## 3 26 1.55 116 4 2.41 40 cesareo não 3840 masc ## 4 19 1.62 62 8 2.17 40 normal não 3780 masc ## 5 19 1.68 64 11 4.1 39 cesareo não 3730 masc ## 6 14 1.68 64 6 0.96 40 normal sim 3455 masc ## 7 29 1.63 66 3 2.07 40 normal não 3340 masc ## 8 34 1.67 70 10 2.89 38 cesareo não 3320 masc ## 9 22 1.55 46 6 1.92 39 normal não 3315 masc ## 10 43 1.65 63 8 4.82 40 normal não 3300 masc ## 11 25 1.48 54 6 1.2 39 cesareo sim 3290 masc ## 12 20 1.6 49 8 2.41 41 normal não 3290 masc ## 13 43 1.65 60 8 1.45 39 normal não 3205 masc ## 14 31 1.48 58 8 3.61 39 cesareo não 3130 masc ## 15 27 1.68 50 8 1.93 41 normal não 3080 masc ## 16 19 1.5 49 8 2.41 40 normal sim 3055 masc ## 17 16 1.61 59 8 2.41 38 normal não 2980 masc ## 18 22 1.65 59.5 4 1.92 38 cesareo não 2980 masc ## 19 37 1.5 60 3 1.45 39 normal não 2795 masc ## 20 31 1.55 44 5 1.76 39 cesareo não 2720 masc 5.3.7 Função count() Permite contar rapidamente os valores únicos de uma ou mais variáveis. Esta função tem os seguintes argumentos: x \\(\\longrightarrow\\) dataframe wt \\(\\longrightarrow\\) pode ser NULL (padrão) ou uma variável sort \\(\\longrightarrow\\) padrão = FALSE; se TRUE, mostrará os maiores grupos no topo name \\(\\longrightarrow\\) O nome da nova coluna na saída; padrão = NULL Quando o argumento name é omitido, a função retorna n como nome padrão. Usando o dataframe mater1, a função count() irá contar o número de parturientes fumantes, variável dicotômica fumo: count(mater1, fumo) ## # A tibble: 2 × 2 ## fumo n ## &lt;fct&gt; &lt;int&gt; ## 1 sim 301 ## 2 não 1067 5.3.8 Operador pipe %&gt;% O operador pipe %&gt;% pode ser usado para inserir um valor ou um objeto no primeiro argumento de uma função. Ele pode ser acionado digitando %&gt;% ou usando o atalho ctrl+shift+M. Em vez de passar o argumento para a função separadamente, é possível escrever o valor ou objeto e, em seguida, usar o pipe para convertê-lo como o argumento da função na mesma linha. Funciona como se o pipe jogasse o objeto dentro da função seguinte. Vários comando foram utilizados, manipulando o banco de dados mater. Alguns orocedimentos, serão mostrados, usando, agora, o operador pipe. Em primeiro lugar, serão selecionadas algumas colunas do dataframe mater; adicionada a variável imc; selecionado os recém-nascidos a termo do sexo masculino, que no banco de dados mater está codificado como 1. Tudo em um só comando! meusDados &lt;- mater %&gt;% select(idadeMae, altura, peso, anosEst, renda, ig, tipoParto, fumo, pesoRN, sexo) %&gt;% mutate(imc = peso/altura^2) %&gt;% filter (ig&gt;=37 &amp; ig&lt;42, sexo == 1) str(meusDados) ## tibble [592 × 11] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:592] 28 31 27 28 18 28 22 28 25 14 ... ## $ altura : num [1:592] 1.5 1.55 1.6 1.58 1.76 1.63 1.54 1.55 1.56 1.51 ... ## $ peso : num [1:592] 48.5 65 60 47 65.5 72 65 74 70 56.7 ... ## $ anosEst : num [1:592] 6 5 8 8 7 11 6 5 9 6 ... ## $ renda : num [1:592] 3.13 0.72 2.41 1.69 1.93 1.92 2.65 2.53 0.48 1.92 ... ## $ ig : num [1:592] 37 37 37 38 39 39 39 39 39 39 ... ## $ tipoParto: num [1:592] 1 2 2 1 1 2 2 1 1 1 ... ## $ fumo : num [1:592] 2 2 1 2 1 1 2 2 2 2 ... ## $ pesoRN : num [1:592] 3285 3100 3100 2800 3270 ... ## $ sexo : num [1:592] 1 1 1 1 1 1 1 1 1 1 ... ## $ imc : num [1:592] 21.6 27.1 23.4 18.8 21.1 ... Observe que o dataframe mater aparece apenas no início e, como ele é um argumento das outras funções, ele é transferido, automaticamente, não havendo necessidade de escrever dentro na função. No final, retornará um novo dataframe que foi colocado em um objeto, denominado meuDados, o qual contém informações de todos os 592 meninos, nascidos a termo e de suas mães. 5.4 Manipulação de datas Originalmente, todos os que trabalham com o R queixavam-se de como era frustrante trabalhar com datas. Era um processo que causava grande perda de tempo nas análises. O pacote lubridate foi criado (67) para simplificar ao máximo a leitura de datas e extração de informações dessas datas. Quando o lubridate é carregado aparece uma mensagem, avisando que alguns nomes de funções também estão contidas no pacote base do R. library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union Para evitar confusões e verificar que as funções corretas estão sendo usadas, usa-se o duplo dois pontos (::) antes do nome da função, precedido do nome do pacote, por exemplo: lubridate::date(). Para obter a data atual ou a data-hora, você pode usar as funções today()14 ou now()15: today() ## [1] &quot;2023-11-16&quot; now() ## [1] &quot;2023-11-16 15:19:06 -03&quot; 5.4.1 Convertendo strings ou caractere para data Para converter string ou caracteres em datas, basta executar funções específicas adequadas aos dados. Elas determinam automaticamente o formato quando você especifica a ordem do componente. Para usá-los, identifique a ordem em que o ano, o mês e o dia aparecem em suas datas e, em seguida, organize “y”, “m” e “d” na mesma ordem. Isso lhe dá o nome da função lubridate que analisará a data. Por exemplo, suponhamos a data de 25/12/2022: natal &lt;- &quot;25/12/2022&quot; natal ## [1] &quot;25/12/2022&quot; Aparentemente, o R aceitou a informação como uma data. Entretanto, se for verificada a classe do objeto, tem-se: class(natal) ## [1] &quot;character&quot; Estando como caractere, esta data não poderá ser usada em operações com datas, pois necessitaria estar como uma classe date. Para converte-la em data, usa-se a função dmy(): natal &lt;- dmy(natal) natal ## [1] &quot;2022-12-25&quot; class(natal) ## [1] &quot;Date&quot; Dessa forma, a data, agora está sendo reconhecida pelo R como date. Como se verifica, é sempre importante verificar a classe da data. Às vezes, observa-se datas escritas com o mês abreviado, como 25/dez/2022. O procedimento é o mesmo minha.data &lt;- &quot;25/dez/2022&quot; class (minha.data) ## [1] &quot;character&quot; minha.data &lt;- dmy(minha.data) class (minha.data) ## [1] &quot;Date&quot; Se além da data, houver necessidade de especificar o horário, basta usar dmy_h(), dmy_hm() e dmy_hms(). Se for usado o padrão americano, pode ser usado ymd(). O lubridate traz diversas funções para extrair os componentes de um objeto da classe date. second() \\(\\rightarrow\\) extrai os segundos. minute() \\(\\rightarrow\\) extrai os minutos. hour() \\(\\rightarrow\\) extrai a hora. wday() \\(\\rightarrow\\) extrai o dia da semana. mday() \\(\\rightarrow\\) extrai o dia do mês. month() \\(\\rightarrow\\) extrai o mês. year() \\(\\rightarrow\\) extrai o ano. Por exemplo, usando a data de nascimento (dn) do autor: dn &lt;- dmy(&quot;04/10/1947&quot;) year(dn) ## [1] 1947 Para acrescentar um horário ao objeto data de nascimento (dn)^[UTC = Coordinated Universal Time}: hour(dn) &lt;- 04 dn ## [1] &quot;1947-10-04 04:00:00 UTC&quot; 5.4.2 Juntando componenetes de datas Para juntar componentes de datas e horas, pode-se utilizar as funções make_date() e make_datetime(). Em muitos arquivos, os componentes da data estão em colunas diferentes e há necessidade de juntá-los em uma única coluna para compor a data: minha.data &lt;- make_date(year = 1947, month = 10, day = 4) minha.data ## [1] &quot;1947-10-04&quot; Para juntar ano, mês, dia, hora e minuto: minha.data &lt;- make_datetime(year = 1947, month = 10, day = 04, hour = 04 , min = 30, sec = 15) minha.data ## [1] &quot;1947-10-04 04:30:15 UTC&quot; 5.4.3 Extraindo componentes de datas Quando temos objetos do tipo POSIXt16 podemos extrair componentes ou elementos deles. Para isso são usadas algumas funções específicas do pacote lubridate como mostrado a seguir. data &lt;- now() year(data) # Extrai o ano ## [1] 2023 month(data) # Extrai o mês ## [1] 11 week(data) # Extrai a semana ## [1] 46 day(data) # Extrai o dia ## [1] 16 minute(data) # Extrai o minuto ## [1] 19 second(data) # Extrai o segundo ## [1] 6.452173 Para verificar o número de dias tem em um determinado mês, usa-se a função days_in_month(): data1 &lt;- dmy(&quot;25/02/2000&quot;) days_in_month(data1) ## Feb ## 29 5.4.4 Operações com datas O pacote lubridate possui funções de duração e de período para manipular as datas. As funções de duração calculam o número de segundos em um determinado num determinado número de dias. As funções de duração não levam em consideração anos bissextos e horário de verão, enquanto as funções de período consideram esses fatores. ddays (1) # Número de segundos em 1 dia ## [1] &quot;86400s (~1 days)&quot; dhours (1) # Número de segundos em 1 hora ## [1] &quot;3600s (~1 hours)&quot; dminutes (1) # Número de segundos em 1 minuto ## [1] &quot;60s (~1 minutes)&quot; days (5) # Cria um período de 5 dias ## [1] &quot;5d 0H 0M 0S&quot; weeks (5) # Cria um período de 5 semanas ## [1] &quot;35d 0H 0M 0S&quot; Suponhamos que se necessita saber em qual dia cairá após acrescentarmos 5 semanas à data1 (25/02/2000), criada acima: data1 + weeks (5) ## [1] &quot;2000-03-31&quot; Adicionando 1 ano à data1 (25/02/2000) com uma função de duração, tem-se: data1 + dyears (1) ## [1] &quot;2001-02-24 06:00:00 UTC&quot; Se for adicionado um ano à mesma data, mas agora com uma função de período, tem-se: data1 + years (1) ## [1] &quot;2001-02-25&quot; Pode-se definir um intervalo de tempo a partir de uma data inicial e uma data final. A sintaxe para calcular um intervalo é dada por data.inicial %--% data.final .Suponha que uma gestante tenha como data da sua última menstruação em 04/10/2022. O bebê nasceu em 30/06/2023. Qual o intervalo em dias? data.inicial &lt;- dmy(&quot;04/10/2022&quot;) data.final &lt;- dmy(&quot;30/06/2023&quot;) tempo &lt;- (data.inicial %--% data.final) %/% days(1) tempo ## [1] 269 Ou seja a gestação durou 269 dias, constituindo-se em um parto a termo, entre 37 (259 dias) e 42 semanas (294 dias). Para mais informações sobre o lubridate, consulte a ajuda do pacote ou o capítulo 16 do livro R for Data Science, Hadley Wickman e Garrett Grolemund, 2017 (https://r4ds.had.co.nz/index.html) . A mudança do nome do dataframe de dadosNeonatos para neonatos é desnecessária. Foi realizada apenas por questões didáticas.↩︎ Observe, na saída, que na variável utiNeo aparece palavras com acentuação (“não”). Às vezes, ao abrir o arquivo com a função read.csv2(), pode acontecer de esta palavra aparecer, por exemplo, como: “n3o”. Louco, não é? Se ocorrer isso, use, após o nome do arquivo e separado por vírgula, o argumento fileEncoding = “latin1”. Dessa forma, o erro será corrigido.↩︎ Da mesma maneira, como acontece com a função read.csv2(), a função equivalente do readr pode retornar erro na leitura de palavras com acento. Para corrigir isso, usa-se o argumento locale (encoding = \"latin1\")↩︎ ATENÇÃO: Volta-se a insistir, o comando para carregar o conjunto de dados somente funciona, sem colocar o caminho (path) completo, se tudo está sendo realizado no diretório de trabalho.↩︎ Conjunto de dados coletados na maternidade-escola do Hospital Geral de Caxias do Sul↩︎ Lembrar que o sinal de igualdade, no R, é duplo =↩︎ Equivalente ao Sys.Date()que acessa a data do sistema operacional.↩︎ Equivalente ao Sys.time() que acessa a data e hora do sistema operacional.↩︎ POSIXt é uma classe de objetos do R que representa datas e horas. POSIXt significa Portable Operating System Interface for Unix Time, que é um padrão para medir o tempo em segundos desde 1 de janeiro de 1970. Existem duas formas internas de implementar POSIXt: POSIXct e POSIXlt. POSIXct armazena os segundos desde a época UNIX e POSIXlt armazena uma lista de dia, mês, ano, hora, minuto, segundo, etc.↩︎ "],["descrevendo-os-dados.html", "Capítulo 6 Descrevendo os dados 6.1 Pacotes necessários neste capítulo 6.2 Dados brutos 6.3 Medidas resumidoras 6.4 Tabelas 6.5 Gráficos 6.6 Introdução ao ggplot2", " Capítulo 6 Descrevendo os dados Nos relatórios ou artigos científicos, a comunicação dos resultados é feita através da combinação de medidas resumidoras e visualização dos dados por meio de tabelas e gráficos. 6.1 Pacotes necessários neste capítulo Para trabalhar neste capítulo, serão necessários os seguintes pacotes. pacman::p_load(dplyr, ggplot2, ggpubr, ggsci, grDevices, Hmisc, kableExtra, knitr, plotrix, readxl, scales) 6.2 Dados brutos Habitualmente, costuma-se armazenar os dados em bancos de dados (dataframes ou tibbles). Entretanto, eles estão registrados de forma aleatória e não classificada. Ao se visualizar um dataframe, é difícil responder perguntas em relação a qualquer variável, principalmente, em grandes banco de dados. Eles se constituem uma lista, um rol de valores colocados na ordem em que foram obtidos. Parecem um jogo de quebra cabeça antes de serem organizados e resumidos! São denominados de dados brutos ou, também, de dados não agrupados. 6.3 Medidas resumidoras 6.3.1 Dados usados nesta seção Para a demonstração prática será usado um conjunto de dados que é uma amostra aleatória de 15 recém-nascidos do banco de dados dadosMater.xlsx (ver Seção 5.3), extraída com a função slice_sample() do pacote dplyr. Cada vez que este comando for reproduzido, retornará uma nova série de 15 valores diferentes do anterior. Para tornar o código reproduzível, retornando o mesmo conjunto de valores, deve-se usar uma “semente” (seed), usando a função set.seed(), cujo argumento é um número que identificará a série gerada. Após extrair a amostra, será selecionada as variáveis que serão usadas nesta seção. A amostra será atribuída a um objeto denominado, dados: set.seed(1234) dados &lt;- readxl::read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) %&gt;% filter(ig&gt;= 37 &amp; ig&lt;42) %&gt;% select(idadeMae, anosEst, pesoRN, apgar1) %&gt;% slice_sample(n=15) str(dados) ## tibble [15 × 4] (S3: tbl_df/tbl/data.frame) ## $ idadeMae: num [1:15] 23 26 18 25 35 19 20 23 26 35 ... ## $ anosEst : num [1:15] 5 9 7 10 11 9 10 11 4 5 ... ## $ pesoRN : num [1:15] 3190 3715 2555 3795 2970 ... ## $ apgar1 : num [1:15] 9 7 9 6 8 8 8 8 9 9 ... Após a manipulação dos dados, tem-se um tibble de 15 linhas e quatro colunas. 6.3.2 Introdução Sempre que se está diante de um novo conjunto de dados para analisar, uma das primeiras tarefas é encontrar maneiras de resumir os dados de forma compacta e fácil de entender. Este processo se constitui na estatística descritiva que compreende métodos de tabulação, gráficos e resumo dos dados. Nesta seção, serão estudas as medidas de resumo dos dados. As maneiras mais usadas para resumir o conjunto de dados são: * Primeiro, um valor em torno do qual os dados têm uma tendência para se reunir ou se agrupar, denominado de medida sumária de localização ou medida de tendência central. * Em segundo lugar, um valor que mede o grau em que os dados se dispersam, denominado de medida de dispersão ou variabilidade 6.3.3 Medidas de tendência central 6.3.3.1 Média A média ( \\(\\overline{x}\\) ) é a mais usada medida de tendência central. Ela é calculada pela razão entre a soma de todas as observações de um conjunto de dados e o total de observações. A média é mais adequada para medidas numéricas simétricas. \\[ \\overline{x}= \\frac{\\sum(x_1 + x_2 + x_3 + ... + x_n)}{n} \\] O R base possui uma função para o cálculo da média – mean(). Ela já foi apresentada no Capítulo 4, seção sobre funções, onde foi mostrado os seus argumentos. Se a variável analisada contiver algum valor ausente (missing), deve-se usar o argumento na.rm = TRUE, para removê-los, pois, caso contrário, a função retorna um resultado como NA (Not Available). Para evitar transtornos, recomenta-se usar sempre o argumento. mean (dados$pesoRN, na.rm = TRUE) ## [1] 3307.667 6.3.3.2 Mediana A mediana (Md) representa o valor central em uma série ordenada de valores. Assim, metade dos valores será igual ou menor que o valor mediano e a outra metade igual ou maior do que ele. No R, usa-se a função median() para calcular o valor da mediana. Vanos utilizar a variável dados$apgar1. Como o Apgar é um escore, a medida resumidora mais adequada é a mediana. median (dados$apgar1, na.rm = TRUE) ## [1] 8 6.3.3.3 Moda Moda (Mo) é o valor que ocorre com maior frequência em um conjunto de dados. Tem o menor nível de sofisticação. É usada primariamente para dados nominais porque há simplesmente contagem dos valores. Ao contrário das outras medidas de tendência central, a moda não informa nada sobre a ordem das variáveis ou variação dentro das variáveis. O R não tem uma função embutida padrão para calcular a moda. Portanto, há necessidade de ser criada uma função de usuário para calcular a moda. moda &lt;- function(x) { z &lt;- table(as.vector(x)) names(z)[z == max(z)]} UEsta função moda() deve ser salva em seu diretório, na pasta das suas funções próprias. Quando necessário ela pode ser acessada como foi visto na seção sobre criação de funções. Será calculada a moda da variável dados$apgar1. moda (dados$apgar1) ## [1] &quot;8&quot; 6.3.3.4 Quantil Uma medida de localização bastante utilizada são os quantis que são pontos estabelecidos em intervalos regulares que dividem a amostra em subconjuntos iguais. Se estes subconjuntos são em número de 100, são denominados de percentis; se são em número de 10, são os decis e em número de 4, são os quartis. A função apropriada no R para obter o quantil é quantile(). Para determinar os três quartis do peso dos recém-nascidos (mater15$pexoRN), usa-se: quantile (dados$pesoRN, c (0.25, 0.50, 0.75)) ## 25% 50% 75% ## 3077.5 3500.0 3677.5 Observe que o percentil 50º é igual a mediana. O percentil 75º é o ponto do conjunto de dados onde 75% dos recém-nascidos têm um peso inferior a 3677.5g e 25% está acima deste valor. 6.3.3.5 Média aparada As médias aparadas são estimadores robustos da tendência central. Para calcular uma média aparada, é removida uma quantidade predeterminada de observações em cada lado de uma distribuição e realizada a média das observações restantes. Um exemplo de média aparada é a própria mediana. A base R tem como calcular a média aparada acrescentando o argumento trim =, proporção a ser aparada. Se for aparado 20%, usa-se trim = 0.2. isto significa que serão removidos 20% dos dados dos dois extremos. No caso da amostra de 15 recém-nascidos, serão removidos três valores mais baixos e três valores mais altos, passando a mostra a ter 9 valores, e a média aparada será a média destes 9 valores. O comando para obter a média aparada é: mean (dados$pesoRN, na.rm = TRUE, trim = 0.20) ## [1] 3397.222 6.3.4 Medidas de Dispersão 6.3.4.1 Amplitude A amplitude de um grupo de medições é definida como a diferença entre a maior observação e a menor. No conjunto de dados dos pesos dos recém-nascidos, a amplitude pode ser obtida, no R, com a função range(), que retorna o valor mínimo e o máximo. range (dados$pesoRN, na.rm = TRUE) ## [1] 2285 3795 6.3.4.2 Intervalo Interquartil A intervalo interquartil (IIQ), também conhecido como amplitude interquartil (AIQ) é uma forma de média aparada. É simplesmente a diferença entre o terceiro e o primeiro quartil, ou seja, a diferença entre o percentil 75 e o percentil 25. Considere a escolaridade (dados$anosEst), anos de estudos completos. Os percentis 25 e 75 são obtidos, usando a função quantile(), vista acima, ou com a função summary() , que retorna os valores mínimo, primeiro quartil, mediana, média terceiro quartil e máximo. quantile (dados$anosEst, c(0.25,0.75)) ## 25% 75% ## 6.5 11.0 summary(dados$anosEst) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 4.000 6.500 9.000 8.467 11.000 11.000 Portanto, o IIQ está entre 6,5 a 11 anos de estudo ou, 11 – 6,5 = 4,5 anos de estudos completos. Em outras palavras, 50% das mulheres desta amostra têm de 6 a 8 anos de estudo. O R possui uma função específica para calcular o intervalo interquartil, denominada IQR() e incluída no pacote stats, pertencente ao R base. Ela possui os seguintes argumentos: x \\(\\longrightarrow\\) Representa o vetor numérico; na.rm \\(\\longrightarrow\\) Este assume um valor lógico, TRUE ou FALSE, indicando se os valores ausentes devem ser removidos ou não; type \\(\\longrightarrow\\) Representa um número inteiro selecionando um dos muitos algoritmos de quantil. Este é um parâmetro opcional. IQR(dados$anosEst, na.rm = TRUE) ## [1] 4.5 6.3.4.3 Variância e Desvio Padrão A variância e o desvio padrão fornecem uma indicação de quão aglomerados em torno da média os dados de uma amostra estão. Estes tipos de medidas representam desvios (erros)da média. Quando se verifica o desvio de cada valor (x) em relação à média \\(\\overline{x}\\), os desvios positivos se anulam com os negativos, resultando em uma soma igual a zero. A consequência deste fato é que não é possível resumir os desvios numa única medida de variabilidade. Para se chegar a uma medida de variabilidade há necessidade de se eliminar os sinais, antes de somar todos os desvios em relação à média. Uma maneira de se fazer isso é elevar todas as diferenças ao quadrado. Assim, se obtém o desvio em relação à média elevado ao quadrado. A soma destes valores é denominada de Soma dos Quadrados (SQ) dos Desvios ou Soma dos Erros ao Quadrado. Se o interesse é apenas saber o erro ou desvio médio, divide-se por n (tamanho da amostra). No entanto, em geral o interesse se concentra em usar o desvio ou erro na amostra para estimar o erro na população. Dessa maneira, divide-se a Soma dos Quadrados por \\(n-1\\). Essa medida é conhecida como variância (\\(s^2\\)). O divisor, \\(n – 1\\), é denominado de graus de liberdade (gl) associados à variância. Os graus de liberdade representam o número de desvios que estão livres para variar. É um conceito de difícil explicação. Suponha uma maternidade há 50 anos atrás, quando não havia alojamento conjunto. Nessa época era comum os recém-nascidos normais ficarem em um berçário. A cada horário de amamentação eles eram levados para os quartos de suas mães para mamar. Posteriormente, eram trazidos para o berçário e colocados nos berços até a próxima mamada. Suponha que, em um determinado momento, havia 15 bebês e que, no berçário, existiam 15 berços (postos) para colocá-los durante o intervalo das mamadas. Quando o primeiro recém-nascido chega, a enfermeira poderá escolher qualquer um dos berços para o colocar. Depois, quando o próximo recém-nascido chegar, ela terá 14 opções de escolha, pois um dos berços está ocupado. Ainda existe uma boa liberdade de escolha. No entanto, à medida que os recém-nascidos forem sendo trazidos para o berçário, chegará a um ponto em que 14 berços estarão ocupados. Agora, a enfermeira não terá liberdade de escolha, pois só resta um berço. Nesse exemplo existem 14 graus de liberdade. Para o último recém-nascido não houve liberdade de escolha (68). Portanto, os graus de liberdade são iguais ao tamanho da amostra menos um (\\(n-1\\)). A variância é a razão entre a soma dos quadrados e as observações realizadas menos um. \\[ s^2= \\frac{\\sum(x_i - \\overline{x})^2}{n-1} \\] No R existem as funções sd() e var(), também incluídas no R base, que facilmente calculam essas medidas de dispersão. Usando a variável dados$pesoRN, tem-se: var(dados$pesoRN, na.rm =TRUE) ## [1] 208310.2 O desvio padrão é a raiz quadrada da variância: \\(s = \\sqrt var\\) sqrt (var(dados$pesoRN)) ## [1] 456.4102 Ou, sd (dados$pesoRN, na.rm = TRUE) ## [1] 456.4102 A variância e desvio padrão são medidas de variabilidade. Representam quão bem a média representa os dados. Informa se ela está funcionando bem como modelo. Pequenos desvios padrão mostram que existe pouca variabilidade nos dados, que eles se aproximam da média. Quando existe um grande desvio padrão, a média não é muito precisa para representar os dados. O desvio padrão, além de medir a precisão com que a média representa os dados, também informa sobre o formato dos dados e por isso é uma medida de dispersão. Em uma amostra onde desvio padrão é pequeno, os dados se agrupam próximo a média e o formato da distribuição fica mais pontiagudo (curva em azul, 6.1). Nesse caso a média representa bem os dados. Em outra amostra, com a mesma média anterior, mas com os dados mais dispersos entorno da média, o desvio padrão é maior e o formato da distribuição fica achatado (curva verde, na Figura 6.1). Nesse caso a média não é uma boa representação dos dados. Figura6.1: Dispersão dos dados em torno da média. 6.3.4.4 Coeficiente de Variação O desvio padrão por si só tem limitações. Um desvio padrão de duas unidades pode ser considerado pequeno para um conjunto de valores cuja média é 100. Entretanto, se a média for 5, ele se torna muito grande. Além disso, o desvio padrão por ser expresso na mesma unidade dos dados, não permite aplicá-lo na comparação de dois ou mais conjunto de dados que têm unidades diferentes. Para eliminar essas limitações, é possível caracterizar a dispersão ou variabilidade dos dados em termos relativos, usando uma medida denominada Coeficiente de Variação (CV), também conhecido como como Desvio Padrão Relativo ou Coeficiente de Variação de Pearson. É expresso, em geral como uma porcentagem, sendo definido como a razão do desvio padrão pela média: \\[ CV = \\frac{s}{\\overline{x}} \\] Multiplicando o valor da equação por 100 tem-se o CV percentual. O R não possui uma função específica para calcular o CV. Foi criada uma função específica para isso,já multiplicada por 100. coef_var &lt;- function (valores) { (sd(valores, na.rm=TRUE) / mean(valores, na.rm=TRUE))*100} Portanto, o CV da variável dados$pesoRN é igual a: coef_var (dados$pesoRN) ## [1] 13.79855 Se usarmos outra variável do banco de dados, por exemplo, dados$idadeMae, o CV será igual a: coef_var (dados$idadeMae) ## [1] 21.37895 O peso do recem-nascido tem um CV = 13.8 e a idade materna um CV = 21.4, mostrando que esta tem uma maior variabilidade. Quanto menor o desvio padrão, menor o CV e, consequentemente, menor a variabilidade. Um CV \\(\\ge\\) 50%, sugere que a variável tem uma distribuição assimétrica. 6.3.5 Escolha da medida resumidora A seleção da medida de tendência central mais adequada depende de vários fatores, incluindo a natureza dos dados e do propósito da sumarização. O tipo da variável tem substancial influência na escolha da medida de tendência central a ser usada. A moda é mais apropriada para dados nominais e seu uso com variáveis ordinais resulta em uma perda no poder em termos de informação que se poderia obter dos dados. A mediana é mais adequada para variáveis ordinais, embora possa ser usada para variáveis contínuas, especialmente quando a distribuição dos dados é assimétrica. A mediana não deveria ser usada com dados nominais porque os postos assumidos não podem ser obtidos com dados de nível nominal. Finalmente, a média somente deve ser usada com dados contínuos simétricos, se houver assimetria a mediana deve ser preferida. As medidas de dispersão devem estar associadas a uma medida de tendência central. Elas caracterizam a variabilidade dos dados na amostra. Com dados ordinais usar a amplitude ou o intervalo interquartil. O desvio padrão não é apropriado em dados ordinais devido à natureza não numérica destes. Com os dados numéricos deve-se usar o desvio padrão, que utiliza toda a informação nos dados, ou o intervalo interquartil (IIQ). Quando os dados forem simétricos, usar a média acompanhada do desvio padrão, caso contrário, usar a mediana e o IIQ. Não misturar e combinar medidas (22). 6.4 Tabelas A apresentação tabular dos dados é a apresentação das informações por meio de tabelas. Uma tabela é uma forma eficiente de mostrar os dados levantados, facilitando a sua compreensão e interpretação. No R existem muitas maneira de criar tabelas. 6.4.1 Dados usados nesta seção Para mostrar como construir as tabelas, será feita a leitura do conjunto de dados dadosMater.xlsx, mencionado na seção anterior . Como visto, este conjunto de dados contém uma grande quantidade colunas e, para tornar mais fácil a análise, serão selecionadas aquelas utilizadas nesta seção. mater &lt;- readxl::read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) %&gt;% select(idadeMae, altura, peso, anosEst, fumo, para, ig, sexo, pesoRN, compRN, utiNeo) str(mater) ## tibble [1,368 × 11] (S3: tbl_df/tbl/data.frame) ## $ idadeMae: num [1:1368] 42 29 19 31 34 29 30 34 17 32 ... ## $ altura : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ... ## $ peso : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ... ## $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ... ## $ fumo : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ... ## $ para : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ... ## $ ig : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ... ## $ sexo : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ... ## $ pesoRN : num [1:1368] 1035 2300 1580 1840 2475 ... ## $ compRN : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ... ## $ utiNeo : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ... 6.4.2 Tabelas de Frequência 6.4.2.1 Tabela de frequência para dados categóricos Uma maneira concisa que permite observar a variável e extrair informação sobre o seu comportamento, é a utilização de uma tabela de frequência. A tabela de frequência deve ser simples, clara e objetiva, ou seja, não deve ter um volume muito grande de informações. Deve ser autoexplicativa, não deve haver necessidade de ler o texto para entendê-la. A tabela de frequência agrupa os dados por categorias ou classes, contabilizando o número ocorrências em cada categoria. O número de observações em uma determinada classe recebe o nome de frequência absoluta (f). Além da frequência absoluta, costuma aparecer a frequência relativa (fr) que representa a proporção da classe em relação ao número total de observações (n), calculada por \\(fr = \\frac{f}{n}\\), a frequência percentual (fp), obtida pela multiplicação da frequência relativa por 100 e a frequência acumulada, que é a soma de todas as classes até a classe atual, podendo ser frequência acumulada absoluta (F), frequência acumulada relativa (Fr) ou frequência acumulada percentual (Fp). Em uma tabela, os dados são apresentados em colunas verticais indicadoras e linhas horizontais. Nas linhas aparecem as categorias e nas colunas as frequências, constituindo o corpo da tabela. O cabeçalho indica a natureza do conteúdo de cada coluna. No cruzamento das colunas e das linhas, tem-se as caselas ou casas. Existem algumas recomendações na construção de uma tabela de frequência (69): deve ter um título na parte superior que responda as perguntas: “o que? quando? onde?” relativas ao fato estudado; deve ter um rodapé, na parte inferior da tabela, onde se coloca notas necessárias e a fonte dos dados; as colunas externas da tabela devem ser abertas, o emprego de linhas verticais para a separação das colunas no corpo da tabela é opcional; Na parte superior e inferior, as tabelas devem, ser fechadas por linhas horizontais; Nenhuma casela deve ficar vazia, apresentando um número ou um símbolo. Se não se dispuser do dado, colocar reticências … e a presença de um X representa que o dado foi omitido para evitar a identificação. Se os dados forem nominais, a ordenação das categorias é arbitrária, costuma-se colocar em primeiro lugar a maior frequência (Tabela 6.1) , colocando-os em categorias ordenadas (70). Tabela6.1: Distribuição de frequência de drogadição em parturientes do Hospital Geral de Caxias do Sul, RS, 2008. Drogadição f fr fp Fp Não drogaditas 904 0,955 95,5 95.5 Medicamentos 23 0,024 2,4 97.9 Álcool 17 0,018 1,8 99.7 Crack 2 0,002 0,2 99.9 Cocaína 1 0,001 0,1 100 Total 947 1,000 100,0 6.4.2.2 Construção da tabela de frequência Para demonstrar como construir uma tabela de frequência, usando o R, será usada uma variável categórica que não existe no conjunto de dados mater. Esta variável vai ser criada, categorizando a variável numérica idadeMae (idade da parturiente) em três categoria, classicamente, usadas: menores de 20 anos (adolescentes), 20 a 35 anos e maiores de 35 anos. No conjunto mater, a variável idadeMae tem como idade mínima 13 anos e idade máxima 46 anos. A nova variável receberá o nome de categIdade. Para realizar este trabalho de transformação da variável numérica em categórica, será usada a função cut() do pacote R base. Esta função tem vários argumentos: x \\(\\longrightarrow\\) vetor numérico breaks \\(\\longrightarrow\\) vetor numérico de dois ou mais pontos de corte exclusivos ou um único número (maior ou igual a 2) dando o número de intervalos nos quais x deve ser subdividido labels \\(\\longrightarrow\\) rótulos para os níveis das categorias resultante. Por padrão, os rótulos são construídos usando a notação de intervalo \\((a, b]\\) (aberto à esquerda e fechado à direita). include.lowest \\(\\longrightarrow\\) valor lógico, se o menor valor será incluido, ou o maior, se right = TRUE. Padrão = include.lowest=TRUE right \\(\\longrightarrow\\) valor lógico indicando se o intervalo deve ser fechado à direita e aberto a esquerda. Padrão = right = TRUE. ordered_result \\(\\longrightarrow\\) valor lógico indicando se o resultado deve ser um fator ordenado. mater$categIdade &lt;- cut(mater$idadeMae, breaks = c(13, 20, 36, 46), labels = c(&quot;&lt;20a&quot;, &quot;20-35a&quot;, &quot;&gt;35a&quot;), include.lowest = TRUE, right = FALSE, ordered_result =TRUE) Para visualizar se a variável foi criada de maneira adequada, pode-se usar novamente a função str(): str(mater) ## tibble [1,368 × 12] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ... ## $ altura : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ... ## $ peso : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ... ## $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ... ## $ fumo : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ... ## $ para : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ... ## $ ig : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ... ## $ sexo : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ... ## $ pesoRN : num [1:1368] 1035 2300 1580 1840 2475 ... ## $ compRN : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ... ## $ utiNeo : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ... ## $ categIdade: Ord.factor w/ 3 levels &quot;&lt;20a&quot;&lt;&quot;20-35a&quot;&lt;..: 3 2 1 2 2 2 2 2 1 2 ... A nova variável foi gerada de forma correta. Observe que para que isso acontecesse, foi usado right = FALSE, em consequência, o intervalo 13 a 20, incluirá o 13 (menor idade) e excluirá o 20, o intervalo 20 a 36, incluirá o 20 e excluirá o 36 e o último intervalo incluirá o 36 e excluirá o 46, que é o valor mais alto. Em função disso, foi incluído mais um argumento include.lowest=TRUE, para incluir o valor 46. Para se verificar como ficou a distribuição de frequência absoluta, constrói-se uma tabela, inicialmente com a função table(): f_abs &lt;- table (mater$categIdade) f_abs ## ## &lt;20a 20-35a &gt;35a ## 219 992 157 As frequências relativas podem ser obtidas com a função prop.table(), Esta função será usada dentro da função round() para arredondar os valores para 3 dígitos. f_rel &lt;- round(prop.table(f_abs), 3) f_rel ## ## &lt;20a 20-35a &gt;35a ## 0.160 0.725 0.115 Multiplicando por 100 a f_rel, tem-se a frequência percentual f_perc. De novo, a operação será colocada dentro da função round(), arredondando o resultado para dois dígitos. f_perc &lt;- round(f_rel*100, 2) f_perc ## ## &lt;20a 20-35a &gt;35a ## 16.0 72.5 11.5 Para construir uma tabela simples no R, pode-se proceder da seguinte maneira: # Criando as colunas das tabelas com o total de cada uma delas f_abs &lt;- c (f_abs, sum(f_abs)) f_rel &lt;- c (f_rel, sum (f_rel)) f_perc &lt;- c (f_perc, sum (f_perc)) # Criando a tabela inicial com a concatenação das coluna - função cbind() tab1 &lt;- cbind(f_abs, f_rel , f_perc) tab1 ## f_abs f_rel f_perc ## &lt;20a 219 0.160 16.0 ## 20-35a 992 0.725 72.5 ## &gt;35a 157 0.115 11.5 ## 1368 1.000 100.0 Transformando a tab1 em um dataframe, nomeando a linha 4 e renomeando as colunas para ficar com os nomes referidos no início da seção: tab1 &lt;- as.data.frame(tab1) row.names(tab1)[4] &lt;- &quot;Total&quot; colnames(tab1) &lt;- c(&quot;f&quot;, &quot;fr&quot;, &quot;fp&quot;) tab1 ## f fr fp ## &lt;20a 219 0.160 16.0 ## 20-35a 992 0.725 72.5 ## &gt;35a 157 0.115 11.5 ## Total 1368 1.000 100.0 Esta uma tabela simples que serve para visualizar as informações. Não serve para publicações. Para obter uma tabela simples, mas muito mais profissional (Tabela 6.2) pode ser utilizada a função kable() do pacote knitr (71). Esta função possui um grande número de argumentos para personalizar a aparência das tabelas17: x \\(\\rightarrow\\) é um objeto do R, tipicamente uma matiz ou dataframe; format \\(\\rightarrow\\) sequência de caracteres. Os valores possíveis são ”latex”, “html”, “pipe”, “simple”; digits \\(\\rightarrow\\) número máximo de dígitos para colunas numéricas, passado para round(); row.names \\(\\rightarrow\\)lógico: se deve incluir nomes de linhas. Por padrão, os nomes de linhas serão incluídos se rownames(x) não for NULL nem idêntico a 1:nrow(x). col.names \\(\\rightarrow\\) vetor de caracteres de nomes de colunas a serem usados na tabela.; align \\(\\rightarrow\\) alinhamento da coluna: um vetor de caracteres que consiste em “l” (esquerda), “c”’ (centro) e/ou “r” (direita). Por padrão ou se align = NULL, as colunas numéricas são alinhadas à direita e as outras colunas são alinhadas à esquerda.; caption \\(\\rightarrow\\) título da tabela; … \\(\\rightarrow\\) outros argumentos Um pacote adicional, kableExtra (72), permite opções de formatação simples, melhorando o aspecto da tabela, utilizando-se o operador pipe %&gt;%. O pacote kableExtra foi projetado para estender a funcionalidade básica das tabelas produzidas usando knitr::kable(). Podem ser acrescentadas várias das suas funções como kable_styling() ou kable_classic() para especificar estilos à tabela, como extensão da tabela, alinhamento, tipo e tamanho da fonte 18. knitr::kable(tab1, booktabs = TRUE, caption = &quot;Distribuição das puérperas por faixa etária, Hospital Geral de Caxias do Sul, RS, 2008.&quot;, format.args = list(decimal.mark = &quot;,&quot;)) %&gt;% kableExtra::kable_classic_2(full_width = FALSE, html_font = &quot;Cambria&quot;, position = &quot;center&quot;) %&gt;% kableExtra::row_spec(0, bold = TRUE) %&gt;% kableExtra::column_spec(1:4, width = &quot;2.5cm&quot;) Tabela6.2: Distribuição das puérperas por faixa etária, Hospital Geral de Caxias do Sul, RS, 2008. f fr fp &lt;20a 219 0,160 16,0 20-35a 992 0,725 72,5 &gt;35a 157 0,115 11,5 Total 1368 1,000 100,0 6.4.2.3 Tabela de frequência para dados numéricos Como fazer a distribuição de frequência de uma variável contínua sem um critério pré-determinado para as classes? Como exemplo, será usado, agora, o IMC pré-gestacional das parturientes do banco de dados dadosMater.xlsx). Esta variável não existe, tem-se apenas o peso e a altura e, portanto, com estes dados, ela pode ser criada: mater$imc &lt;- round(mater$peso/mater$altura^2, 1) str(mater) ## tibble [1,368 × 13] (S3: tbl_df/tbl/data.frame) ## $ idadeMae : num [1:1368] 42 29 19 31 34 29 30 34 17 32 ... ## $ altura : num [1:1368] 1.65 1.66 1.72 1.55 1.6 1.5 1.54 1.63 1.68 1.5 ... ## $ peso : num [1:1368] 69.9 78 81 74 60 60 75.5 61 57 70 ... ## $ anosEst : num [1:1368] 3 11 9 5 7 8 4 6 10 1 ... ## $ fumo : num [1:1368] 2 2 2 2 2 1 1 2 2 2 ... ## $ para : num [1:1368] 5 0 0 1 2 1 2 1 0 4 ... ## $ ig : num [1:1368] 29 33 33 33 33 33 33 33 34 34 ... ## $ sexo : num [1:1368] 2 2 2 2 2 2 2 2 2 2 ... ## $ pesoRN : num [1:1368] 1035 2300 1580 1840 2475 ... ## $ compRN : num [1:1368] 35.5 45 39 41 47 41 44 44 47 48 ... ## $ utiNeo : num [1:1368] 1 2 1 1 1 1 2 2 1 1 ... ## $ categIdade: Ord.factor w/ 3 levels &quot;&lt;20a&quot;&lt;&quot;20-35a&quot;&lt;..: 3 2 1 2 2 2 2 2 1 2 ... ## $ imc : num [1:1368] 25.7 28.3 27.4 30.8 23.4 26.7 31.8 23 20.2 31.1 ... A variável imc foi criada de forma adequada. Essa variável foi construida por questões didáticas como exercício de manipulação dos dados no R. Bem! Após, isso, para verificar a sua distribuição, segue-se os seguintes passos: Estabelecimento do número de classes (k): Antes, as classes foram estabelecidas de acordo com algum critério. Em geral, quando não há um padrão pré-determinado, o número de classes é estabelecido de acordo com o tamanho da amostra. Este número pode ser escolhido lembrando-se das oscilações que ocorrem nos dados e do interesse do pesquisador em mostrar seus dados. Não existe uma regra totalmente eficiente para determinar o número de classes. É importante ter bom senso, de maneira que seja possível ver como os valores se distribuem. Para a maioria dos dados, é recomendado e 8 a 20 classes, isto é, 8 \\(\\le\\) k \\(\\le\\) 20. Com poucas classes, perde-se precisão e, com muitas classes, a tabela torna-se muito extensa. Baseado na regra de Sturges , é sugerido usar a recomendação da Figura 6.2 (73). Figura6.2: Número de classes baseado em Sturges Para a variável imc, como existem 1368 observações, deve-se usar ao redor de 10 classes. Executando a função nclass.Sturges (), abaixo, o número de classes é igual a: k &lt;- nclass.Sturges (mater$imc) k ## [1] 12 Amplitude e limites das classes: A classe possui um limite inferior e um limite superior. O importante é que os limites dos intervalos sejam mutuamente exclusivos, isto cada valor deve ser representado em um único intervalo. Além disso, os intervalos devem ser exaustivos, isto é, devem conter todos os valores possíveis entre o valor mínimo e o máximo. O recomendado é que as classes sejam homogêneas, ou seja, tenham a mesma amplitude. A amplitude dos valores pode ser obtida com a função range(): amplitude &lt;- range(mater$imc) amplitude ## [1] 11.8 48.7 Usando esta amplitude dos dados, é possível ter a largura (amplitude) das classes (h), usando a diferença entre o mínimo e máximo e divdindo pelo número de clsasses (k): h &lt;- round(diff(amplitude)/k, 0) h ## [1] 3 A fórmula é apenas a diferença absoluta dos limites inferior e superior dividida pelo número de classes, arredondado com o a função round () com 1 dígito decimal. A partir desses dados, é possível construir as classes. A primeira classe será o valor mínimo de 11,8, que pode ser arredondado para 11,8 até 14,8 (11,8 + 3) exclusive; a segunda classe será 14,8 até 17,8 (14,8 + 3) e assim por diante. Construção da tabela: Pode-se construir a tabela, usando a função table() e dentro desta a função cut() e dentro dela a função seq(limite inferior, limite superior, l = número de classes). categImc &lt;- table(cut(mater$imc, rigth = TRUE, include.lowest = TRUE, seq(11.8, 48.7, l = k + 1))) categImc ## ## [11.8,14.9] (14.9,18] (18,21] (21,24.1] (24.1,27.2] (27.2,30.3] ## 2 46 258 480 237 176 ## (30.3,33.3] (33.3,36.4] (36.4,39.5] (39.5,42.6] (42.6,45.6] (45.6,48.7] ## 87 39 22 12 5 4 Preste atenção! Estes comandos que vão gerar a tabela têm o argumento right = TRUE (padrão). Neste caso, ao contrário do comentado anteriormente, onde foi usado right = FALSE, os símbolos aparecem como (] (na tabela) e significa que o limite inferior da classe foi excluído (aberto à esquerda) e o superior foi incluído (fechado à direita). Aqui, também foi introduzido o argumento include.lowest = TRUE para incluir o valor mínimo dos dados (11,8), e a representação gráfica fica []. Olhando a saída do objeto nutriCateg, ela parece pouco esclarecedora e, no caso do IMC, talvez fosse melhor usar outro critério. Como por exemplo o que define o estado nutricional no 1° trimestre de gestação e classifica as gestantes em baixo peso (IMC \\(&lt;\\) 18,5 kg/\\(m^2\\)), peso adequado (18,5 \\(\\le\\) IMC \\(\\le\\) 24,9 kg/\\(m^2\\)), sobrepeso (25,0 \\(\\le\\) IMC \\(\\le\\) 29,9 kg/\\(m^2\\)) e obesidade (IMC \\(\\ge\\) 30 kg/\\(m^2\\)). Assim, é recomendado um ganho de peso total adequado de 12,5 kg a 18 kg para as gestantes classificadas como baixo peso; de 11,5 kg a 16,0 kg para as classificadas como peso adequado; de 7,0 a 11,5 kg nas classificadas com sobrepeso; e de 5,0 a 9,0 kg nas obesas (74). mater$estNutri &lt;- cut(mater$imc, breaks = c(11.8, 18.5, 25, 30, 48.7), labels = c(&quot;Baixo Peso&quot;, &quot;Peso adequado&quot;, &quot;Sobrepeso&quot;, &quot;Obesidade&quot;), include.lowest = TRUE, right = FALSE, ordered_result =TRUE) Isto cria uma nova variável estNutri (estado nutricional), no conjunto de dados mater, com 4 níveis (baixo Peso, Peso adequado, sobrepeso e Obesidade). Desta forma, pode-se construir uma tabela que melhor define este grupo de mulheres quanto ao estado nutricional. f.abs &lt;- table (mater$estNutri) f.rel &lt;- round(prop.table(f.abs), 3) f.perc &lt;- round(f.rel*100, 2) f.abs &lt;- c (f.abs, sum(f.abs)) f.rel &lt;- c (f.rel, sum (f.rel)) f.perc &lt;- c (f.perc, sum (f.perc)) tab2 &lt;- cbind(f.abs, f.rel , f.perc) tab2 &lt;- as.data.frame(tab2) row.names(tab2)[5] &lt;- &quot;Total&quot; colnames(tab2) &lt;- c(&quot;f&quot;, &quot;fr&quot;, &quot;fp&quot;) tab2 ## f fr fp ## Baixo Peso 67 0.049 4.9 ## Peso adequado 791 0.578 57.8 ## Sobrepeso 335 0.245 24.5 ## Obesidade 175 0.128 12.8 ## Total 1368 1.000 100.0 Colocando em um formato mais científico, tem-se uma tabela (Tabela 6.3) bem mais elegante sobre o estado nutricional pré-gestacional: knitr::kable(tab2, booktabs = TRUE, caption = &quot;Estado nutricional pré-gestacional das parturientes, HGCS, 2008.&quot;, format.args = list(decimal.mark = &quot;,&quot;)) %&gt;% kableExtra::kable_classic_2(full_width = TRUE, html_font = &quot;Cambria&quot;, position = &quot;center&quot;) %&gt;% kableExtra::row_spec(0, bold = TRUE) %&gt;% kableExtra::column_spec(1, width = &quot;3.5cm&quot;) %&gt;% kableExtra::column_spec(2:4, width = &quot;2.5cm&quot;) Tabela6.3: Estado nutricional pré-gestacional das parturientes, HGCS, 2008. f fr fp Baixo Peso 67 0,049 4,9 Peso adequado 791 0,578 57,8 Sobrepeso 335 0,245 24,5 Obesidade 175 0,128 12,8 Total 1368 1,000 100,0 6.4.3 Tabelas de contingência As tabelas de contingência, também chamadas tabelas cruzadas, são bastante usadas em estatísticas epidemiológicas para resumir a relação entre duas ou mais variáveis categóricas. Uma tabela de contingência é um tipo especial de tabela de distribuição de frequência, onde duas variáveis são mostradas simultaneamente. Por exemplo, um pesquisador pode estar interessado em saber se o hábito de fumar na gestação aumenta o risco de o recém-nascido precisar de cuidados intensivos. Existem duas variáveis fumo (fumo na gestação) e utiNeo (necessidade de cuidados intensivos neonatais) no banco de dados dadosMater.xlsx. Cada uma dessas variáveis tem duas alternativas, sim e não, por isso a tabela de cruzamento é denominada tabela de contingência 2 x 2. No arquivo, estão registradas como variáveis numéricas , 1 e 2, e devem ser transformadas para fatores (1 = sim e 2 = não)19, usando a função factor(). mater$fumo &lt;- factor (mater$fumo, ordered = TRUE, levels = c (1,2), labels = c (&quot;sim&quot;, &quot;não&quot;)) mater$utiNeo &lt;- factor (mater$utiNeo, ordered = TRUE, levels = c (1,2), labels = c (&quot;sim&quot;, &quot;não&quot;)) Basta agora, usar a função with() junto com a função table(variável da linha, variável das colunas). Por convenção, costuma-se colocar a variável explicativa ou explanatória nas linhas (fumo) e o desfecho nas colunas (utiNeo): tabFumo &lt;- with(data = mater, table(fumo, utiNeo)) tabFumo ## utiNeo ## fumo sim não ## sim 71 230 ## não 204 863 Para ter a soma das margens, usar a função addmargins (tabela, margin = c (1,2), FUN = sum) do pacote stats, incluído na instalação básica do R. A função adiciona a soma das linhas (1) e das colunas (2) às margens da tabela (tabFumo). addmargins (tabFumo, margin = c(1,2), FUN = sum) ## Margins computed over dimensions ## in the following order: ## 1: fumo ## 2: utiNeo ## utiNeo ## fumo sim não sum ## sim 71 230 301 ## não 204 863 1067 ## sum 275 1093 1368 Observando a tabela de contingência, verifica-se que a proporção entre as gestantes fumantes de internação na UTI neonatal foi 71/301 = 0,236 e entre as não fumantes foi de 204/1067 = 0,191. Para verificar se esta diferença ocorreu por acaso ou ela é significativa, há necessidade de se realizar um teste de hipótese, o qui-quadrado, que será visto no Capítulo 16. 6.5 Gráficos Para descrever os dados e visualizar o que está acontecendo, recomenda-se utilizar um gráfico adequado. O que é adequado depende principalmente do tipo de dados, bem como das características particulares do que se quer explorar. Além disso, um gráfico em um relatório sempre é um fator de “impacto”. Ou seja, pode ter um efeito positivo no leitor ou fazê-lo abandonar a leitura. Finalmente, um gráfico de frequência pode ser utilizado para ilustrar, explicar uma situação complexa onde palavras ou uma tabela podem ser confusos, extensos ou de outro modo insuficiente. Por outro lado, deve-se evitar usar gráficos onde poucas palavras expressam claramente o que se quer mostrar. Aconselha-se que, ao analisar os dados, é importante inspecioná-los como se fossem uma imagem, uma fotografia, ver como eles se parecem, qual o seu aspecto, e só então pensar em interpretar os aspectos vitais da estatística (75). O R básico fornece uma grande variedade de funções para visualizar dados, elas de uma maneira relativamente simples permitem a construção de gráficos que facilitam a interpretação tanto de variáveis categórica como contínuas. Para gráficos mais sofisticados existe um pacote denominado ggplot2 (76). Este pacote é uma ferramenta extremamente versátil. É um pouco mais complexo e exige mais tempo para dominá-lo, mas, uma vez que se aprenda o básico sobre ele, oferece uma estrutura extremamente flexível para exibir os dados . Inicialmente, serão usadas as funções do R básico e,posteriormente, será feita uma introdução ao ggplot2 (seção 6.6). 6.5.1 Gráfico de setores Também conhecido como gráfico de pizza. Cada segmento (fatia) do gráfico de pizza deve ser proporcional à frequência da categoria que representa. A desvantagem do gráfico de pizza é que ele só pode representar uma variável, portanto, há necessidade de um gráfico separado para cada variável que se deseja representar. Além disso, um gráfico de pizza pode perder clareza se ele é usado para representar mais do que quatro ou cinco categorias. Na maioria das vezes, em um artigo ou relatório não há necessidade de se usar este tipo de gráfico. As tabelas são muito melhores. Segundo Edward Tufte, professor emérito de estatística, design gráfico e economia política na Universidade de Yale, o único gráfico pior do que um gráfico de pizza são vários deles (77)! Ele é usado mais no mundo dos negócios. Como regra, não use gráfico de pizza! Em uma consulta, entre estudantes de Medicina, foi perguntado a sua opinião em relação a este tipo de gráfico. A pergunta feita foi: “O que você sente ao ver um gráfico de pizza em um artigo científico?” As alternativas para a resposta eram quatro (ódio, irritação, indiferença, amor). O resultado do inquérito está na Tabela 6.4. Tabela6.4: Sentimento dos alunos de Medicina em relação ao gráfico de pizza, UCS, 2012. Sentimento f fr fp Fp Odeiam 6 0,15 15 15 Não gostam 12 0,30 30 45 Indiferentes 14 0,35 35 80 Amam 8 0,20 20 100 Total 40 1,00 100 No R base, pacote graphics, existe a função pie()para obter um gráfico de setores simples. Esta função usa os seguintes argumentos basicos, consulte a ajuda do R para outras informações: x \\(\\longrightarrow\\) vetor numérico não negativo labels \\(\\longrightarrow\\) caracteres que fornecem nomes para as fatias. Para rótulos vazios ou NA (após coerção para caractere), nenhum rótulo ou linha indicadora é desenhada radius \\(\\longrightarrow\\) A pizza é desenhada centralizada em um quadrado cujos lados variam de -1 a +1. Se os caracteres que rotulam as fatias forem longos, pode ser necessário usar um raio menor. O padrão é 0,8. density \\(\\longrightarrow\\) Densidade das linhas de sombreamento, em linhas por polegada. O padrão é NULL significa que nenhuma linha de sombreamento é desenhada. Valores não positivos de densidade também inibem o desenho de linhas sombreadas col \\(\\longrightarrow\\) Vetor de cores a ser usado no preenchimento ou sombreamento das fatias. Se estiver faltando, um conjunto de 6 cores pastel é usado Os valores da coluna de frequência absoluta (f) da Tabela 6.4 serão usados como o argumento x. Ele informa a área (proporção de cada fatia. Os rótulos das fatias são escritos com a função concatenar c(). pie(x = c(6, 12, 14, 8), labels = c(&quot;Odeiam&quot;, &quot;Não gostam&quot;, &quot;Indiferentes&quot;, &quot;Amam&quot;)) Figura6.3: Gráfico de Pizza: Opinião dos estudantes de Medicina. As cores que aparecem na Figura 6.3 foram escolhidas pelo R, usando o seu padrão. Entretanto, elas podem ser customizadas, especificando-as pelo nome colocado entre parênteses. Por exemplo, col = \"red\" e se for mais de uma cor usar a função concatenar, col = c(“gray58“, “yellow4”, “cyan”, “tomato”). As cores também podem se denotadas pelo sistemas RGB ou hexadecimal. A sigla RGB representa as cores primária em inglês (Red, Green, Blue). O código hexadecimal da cor branca é #FFFFFFF, da gray58 é #949494, da yellow4 é #999900, da cyan é #00FFFFe da tomato é #FF6347 (6.4). pie(x = c(6, 12, 14, 8), labels = c(&quot;Odeiam&quot;, &quot;Não gostam&quot;, &quot;Indiferentes&quot;, &quot;Amam&quot;), col = c(&quot;gray58&quot;, &quot;yellow4&quot;, &quot;cyan&quot;, &quot;tomato&quot;)) Figura6.4: Figura anterior com cores personalizadas. As cores parecem espetaculosas, mas o objetivo foi de criticar os gráficos tipo pista. Se o leitor quiser insistir no seu uso e com um gráfico em 3D (Figura 6.5), pode-se instalar o pacote plotrix ((78)) e carregar a função pie3D(). Os argumentos são praticamente os mesmos do gráfico simples. Acrescenta-se radius = 0.9 que muda o raio da pizza e explode = 0.1 que determina o afastamento das fatias (0, as mantém juntas). Além disso, como o gráfico exibe rótulos com textos muito grandes, usa-se o argumento labelcex = 1. Como qualquer função nova, basta clicar na tecla Tab, dentro da mesma, que aparece um menu com as alternativas de argumentos. library (plotrix) pie3D(x = c(6, 12, 14, 8), labels = c(&quot;Odeiam&quot;, &quot;Não gostam&quot;, &quot;Indiferentes&quot;, &quot;Amam&quot;), radius = 0.9, explode = 0.1, col = c(&quot;gray58&quot;, &quot;yellow4&quot;, &quot;cyan&quot;, &quot;tomato&quot;), labelcex = 1) Figura6.5: Gráfico de Pizza: Opinião dos estudantes de Medicina. 6.5.2 Gráfico de barras Os gráficos de barra exibem a distribuição (frequências) de uma variável categórica através de barras verticais ou horizontais, ou sobrepostas (79). Assim como o gráfico de setores, o gráfico de barras é utilizado para representar a frequência absoluta ou percentual de diferentes categorias. As barras são proporcionais as frequências. A forma mais simples de solicitar um gráfico de barra no R é digitar a função barplot() do pacote básico. Esta função é específica para desenhar gráficos de barras horizontais e verticais e usa os seguintes argumentos: height \\(\\longrightarrow\\) um vetor ou matriz de valores que descreve as barras que constituem o gráfico; width \\(\\longrightarrow\\) especifica largura das barras, com padrão de 1, opcional; space \\(\\longrightarrow\\) a quantidade de espaço (como uma fração da largura média da barra) restante antes de cada barra. Pode ser fornecido como um único número ou um número por barra; beside \\(\\longrightarrow\\) argumento lógico para especificar se colunas devem ser mostradas lado a lado; col \\(\\longrightarrow\\) cores das barras componentes das barras, por padrão é usado grey (cinza); border \\(\\longrightarrow\\) cor das bordas das barras; … \\(\\longrightarrow\\) outros argumentos. Consulte a ajuda do R. Para a construção do gráfico de barras simples da Figura 6.6), foi utilizada a variável categIdade, anteriormente criada, a partir do conjunto de dados dadosMater.xlsx. barplot(table(mater$categIdade)) Figura6.6: Gráfico de barra simples. Observando a Figura 6.6, verifica-se que não existem rótulos nos eixos x e y e o eixo y tem um tamanho inferior a barra mais alta. Estes e outros problemas podem ser resolvidos modificando-se ou acrescentando outros argumentos na função barplot(). Existem vários argumentos e para conhece-los melhorpesquise no Help do RStudio. Em um gráfico de barra simples são suficientes as seguintes modificações que irão resultar na Figura 6.7: Para corrigir a amplitude do eixo y, existe o argumento ylim = c(lim inf, lim sup). Na Tabela 6.2, observa-se que a frequência máxima é de 992, assim estende-se até 1000, bem próximo da frequência da categoria, acrescentando ylim = c (0,1000), separado por vírgulas de outros argumentos. Para os rótulos se utiliza os argumentos ylab = (“Frequência”) e xlab = (“Faixa Etária”). Também, pode ser incluído um título no gráfico com o argumento main = “Título”. Observe que os títulos estão entre aspas. Para modificar o tamanho das letras dos eixos x e y, que estão pouco visíveis, existe o argumento cex.lab = 1, que é o padrão. Para aumentar em 30%, por exemplo, usar cex.lab = 1.3. Os nomes tem padrão cex.names = 1, para modificar pode-se usar 1.3, 1.5, etc. Se nada for modificado, o R imprime o padrão. Para a cor das barras, use o argumento col = (“cor”). Escolha a cor entre as 657 opções, ou deixe o padrão cinza (grey). O argumento col.axis = “cor” controla a cor dos valores dos eixos. Para modificar a borda das barras que por padrão é preta, é possível mudar, usando o argumento border = “cor”. Sem borda basta colocar 0 (zero), no lugar da cor. Para colocar as barras na posição horizontal, pode ser utilizado o argumento horiz = TRUE. Lembrar de inverter as barras. Ou seja, a variável x passa a ser y e vice-versa. O argumento las = 1 faz o o texto do eixo y ficar horizontal A função box(bty = \"L\"), colocada após, e opcional, faz os eixos se encontraren em 0. barplot(table(mater$categIdade), ylim = c (0,1000), col= &quot;tomato&quot;, border = &quot;black&quot;, ylab= &quot;Frequência absoluta&quot;, xlab = &quot;Faixa etária&quot;, cex.lab = 1.2, las = 1) box(bty = &quot;L&quot;) Figura6.7: Gráfico de barra simples modificado. Para que as barras fiquem horizontais como na Figura 6.8, usa-se o argumento horiz=TRUE: barplot(table(mater$categIdade), xlim = c (0,1000), col= &quot;tomato&quot;, border = &quot;black&quot;, ylab= &quot;Faixa Etária&quot;, xlab = &quot;Frequência absoluta&quot;, cex.lab = 1.2, horiz=TRUE) box(bty = &quot;L&quot;) Figura6.8: Gráfico com barras horizontais. Além das modificações realizadas, é possível fazer outras para tornar o gráfico mais informativo . Por exemplo, pode-se colocar as frequência de cada barra no topo das mesmas (Figura 6.9): 1º Passo: Criar um gráfico de barras , colocando-o em um objeto x, que conterá a coordenada X do centro de cada uma das barras. Para verificar isso, basta executar o objeto x; 2º Passo: colocar a tabela table(mater$idadeCateg) com um objeto y da classe matriz; 3º Passo: usar a funçãoo text() para colocar os valores. x &lt;- barplot(table(mater$categIdade), ylim = c (0,1000), col= &quot;springgreen&quot;, border = &quot;black&quot;, ylab = &quot;Frequência absoluta&quot;, xlab = &quot;Faixa etária&quot;, cex.lab = 1.2, las = 1) box(bty = &quot;L&quot;) y &lt;- as.matrix(table(mater$categIdade)) text (x, y, labels = as.character(y), adj = c(0.5, 2), col = &quot;black&quot;) Figura6.9: Gráfico de barra simples com frequências no topo. 6.5.2.1 Gráfico de barras empilhadas Para este tipo de apresentação são utilizados, praticamente, os mesmos argumentos vistos para gerar um gráfico de barra simples. Como existem duas variáveis, há necessidade de avisar ao R como elas devem aparecer. Para isso, entra o argumento beside = FALSE, que informa que as barras não estarão uma ao lado da outra e sim empilhadas (Figura 6.10). O padrão é as barras ficarem uma ao lado da outra. Acrescenta-se uma legenda com a função legend() na parte superior esquerda (topleft). O argumento bty = \"n\" informa que será removido o quadro ao redor da legenda e fill = c(\"dimgrey\", \"salmon\") são as cores das barras. As duas variáveis a serem visualizadas são o hábito tabagista entre as puérperas de acordo com a idade. No conjunto de dados dadosMater.xlsx, o hábito tabagista está registrado na variável fumo, vista quando se estudou tabelas de contingência. Aqui se construirá uma tabela 3 x 2, tabFumo2: tabFumo2 &lt;- table(mater$fumo, mater$categIdade) barplot(tabFumo2, beside = FALSE, ylim = c(0, 1000), xlab=&quot;Faixa Etária&quot;, ylab = &quot;Frequência&quot;, col = c (&quot;dimgrey&quot;, &quot;cadetblue1&quot;), cex.lab = 1, cex.axis = 1, cex.names = 1, las = 1) box(bty = &quot;L&quot;) legend (&quot;topleft&quot;, legend = c(&quot;Fumantes&quot;, &quot;Não Fumantes&quot;), fill = c(&quot;dimgrey&quot;, &quot;cadetblue1&quot;), bty=&quot;n&quot;, cex = 1) Figura6.10: Gráfico de barras empilhadas. 6.5.2.2 Gráfico de barras lado a lado É igual a anterior, apenas com o argumento beside = TRUE (Figura 6.11). barplot(tabFumo2, beside = TRUE, ylim = c(0, 1000), xlab=&quot;Faixa Etária&quot;, ylab = &quot;Frequência&quot;, col = c (&quot;dimgrey&quot;, &quot;cadetblue1&quot;), cex.lab = 1, cex.axis = 1, cex.names = 1, las = 1) box(bty = &quot;L&quot;) legend (&quot;topleft&quot;, legend = c(&quot;Fumantes&quot;, &quot;Não Fumantes&quot;), fill = c(&quot;dimgrey&quot;, &quot;cadetblue1&quot;), bty=&quot;n&quot;, cex = 1) Figura6.11: Gráfico de barras lado a lado 6.5.2.3 Gráfico de barras para uma variável discreta A variável mater$para, número de filhos anteriores ao atual, é uma variável numérica discreta e, para representá-la, o mais adequado é usar um gráfico de barras simples Figura 6.12). tab_filhos&lt;- table (mater$para) barplot (tab_filhos, col = &quot;tomato&quot;, xlab=&quot;Número de filhos anteriores ao atual&quot;, ylab = &quot;Frequência&quot;, ylim = c(0, 500), cex.lab = 1, cex.axis = 1, cex.names = 1, las = 1) box(bty = &quot;L&quot;) Figura6.12: Gráfico de barras para uma variável discreta 6.5.3 Gráfico de barra de erro O gráfico de barra de erro é um tipo de gráfico barra acrescido de uma medida de dispersão: desvio padrão, intervalos de confiança ou erro padrão. As barras de erro dão uma ideia geral de quão precisa é uma medição ou, inversamente, quão longe o valor observado está do valor verdadeiro. Continuando a usar o arquivo dadosMater.xlsx, será selecionada uma amostra de recém-nascidos a termo, definido pela OMS como o nascido de 37 semanas completas a 42 semanas incompletas (259 a 293 dias). A partir destes dados, será construido um gráfico de barra de erro dos recém-nascidos do sexo masculino e feminino. Inicialmente, deve ser instalado e carregado o pacote Hmisc (80), necessário para fornecer a função errbar() que irá construir o gráfico de de barra de erro. A seguir, serão filtrados do conjunto de dados em uso, mater, os recém-nascidos a termo. O conjunto resultante será atribuído a um objeto denominado rnt e , usando o operador pipe %&gt;% será usada a função summarise() e group_by() provenientes do pacote dplyr, para calcular as medidas resumidoras, de acordo com o sexo. Como a variável sexo encontra-se como numérica, será transformada em fator: mater$sexo &lt;- factor(mater$sexo, labels = c(&#39;masc&#39;, &#39;fem&#39;)) rnt &lt;- mater %&gt;% filter(ig &gt;= 37 &amp; ig &lt; 42) %&gt;% group_by(sexo) %&gt;% summarise(n = n(), media = mean(pesoRN, na.rm = T), dp = sd(pesoRN, na.rm = T), l_inf = media - 1.96*dp, l_sup = media + 1.96*dp) rnt ## # A tibble: 2 × 6 ## sexo n media dp l_inf l_sup ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 masc 592 3274. 458. 2376. 4172. ## 2 fem 493 3147. 458. 2250. 4044. O próximo passo é criar um objeto, denominado barras, que irá receber as médias dos pesos dos recém-nascidos masculinos e femininos, que representam a altura das barras. Este objeta servirá de base para a construção de um gráfico de barras que será recebido por outro objeto, bp. Finalmente, coloca-se os limites inferiores e superiores para cada sexo, usando os valores calculados pela função summarise() que junto com o objeto bp constituem-se de argumentos da função errbar() (Figura 6.13). Veja maiores detalhes na ajuda do R (?errbar). barras &lt;- c(rnt$media[1], rnt$media[2]) bp &lt;- barplot(barras, ylim=c(0,4200), ylab = &quot;Peso do Recém-nascido (g)&quot;, cex.lab = 1.2, cex.axis = 0.8, cex.names = 1, space = c(0,0.5), names.arg=c(&quot;Meninos&quot;, &quot;Meninas&quot;), col = c(&quot;lightblue&quot;, &quot; pink2&quot;), las = 1) box(bty = &quot;L&quot;) lim_inf &lt;- c(rnt$l_inf[1], rnt$l_inf[2]) lim_sup &lt;- c(rnt$l_sup[1], rnt$l_sup[2]) errbar(bp, barras, lim_inf, lim_sup, add = TRUE, xlab = NULL) Figura6.13: Gráfico de barras de erro 6.5.4 Histograma O histograma é uma ferramenta gráfica que fornece informações sobre o formato da distribuição e dispersão dos dados, permitindo verificar se existe ou não simetria. É usado para dados contínuos. No histograma as frequências observadas são representadas por intervalos de classes de ocorrência que estão no eixo x e a altura das barras, representando a frequência de cada intervalo, no eixo y. A área de cada barra é proporcional à porcentagem de observações de cada intervalo. O R base possui uma função, denominada de hist() que constroi o histograma e possui vários argumentos: x \\(\\longrightarrow\\) um vetor numérico usado na construção do histograma breaks \\(\\longrightarrow\\) especifica o número de barras freq \\(\\longrightarrow\\) lógico; se TRUE (padrão), o histograma é uma representação de frequências; se FALSE, densidades de probabilidade, densidade de componentes, são plotados col \\(\\longrightarrow\\) cor a ser usada para preencher as barras. O padrão de NULL produz barras não preenchidas border \\(\\longrightarrow\\) cor da borda ao redor das barras. O padrão é usar a cor de primeiro plano padrão main, xlab, ylab \\(\\longrightarrow\\) rótulo do título, do eixo x e do eixo y. Para remover o rótulousar NULL. xlim, ylim \\(\\longrightarrow\\) limites do eixo x e do eixo y. 6.5.4.1 Histograma Simples Será usada a variável altura, proveniente da arquivo mater (veja início da seção), para a construção do histograma, executando:. hist(mater$altura) Figura6.14: Histograma básico No histograma da Figura 6.14, observam-se alguns problemas que devem ser melhorados para tornar a sua aparência mais elegante. O rótulo dos eixo x está com o nome da variável e do eixo y está em inglês; O título do histograma está em inglês e repete o eixo x. Pode ser removido. O eixo y tem um limite superior menor do que a barra mais alta; O gráfico está na cor cinza, que conforme o interesse pode ser modificada; O número de barras pode ser modificado com o argumento breaks. Existe uma função no R que permite calcular o número de intervalos, usando a regra de Sturges (nclass.Sturges()). Entretanto, na maioria das vezes, é o objetivo do estudo quem determina o número de barras e, também, porque nem sempre o R obedece ao argumento. É importante saber o limite inferior e superior da variável, para construir o eixo x. Pode-se fazer isso, com as funções min() e max(): min(mater$altura, na.rm = TRUE) ## [1] 1.4 max(mater$altura, na.rm = TRUE) ## [1] 1.85 O número de classes é igual a: nclass.Sturges(mater$altura) ## [1] 12 Acrescentado argumentos, modifica-se o aspecto do histograma (Figura 6.15): hist(mater$altura, breaks = 12, ylim = c (0, 450), xlim = c (1.4, 1.9), main= NULL, ylab = &quot;Frequência&quot;, xlab = &quot;Altura da gestante (metros)&quot;, col = &quot;tomato&quot;, las = 1) box(bty = &quot;L&quot;) Figura6.15: Histograma modificado Observe que o formato do histograma é igual ao anterior, mudando a cor das barras, o limite do eixo y e os rótulos dos eixos. O R não modificou o número de barras. Ou seja, não obedeceu à modificação do argumento breaks = 12. Ele escolheu o que achou mais adequado! 6.5.4.2 Histograma com curva normal sobreposta Eventualmente, para melhor comparar a distribuição dos dados, usamos uma curva normal sobreposta que servirá de indicador (Figura 6.16). A distribuição normal será discutida mais adiante (Capítulo 7). Construir um histograma de densidade, que é a proporção de todas as observações que se enquadram dentro do intervalo. Na função hist(), modificar o argumento para freq = FALSE. Adicionar uma curva normal ao histograma, usando a função curve(). Calcular antes a média e o desvio padrão da variável mater$altura. mu &lt;- mean(mater$altura, na.rm =TRUE) dp &lt;- sd(mater$altura, na.rm = TRUE) hist(mater$altura, ylim = c (0, 6), xlim = c (1.4, 1.9), main= NULL, ylab = &quot;Densidade&quot;, xlab = &quot;Altura da gestante (metros)&quot;, col =&quot;steelblue&quot;, freq = FALSE, border = &quot;white&quot;) box (bty = &quot;L&quot;) curve (dnorm (x, mean=mu, sd=dp), col=&quot;red&quot;, lty=1, lwd=2, add=TRUE) Figura6.16: Histograma com curva normal sobreposta 6.5.4.3 Componentes do Histograma Para verificar a lista de componentes de um histograma , há necessidade de colocar o histograma da Figura 6.17 em um objeto, no exemplo, denominado de h: h &lt;- hist(mater$altura, breaks = 8, ylim = c (0, 450), xlim = c (1.4, 1.9), main= NULL, ylab = &quot;Frequência&quot;, xlab = &quot;Altura da gestante (metros)&quot;, col =&quot;tomato&quot;, freq = TRUE, border = &quot;white&quot;) box (bty = &quot;L&quot;) Figura6.17: Histograma da altura da gestante h ## $breaks ## [1] 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 1.80 1.85 ## ## $counts ## [1] 18 87 304 406 334 151 50 16 2 ## ## $density ## [1] 0.26315789 1.27192982 4.44444444 5.93567251 4.88304094 2.20760234 0.73099415 ## [8] 0.23391813 0.02923977 ## ## $mids ## [1] 1.425 1.475 1.525 1.575 1.625 1.675 1.725 1.775 1.825 ## ## $xname ## [1] &quot;mater$altura&quot; ## ## $equidist ## [1] TRUE ## ## attr(,&quot;class&quot;) ## [1] &quot;histogram&quot; Uma das utilidades dos componentes, é construir um histograma com os valores correspondentes as barras sobrepostos ao gráfico (Figura 6.18). hist(mater$altura, breaks = 8, ylim = c (0, 450), xlim = c (1.4, 1.9), main= NULL, ylab = &quot;Frequência&quot;, xlab = &quot;Altura da gestante (metros)&quot;, col = &quot;salmon&quot;) box (bty = &quot;L&quot;) text (h$mids, h$counts, labels = h$counts, adj= c(0.5, -0.5)) Figura6.18: Histograma com frequência sobreposta Note que as informações deste gráfico são as mesmas de uma tabela de frequência construída com os mesmos dados. Maneiras diferentes de informar uma distribuição de frequência (veja seção sobre tabelas de frequência). 6.5.5 Boxplot O boxplot descreve a distribuição de uma variável contínua exibindo o resumo de cinco números: mínimo, 1º quartil (percentil 25), mediana (percentil 50), 3ª quartil (percentil 75) e máximo (Figura 6.19). Figura6.19: Boxplot 6.5.5.1 Boxplot a partir de um vetor Para construir um boxplot, serão usados os mesmos dados dos recém-nascidos a termo, filtrados do conjunto de dados dadosMater.xlsx, como realizado na seção da construção de um gráfico de barra de erro. Os dados obtidos, novamente serão atribuídos a um objeto de nome rnt. A variável usada para construir o boxplot será rnt$pesoRN. rnt &lt;- read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) %&gt;% select(idadeMae, altura, peso, anosEst, fumo, para, ig, sexo, pesoRN, compRN, utiNeo) %&gt;% filter(ig &gt;= 37 &amp; ig &lt; 42) O R possui uma função no pacote básico denominada boxplot() que será usada para construir o gráfico da Figura 6.20. A função solicita vários argumentos que podem alterar a sua aparência e devem ser utilizados de acordo com necessidade: formula \\(\\longrightarrow\\) este parâmetro é definido como um vetor ou uma fórmula (y ~ grupo); data \\(\\longrightarrow\\)este parâmetro define o conjunto de dados; notch \\(\\longrightarrow\\) parâmetro lógico. Se TRUE um entalhe será desenhado em cada lado da caixa, representando o intervalo de confiança para a mediana. Se os entalhes de dois boxplots não se sobrepuserem, indica uma “forte evidência” de que as duas medianas diferem; varwidth \\(\\longrightarrow\\) parâmetro lógico. Se for TRUE, as caixas serão desenhadas com larguras proporcionais às raízes quadradas do número de observações nos grupos; border \\(\\longrightarrow\\)um vetor opcional de cores para os contornos dos boxplots: main \\(\\longrightarrow\\) este parâmetro é o título do gráfico; xlab, ylab ⟶ rótulos dos eixos x e y ; cex \\(\\longrightarrow\\)ver https://www.statology.org/r-plot-cex/; las \\(\\longrightarrow\\)altera orientação do rótulos do eixo. Valores aceitos 0 (paralelo ao eixo), 1 (horizontal), 2 (perpendicular) e 3 (vertical); names \\(\\longrightarrow\\) Este parâmetro são os rótulos dos grupos que serão mostrados em cada boxplot; … \\(\\longrightarrow\\) Outros parâmetros (ver ajuda do R, digitando ?boxplot no Console) boxplot (rnt$pesoRN) Figura6.20: Boxplot simples Esse boxplot pode ser modificado (Figura 6.21), alterando alguns argumentos como colocação de um título no gráfico, e rótulos nos eixos e mudança na cor. Os argumento cex.lab, cex.axis e cex.names estabelecem o tamanho fontes. Por exemplo, para aumentar em 20%, usamos 1.2. boxplot (rnt$pesoRN, col = &quot;lightblue2&quot;, main = &quot;RN a termo&quot;, ylab = &quot;Peso do Recém-nascido (g)&quot;, border = &quot;black&quot;, cex.lab = 1.2, cex.axis = 1, cex.names = 1, las = 1) Figura6.21: Boxplot modificado 6.5.5.2 Interpretação do boxplot O boxplot nos fornece uma análise visual da posição, dispersão, simetria, caudas e valores discrepantes (outliers) do conjunto de dados (Figura 6.19). Posição – Em relação à posição dos dados, observa-se a linha central do retângulo (a mediana ou segundo quartil). Dispersão – A dispersão dos dados pode ser representada pelo intervalo interquartil (IIQ), tamanho da caixa, que é a diferença entre o terceiro quartil (3ºQ) e o primeiro quartil (1ºQ), ou ainda pela amplitude que é calculada da seguinte maneira: valor máximo – valor mínimo. Embora a amplitude seja de fácil entendimento, o intervalo interquartil é uma estatística mais robusta para medir variabilidade uma vez que não sofre influência de outliers. Simetria – Um conjunto de dados que tem uma distribuição simétrica, terá a linha da mediana no centro do retângulo. Quando a linha da mediana está próxima ao primeiro quartil, os dados são assimétricos positivos e quando a posição da linha da mediana é próxima ao terceiro quartil, os dados são assimétricos negativos. Vale lembrar que a mediana é a medida de tendência central mais indicada quando os dados possuem distribuição assimétrica, uma vez que a média aritmética é influenciada pelos valores extremos. Caudas – As linhas que vão do retângulo até aos outliers podem fornecer o comprimento das caudas da distribuição. Valores atípicos (Outliers) – Os outliers indicam possíveis valores discrepantes. No boxplot, as observações são consideradas outliers quando estão abaixo ou acima dos limites superior e inferior. O limite de detecção de outliers é construído utilizando o intervalo interquartil, dado pela distância entre o primeiro e o terceiro quartil. Sendo assim, os limites inferior e superior de detecção de outlier são dados por: o Limite Inferior: 1ºQ – (1,5 * IIQ); o Limite Superior: 3ºQ + (1,5 * IIQ). Tanto o limite superior como o inferior são representados por (º). o Valores extremos: Valores que estão acima ou abaixo de 3 vezes o IIQ são considerados extremos, representados por (*). 6.5.5.3 Adicionando pontos ao boxplot Quando se observa um boxplot, verifica-se que os mesmos ocultam a distribuição subjacente dos dados. Para resolver este “problema”, pode-se adicionar pontos ao gráfico, usando a função stripchart(). Esta função permite criar um gráfico de dispersão unidimensional sobreposto ao boxplot (Figura 6.22). Os comandos para esta ação são: boxplot (rnt$pesoRN, col = &quot;lightblue2&quot;, ylab = &quot;Peso do Recém-nascido (g)&quot;, border = &quot;black&quot;, cex.lab = 1.2, cex.axis = 1, cex.names = 1, las = 1) stripchart(x= rnt$pesoRN, method = &quot;jitter&quot;, col = &quot;tomato&quot;, cex = 0.5, pch = 16, vertical = TRUE, add = TRUE) Figura6.22: Boxplot com pontos de dispersão Neste exemplo, há uma grande sobreposição de pontos, pois a amostra é muito grande (n = 1085). Isto dificulta um pouco a visualização, mas ajuda a ver como a dispersão se comporta. Você também pode personalizar o símbolo (pontos) para criar o gráfico, a largura da linha e sua cor com os argumentos pch, lwd e col, respectivamente. Alguns símbolos, como pch = 21 a 25 permitem que você modifique a cor de fundo do símbolo com o argumento bg. O argumento vertical = TRUE, coloca os pontos na vertical sobreposto ao boxplot, quando o argumento add = TRUE. O argumento cex = 0.5 é o tamanho dos pontos e method = \"jitter\", espalha os pontos para diminuir a sobreposição entre eles. 6.5.5.4 Boxplot com intervalos de confiança para a mediana É possível representar os intervalos de confiança de 95% para a mediana em um boxplot (Figura 6.23), definindo o argumento notch como TRUE. boxplot (rnt$pesoRN, col = &quot;lightblue2&quot;, ylab = &quot;Peso do Recém-nascido (g)&quot;, border = &quot;black&quot;, cex.lab = 1.2, cex.axis = 1, cex.names = 1, las = 1, notch = TRUE) Figura6.23: Boxplot modificado 6.5.5.5 Estatísticas do boxplot A função boxplot.stats() do pacote grDevices fornece as estatísticas do boxplot, facilitando a interpretação do mesmo, de modo semelhante ao visto para o histograma. boxplot.stats (rnt$pesoRN) ## $stats ## [1] 2051 2920 3215 3505 4380 ## ## $n ## [1] 1085 ## ## $conf ## [1] 3186.939 3243.061 ## ## $out ## [1] 1440 1980 1795 1810 4400 4950 4535 4670 1425 4410 4660 1715 1895 4485 4390 ## [16] 4445 4620 1785 Interpretação * $stats = é o resumo dos 5 números: mínimo, percentil 25, mediana, percentil 75 e máximo; * $n = nº de obs; * $conf = limite inf/sup do entalhe se houver; * $out = são os outliers. 6.5.5.6 Múltiplos boxplots Os boxplots são muito usados na comparação de grupos. A necessidade mais comum é ordenar as categorias de acordo com o aumento da mediana, mas isto é opcional. Permite identificar rapidamente qual grupo tem o maior valor e como as categorias são classificadas (Figura 6.24). Será realizada uma comparação visual, usando boxplots, dos pesos dos recém-nascidos por sexo. As variáveis são rnt$pesoRN e rnt$sexo. Esta última está codificada como numérica 1 e 2, portanto há necessidade de ser transformada em fator: rnt$sexo &lt;- factor(rnt$sexo, levels = c(1, 2), labels = c(&quot;masc&quot;, &quot;fem&quot;)) boxplot (rnt$pesoRN ~ rnt$sexo, col = c(&quot;lightblue2&quot;, &quot;pink&quot;), ylab = &quot;Peso do Recém-nascido (g)&quot;, xlab = &quot;Sexo&quot;, ylim = c(1000, 5000), border = &quot;black&quot;, cex.lab = 1, cex.axis = 1, cex.names = 1, las = 1) Figura6.24: Múltiplos boxplots Observe que foi utilizado o argumento rnt$pesoRN ~ rnt$sexo (y ~ grupo) para obter os dois boxplots. Existe uma pequena diferença entre eles, as caixas são quase coincidentes.. Foi suprimido o argumento (xlab = sexo) relativo ao rótulo do eixo x, pois seria redundante. Pode-se fazer um entalhe (notch) que podem ser interpretados como um intervalo de confiança em torno dos valores medianos (Figura 6.25). É calculado pela fórmula :\\(mediana \\pm 1.57\\times IIQ/\\sqrt{n}\\). No nosso exemplo, observe que o entalhe nos meninos está um pouco acima do das meninas.. boxplot (rnt$pesoRN ~ rnt$sexo, col = c(&quot;lightblue2&quot;, &quot;pink&quot;), ylab = &quot;Peso do Recém-nascido (g)&quot;, xlab = &quot;Sexo&quot;, ylim = c(1000, 5000), border = &quot;black&quot;, cex.lab = 1, cex.axis = 1, cex.names = 1, las = 1, notch = TRUE) Figura6.25: Boxplots com entalhes 6.5.5.7 Boxplots horizontais Para criar um boxplot horizontal (Figura 6.26), usamos o argumento horizontal = TRUE e invertemos os rotulos dos eixos x e y. boxplot (rnt$pesoRN ~ rnt$sexo, col = c(&quot;lightblue2&quot;, &quot;pink2&quot;), xlab = &quot;Peso do Recém-nascido (g)&quot;, ylab = NULL, horizontal = TRUE, border = &quot;black&quot;, cex.lab = 1.2, cex.axis = 1, cex.names = 1, las = 1) Figura6.26: Boxplots horizontais 6.5.6 Gráfico de Dispersão Um gráfico de dispersão (Scatterplot) exibe a relação entre duas variáveis numéricas (Figura 6.27). Cada ponto representa uma observação. Suas posições nos eixos x (horizontal) e y (vertical) representam os valores das duas variáveis. O R Base é uma boa opção para construir um gráfico de dispersão, usando a função plot(). Ambas as variáveis numéricas do banco de dados devem ser especificadas nos argumentos x e y. A função plot() é uma função genérica que pode ser facilmente editada com múltiplos argumentos envolvendo os eixos e caracteres plotados da mesma maneira que foi feita com os gráficos anteriores. Aqui, novamente, serão usados os dados incluídos no conjunto de dados rnt: plot (x = rnt$compRN, y = rnt$pesoRN, ylab = &quot;Peso de Recém-nascido (g)&quot;, xlab = &quot;Comprimento do Recém-nascido (cm)&quot;, cex.axis = 0.8, las = 1) Figura6.27: Gráfico de dispersão Este mesmo gráfico pode ser obtido, usando uma fórmula y~x e acrescentando o argumento bty = \"L\" (Figura 6.28). Este argumento permite personalizar a caixa ao redor do gráfico. o: caixa completa (parâmetro padrão), n: sem caixa 7: superior + direita L: inferior + esquerda C: superior + esquerda + inferior U: esquerda + inferior + direita plot (pesoRN ~ compRN, data = rnt, ylab = &quot;Peso de Recém-nascido (g)&quot;, xlab = &quot;Comprimento do Recém-nascido (cm)&quot;, cex.axis = 0.8, las = 1, bty = &quot;L&quot;) Figura6.28: Gráfico de dispersão Como em qualquer outro gráfico, este também pode ser melhorado em seu aspecto, tornando os pontos sólidos e coloridos. O argumento pch estabelece o tipo de pontos (Figura 6.29). Figura6.29: Símbolo dos formatos Na Figura 6.27, como os pontos estão aglomerados, devido a quantidade, é possível tentar espalhá-los, usando a função jitter() na variável compRN (Figura 6.30). O argumento 10 é variável e significa o grau de espalhamento: plot (jitter(rnt$compRN,10), rnt$pesoRN, col = &quot;steelblue&quot;, ylab = &quot;Peso de Recém-nascido (g)&quot;, xlab = &quot;Comprimento do Recém-nascido (cm)&quot;, las = 1, bty = &quot;L&quot;, pch = 16, cex = 1, cex.lab = 1.1, cex.axis = 0.8) Figura6.30: Gráfico de dispersão com jitter 6.5.6.1 Mapeamento dos pontos de acordo com uma variável categórica Inicialmente, será criado um vetor para representar as cores, de acordo com o sexo (meninos = azul; meninas = vermelho). Usa-se a função unclass() para discriminar os sexos (Figura 6.31). Acrescenta-se uma legenda para ilustrar a separação. cores &lt;- c(&quot;dodgerblue3&quot;, &quot;tomato&quot;) plot(x = jitter(rnt$compRN, 10), y = rnt$pesoRN, bg = cores[unclass(rnt$sexo)], ylab = &quot;Peso de Recém-nascido (g)&quot;, xlab = &quot;Comprimento do Recém-nascido (cm)&quot;, las = 1, bty = &quot;L&quot;, cex = 1.5, pch=21, cex.lab = 1, cex.axis = 0.8) legend (legend = c(&quot;Meninos&quot;, &quot;Meninas&quot;), fill = cores, bty=&quot;n&quot;, cex = 1, &quot;topleft&quot;) Figura6.31: Mapeamento dos pontos de acordo com uma variável categórica 6.5.6.2 Adição da reta de ajuste Uma linha reta de ajuste dos dados (Figura 6.32) pode ser acrescentada usando a função abline (), associada a função lm (). Um modelo típico lm (linear model) tem o formato resposta (y) ~ preditor (x). Mais detalhes sobre o modelo de ajuste linear na regressão linear (veja capítulo 15). # Construção do gráfico de dispersão plot (jitter(rnt$compRN,10), rnt$pesoRN, col = &quot;gray40&quot;, bg = &quot;darkturquoise&quot;, ylab = &quot;Peso de Recém-nascido (g)&quot;, xlab = &quot;Comprimento do Recém-nascido (cm)&quot;, las = 1, bty = &quot;L&quot;, pch = 21, cex = 1.3, cex.lab = 1, cex.axis = 0.8) # Criação do modelo de ajuste modelo &lt;- lm (rnt$pesoRN ~ rnt$compRN) # Adição da reta, usando o modelo abline (modelo, col=&quot;red&quot;, lwd=2, lty = 2) Figura6.32: Gráfico de dispersão com reta de ajuste Ao executar o modelo, se obtém os parâmetros para a construção da equação da regressão linear: summary(modelo) ## ## Call: ## lm(formula = rnt$pesoRN ~ rnt$compRN) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1434.56 -218.40 -19.56 177.76 2097.87 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3416.451 215.821 -15.83 &lt;2e-16 *** ## rnt$compRN 137.674 4.475 30.77 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 337.7 on 1083 degrees of freedom ## Multiple R-squared: 0.4664, Adjusted R-squared: 0.4659 ## F-statistic: 946.6 on 1 and 1083 DF, p-value: &lt; 2.2e-16 A equação de predição da regressão linear permite que ao conhecer o valor do comprimento é possível prever o peso do recem-nascido: \\[ \\hat{y} = b_{0}+ b_{1}\\times x \\] Desta forma, substituindo pelos valores contidos nas estimativas da tabela dos coeficientes do sumário do modelo, um bebê com 50 cm terá um peso de aproximadamente: \\[ \\hat{y} = -3416.45 + 137.67\\times 50 = 3467.05 \\] 6.6 Introdução ao ggplot2 O R tem vários sistemas para fazer gráficos e, na ,maioria das vezes, eles são suficientes. Entretanto, o surgimento do ggplot2 (81) trouxe a possibilidade de serem construídos gráficos mais elegantes e versáteis. Além disso, torna o processo mais rápido, baseado uma sofisticada gramática (82). O gráfico é construído, usando função ggplot(), a partir de alguns elementos básicos 20: Dados \\(\\longrightarrow\\) os dados brutos que você deseja representar graficamente. Geometria (geoms) \\(\\longrightarrow\\) As formas geométricas que irão representar os dados. Estética (aes) \\(\\longrightarrow\\) Estética dos objetos geométricos e estatísticos, como posição, cor, tamanho, forma e transparência Escala (scales) \\(\\longrightarrow\\) Mapas entre os dados e as dimensões estéticas, como intervalo de dados para plotar largura ou valores de fator para cores. Transformações estatísticas (stats) \\(\\longrightarrow\\) resumos estatísticos dos dados, como quantis, curvas ajustadas e somas. Sistemas de coordenadas (coordinates Systems) \\(\\longrightarrow\\) A transformação usada para mapear coordenadas de dados no plano do retângulo de dados. Facetas (faceting) \\(\\longrightarrow\\) A organização dos dados em uma grade de gráficos. Temas visuais (themes) \\(\\longrightarrow\\) Os padrões visuais gerais de um gráfico, como plano de fundo, grades, eixos, tipo de letra padrão, tamanhos e cores. 6.6.1 Dados usados nesta seção Serão usados os mesmos dados do conjunto rnt, acrescidos da variável categIdade (idade categorizada) e da variável categEscola (anos de estudo completos categorizados). rnt$categIdade &lt;- cut(rnt$idadeMae, breaks = c(13, 20, 36, 46), labels = c(&quot;&lt;20a&quot;, &quot;20-35a&quot;, &quot;&gt;35a&quot;), include.lowest = TRUE, right = FALSE, ordered_result =TRUE) rnt$categEscola &lt;- cut (rnt$anosEst, breaks= c (0,10,13,18), right = FALSE, labels = c(&quot;Fundamental&quot;, &quot;Médio&quot;, &quot;Superior&quot;), include.lowest = TRUE, ordered_result =TRUE) A variável rnt$fumo será transformada em fator:: rnt$fumo &lt;- factor (rnt$fumo, levels = c(1, 2), labels = c(&quot;sim&quot;, &quot;não&quot;)) 6.6.2 Função ggplot() A sintaxe do ggplot2 é diferente do R básico. De acordo com os elementos básicos, um ggplot padrão precisa de três informações que devem ser especificadas: os dados, a estética e a geometria. Essas são as camadas principais. Será criado um ggplot padrão (Figura 6.33) que será atribuído a um objeto g, usando data = dados e a estética (aes), no eixo x, a variável compRN e, no eixo y, a variável pesoRN. g &lt;- ggplot (data = rnt, aes (x = compRN, y = pesoRN)) g Figura6.33: Gráfico ggplot padrão A este gráfico básico “vazio” adiciona-se uma camada que especifique o tipo de gemometria desejada (Figura 6.34). A geometria geom_point() retorna um gráfico de dispersão. g + geom_point() Figura6.34: Gráfico de dipersão A estética (aes) pode ser definida tanto na camada ggplot como na geom. Especificando no ggplot, esta aes será usada em todos os outros geoms que aoarecerem. Usando no geom, servirá apenas para ele. No exemplo. é indiferente o local da aes, o resultado será o mesmo, pois existe apenas um geom. 6.6.3 Tipos de geoms Encontra-se uma grande possibilidade de geometrias, de acordo com o tipo de gráfico que será plotado. Elas podem ser visualizadas aqui. 6.6.3.1 Histograma O geom_histogram() é a geometria para a construção de um histograma. Aqui, há necessidade apenas do eixo x, pois existe uma única variável (pesoRN). A execução do comando retorna a distribuição dessa variável (Figura 6.35): ggplot(data = rnt) + geom_histogram(aes(x = pesoRN)) Figura6.35: Histograma no ‘ggplot2’ 6.6.3.2 Gráfico de barras A função geom_bar()permite delinear um gráfico de barras (Figura 6.36) com a variável categIdade: ggplot(data = rnt) + geom_bar(aes(x = categIdade, y = after_stat(count/sum(count)))) Figura6.36: Gráfico de barras no ‘ggplot2’ A função after_stat() colocada no argumento y é usada para determinar o tipo de estatística que irá aparecer no eixo y, quando ela não está disponível nos dados. No exemplo, foi usada a proporção que cada categoria representa na variável categIdade (count/sum(count). Dessa forma, as parturientes acima de 35 anos correspondem a uma proporção de aproximadamente 0,10 (ou 10%). 6.6.3.3 Boxplot Com o geom_boxplot(), serão criados boxplots (Figura 6.37) comparando os pesos dos neonatos por sexo.. ggplot(data = rnt) + geom_boxplot(aes(x = sexo, y = pesoRN)) Figura6.37: Boxplot no ‘ggplot2’ 6.6.3.4 Gráfico de linhas Os dados, para produzir o gráfico de linha com a função geom_line(), podem ser obtidos aqui, acessar e baixar para o seu diretório de trabalho o arquivo dadosObitos.xlsx. Este conjunto de dados é constituído pelos óbitos por COVID-19 no Rio Grande do Sul entre 2020 e 2022. Atribua os dados ao objeto obitos, a partir do diretório de trabalho. obitos &lt;- read_excel(&quot;Arquivos/dadosObitos.xlsx&quot;) str(obitos) ## tibble [25 × 2] (S3: tbl_df/tbl/data.frame) ## $ data : POSIXct[1:25], format: &quot;2020-03-01&quot; &quot;2020-04-01&quot; ... ## $ obitos: num [1:25] 4 60 182 440 1391 ... Para a construção do gráfico de linha (Figura 6.38), use o seguinte comando: ggplot(data = obitos) + geom_line(aes(x = data, y = obitos)) Figura6.38: Gráfico de linha no ‘ggplot2’ 6.6.4 Modificação dos argumentos do geoms É possível mofificar alguns argumentos no geom. Cada tipo de geom possibilita alterações específicas. Inicialmente, serão realizadas modificações no gráfico de dispersão, construído acima com a geom_point, modificando a cor, de acordo com o sexo do recém-nascido (Figura 6.39) com o argumento color=: Quando o argumento cor for colocado dentro da aes, ele será definido por uma variável, no caso sexo. Desta forma, cada um dos sexos serão representados por pontos coloridos diferentes. A escolha da cor foi automática pelo ggplot2, entregando duas cores conforme o padrão da sua paleta. Quando o argumento é colocado fora da estética da geom, há necessidade de escolher a cor para os pontos. ggplot(data = rnt) + geom_point(aes(x = compRN, y = pesoRN, color = sexo)) Figura6.39: Gráfico de dispersão modificado com cores de acordo com o sexo Para modificar o tamanho dos pontos, está disponível o argumento size =, colocado na estética do geom_point(). O tamanho padrão é 1.5, mas você pode diminuir ou aumentar esse valor para diminuir ou aumentar os pontos (Figura 6.40). ggplot(data = rnt) + geom_point(aes(x = compRN, y = pesoRN, color = sexo, size = 1)) Figura6.40: Gráfico de dispersão com cores e tamanhos diferentes dos pontos, de acordo com o sexo Além disso, pode-se modificar o formato dos pontos com o argumento shape =. O formato (shape) pode ser colocado fora estética e aqui se encontram vários formatos que podem ser usados no ggplot2. É possível observar os diferentes formatos (Figura 6.29) ou usando a função show_point_shapes(), encontrada no pacote ggpubr. ggpubr::show_point_shapes() ## Scale for y is already present. ## Adding another scale for y, which will replace the existing scale. 6.6.5 Resumo de dados usando o geom Inicialmente, serão usados os argumentos stat = \"summary\" e fun = \"mean\" dentro da geom_point(). Concomitante, será feito uso do argumento show.legend = FALSE para evitar o aparecimento da legenda, indicando o sexo, pois o eixo x já mostra. Pode-se chegar ao mesmo resultado, usando a função stat_summary() para acrescentar estatísticas de resumo 21. A execução dos comandos resulta na Figura 6.41. ggplot(data = rnt, aes(x = sexo, y = pesoRN, color = sexo)) + geom_point(stat = &quot;summary&quot;, fun = &quot;mean&quot;, size = 3, show.legend = FALSE) Figura6.41: Gráfico resumo, mostrando as médias por sexo 6.6.5.1 Incluindo barras de erro Acrescenta-se uma camada geom_errorbar(), com os argumentos stat= summary e fun.data = \"mean_se\". Este último argumento fornece a média e o erro padrão e o width = 0.1, o tamanho da barra horizontal, gerando o gráfico da Figura 6.42: ggplot(data = rnt, aes(x = sexo, y = pesoRN, color = sexo)) + geom_point(stat = &quot;summary&quot;, fun = &quot;mean&quot;, size = 3, show.legend = FALSE) + geom_errorbar(stat = &quot;summary&quot;, fun.data = &quot;mean_se&quot;, width = 0.1, show.legend = FALSE) Figura6.42: Gráfico de barra de erro no ggplot2 Usando o pacote ggpubr consegue-se produzir um gráfico (Figura 6.43) semelhante ao da Figura 6.42 que usa (média ± erro padrão), mas com o intervalo de confiança (média ± 1.96 × erro padrão). A função do ggpubr para esta ação é a ggerrorplot() que possui múltiplos argumentos22: ggerrorplot(rnt, x = &quot;sexo&quot;, y = &quot;pesoRN&quot;, desc_stat = &quot;mean_ci&quot;, color = &quot;red&quot;, size = 1, xlab = NULL, ylab = &quot;Peso dos Recém-nascidos (g)&quot;) Figura6.43: Gráfico de barra de erro com o ggpubr 6.6.5.2 Incluindo múltiplos grupos Agora, será delineado o mesmo gráfico do peso dos recém-nascidos por sexo, levando em consideração o tabagismo materno (Figura 6.44). Em primeiro lugar, filtra-se pelo tabagismo, presente ou ausente. Depois seguindo a mesma programação anterior, separando as cores pelo tabagismo (fumo). Coloca-se também o argumento position = position_dodge (0.4) para que não haja sobreposição das barras no gráfico, exibe-se a legenda (show.legend = TRUE) e coloca-se um rótulo adequado no eixo y e suprime-se o do eixo x com os argumentos ylab e xlab, respectivamente: rnt %&gt;% ggplot(aes(x = sexo, y = pesoRN, color = fumo)) + ylab(&quot;Peso do Recém-nascido (g)&quot;) + xlab(&quot;&quot;) + geom_point(stat = &quot;summary&quot;, fun = &quot;mean&quot;, position = position_dodge(0.4), size = 3, show.legend = TRUE) + geom_errorbar(stat = &quot;summary&quot;, fun.data = &quot;mean_ci&quot;, width = 0.1, show.legend = TRUE, position = position_dodge(0.4)) Figura6.44: Gráfico de barra de erro com o ggpubr 6.6.6 Modificação do tema O tema padrão do ggplot2 tem uma aparência acinzentada que pode ser modificada pela definição de outro tema integrado, como o theme_bw(), Figura 6.45, que é uma variação de theme_grey(), que usa um fundo branco e linhas finas de grade cinza. Outro tema interessante é o theme_classic() que é um tema de aparência clássica, com linhas dos eixos x e y e sem linhas de grade. Para ver outras possibilidades acesse https://ggplot2.tidyverse.org/reference/ggtheme.html. Será repetido o boxplot criado anteriormente, acrescentando rótulos mais adequados, usando os argumentos xlab e ylab, cores aos boxplots , o theme_bw e remover a legenda com a função theme(legend.position=”none”), pois os sexos já estão explícitos. ggplot(data = rnt) + geom_boxplot(aes(x = sexo, y = pesoRN, color = sexo)) + xlab (&quot;&quot;) + ylab (&quot;Peso do Recém-nascido (g)&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) Figura6.45: Boxplot dos pesos dos recém-nascidos, usando tema theme_bw() 6.6.7 Reta de ajuste em um gráfico de dispersão No ggplot2, para incluir uma reta de ajuste, acrescenta-se uma nova camada geom_smooth(), além da camada geom_point(). O argumento method = \"lm\" irá ajustar uma reta aos pontos. Também é possível colocar um intervalo de mais ou menos um erro padrão para a reta com o argumento se = TRUE. No exemplo da Figura 6.46, foi usado se = FALSE. Além disso, foi solicitado que cor da reta seja preta (color = \"black\"), reduzido o seu tamanho (size = 0.5) e estabelecido que a reta seja tracejada (linetype = \"dashed\") 23. ggplot(data = rnt, aes(x = compRN, y = pesoRN)) + geom_point(color = &quot;tomato&quot;, size = 3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, linewidth = 0.5, linetype = &quot;dashed&quot;) + xlab(&quot;Comprimento do Recém-nascido (cm)&quot;) + ylab(&quot;Peso do Recém-nascido (g)&quot;) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figura6.46: Gráfico de dispersão com reta de ajuste Quando se usa mais de um geom, é importante a ordem em que eles são escritos, pois, como cada um deles é uma camada, elas se sobrepõem e podem se confundir. ### Filtrando dados para o gráfico Faz-se isso, usando a função filter() do pacote dplyr. Será construído um gráfico igual ao anterior, filtrando apenas o sexo masculino (Figura 6.47): rnt %&gt;% filter (sexo == &quot;masc&quot;) %&gt;% ggplot(aes(x = compRN, y = pesoRN)) + geom_point(color = &quot;steelblue&quot;, size = 3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, linewidth = 0.5, linetype = &quot;dashed&quot;) + xlab(&quot;Comprimento do Recém-nascido (cm)&quot;) + ylab(&quot;Peso do Recém-nascido (g)&quot;) + theme_bw() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figura6.47: Gráfico de dispersão com reta de ajuste para o sexo masculino 6.6.8 Trabalhando com os eixos no ggplot2 6.6.8.1 Rótulo dos eixos Em algumas seções anteriores, os rótulos dos eixos foram acrescentados, usando as funções xlab() e ylab(). Entretanto, como é muito comum no R, outra função, denominada labs(), permite adicionar os rótulos dos eixos x e y. A Figura 6.47, usando a função labs() teria os seguintes comandos: 6.6.8.2 Acrescentando um título O título pode ser adicionado através da função ggtitle(). Se for usado os dados, usados para a criação do hgráfico de linha, chega-se ao resultado da Figura 6.48 ggplot(data = obitos) + geom_line(aes(x = data, y = obitos)) + labs(x = &quot;Data(ano/mês)&quot;, y = &quot;Nº de mortes&quot;) + ggtitle(&quot;Mortes por COVID-19 - SES/RS, 2020-22&quot;) + theme_bw() Figura6.48: Mortes por COVID - RS, 2020-22 Uma outra maneira de colocar título, subtítulo e fonte no gráfico é mostrada nos comandos da Figura 6.49: ggplot(data = obitos) + geom_line(aes(x = data, y = obitos)) + labs(x = &quot;Data (ano/mês)&quot;, y = &quot;Nº de mortes&quot;, title = &quot;Mortes por COVID-19&quot;, subtitle = &quot;RS - 2020-2022&quot;, caption = &quot;Fonte: SES&quot;) + theme_classic() Figura6.49: Mortes por COVID - RS, 2020-22 6.6.8.3 Outro exemplo modificando o tamanho, estilo e fonte Esta ação é realizada, acrescentando uma camada com componentes da função theme(): plot.title, plot.subtitle, etc.24 Os dados para o exemplo são os do conjunto de dados rnt, criado anteriormente, usando, agora a função theme()na construção da Figura 6.50: ggplot(data = rnt, aes(x = sexo, y = pesoRN, color = sexo)) + geom_errorbar(stat = &quot;boxplot&quot;, width = 0.1) + geom_boxplot() + labs(x = &quot;&quot;, y = &quot;Peso do recém-nascido (g)&quot;, title = &quot;Maternidade do HGCS&quot;, subtitle = &quot;Caxias do Sul, RS, 2008&quot;, caption = &quot;Fonte: Autor&quot;) + theme_bw() + theme (plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 12, face = &quot;bold&quot;, color = &quot;darkgreen&quot;)) + theme(legend.position = &quot;none&quot;) Figura6.50: Boxplots com elementos dos textos modificados 6.6.8.4 Modificação dos limites dos eixos O sistema de coordenadas cartesianas é o tipo de sistema de coordenadas mais familiar e comum. Definir limites no sistema de coordenadas ampliará o gráfico (como se você estivesse olhando para ele com uma lupa) e não alterará os dados subjacentes, como definir limites em uma escala. Para realizar este trabalho, se fará uso da função coord_cartesian () ou scale_y_continuous () ou scale_x_continuous (). Tomando o gráfico, já construído acima (Figura 6.46) com pequenas alteraçõese e armazenando-o em um objeto, denominado gd (gráfico de dipersão), isto facilita a repetição do gráfico em outros códigos, pois basta escrever gd e executar (Figura 6.51). gd &lt;- ggplot(data = rnt, aes(x = compRN, y = pesoRN)) + geom_point(color = &quot;tomato&quot;, size = 3) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, linewidth = 0.8, linetype = &quot;dashed&quot;) + labs(x = &quot;Comprimento do RN (cm)&quot;, y = &quot;Peso do RN (g)&quot;, title = &quot;Gráfico de Dispersão&quot;, caption = &quot;Fonte: Autor&quot;) + theme_classic() gd Figura6.51: Gráfico de dispersão Função coord_cartesian() xlim, ylim \\(\\longrightarrow\\) limites dos eixos x e y expand \\(\\longrightarrow\\) Se TRUE, o padrão, adiciona um pequeno fator de expansão aos limites para garantir que dados e eixos não se sobreponham. Se FALSE, os limites são tirados exatamente dos dados ou xlim/ylim (Figura 6.52). gd + coord_cartesian(ylim = c(3000, 4000), xlim = c(45, 55), expand = TRUE) Figura6.52: Gráfico de dispersão expandido Observe que é como se fizesse um zoom no gráfico nos limites estabelecidos. Há um corte um pouco acima dos limites. No eixo y, um pouco acima de 4000 e um pouco abaixo de 3000 e, no eixo x, um pouco à esquerda de 45 e um pouco à direita de 55. Isto aconteceu, porque colocamos expand = TRUE. Para extrair esta margem, colocar expand = FALSE: Usando escalas de posição contínuas Pode-se usar a função scale_y_continous() e scale_x_continuous() para fazer algo parecido com a coord_cartesian() (Figura 6.53): gd + scale_x_continuous(limits = c(45, 55)) + scale_y_continuous (limits = c(3500, 4000)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figura6.53: Gráfico de dispersão expandido A função removeu os casos que estão fora dos limites estabelecidos. No caso, a mensagem do R mostra que foram removidos 857 casos. O gráfico foi construido sem estes casos e, no gráfico anterior, houve apenas uma aproximação (um zoom) dentro dos limites. Portanto, houve um impacto importante no gráfico. Se os dados não forem em escala contínua, é possível escolher outra escala, por esxemplo scale_y_discrete (). 6.6.8.5 Modificando a expansão É possível intervir na expansão da margem com o argumento expand = TRUE ou FALSE que pode ser usado também com a função scale_y_continuous().Será visto um exemplo de um gráfico de barras das faixas etárias das gestantes (Figura 6.54): gb &lt;- ggplot(data = rnt) + geom_bar(aes(x = categIdade, y = after_stat(count/sum(count))), show.legend = FALSE) + labs(y = &quot;Frequência&quot;, x = &quot;Faixa Etária da Parturiente&quot;) + theme_bw() gb Figura6.54: Gráfico de barras Observe que abaixo do 0 (zero) existe uma expansão. Para que as barras tenham início exatamente no 0, pode-se empregar a função scale_y_continuous() com o argumento expand = expansion (add = c(0,50)), significando que não se expande nada abaixo do 0 e se adiciona 50 unidades para cima, criando uma margem superior (Figura 6.55). gb + scale_y_continuous (expand = expansion(add = c(0,0.05))) Figura6.55: Gráfico de barras com expansão Isto também poderia ser feito com mult no lugar do add (Figura 6.56), representando o multiplicador que se coloca acima e abaixo: gb + scale_y_continuous (expand = expansion(mult = c(0,0.05))) Figura6.56: Gráfico de barras, igual a anterior 6.6.8.6 Usando a proporção ou percentagem nos eixos Na Figura 6.54, a unidade do eixo y encontra-se como um proporção y = after_stat(count/sum(count). É possível modificar para percentagem (Figura 6.57), empregando a função percent_format() do pacote scales (83): gb + scale_y_continuous (expand = expansion(mult = c(0,0.05)), labels = percent_format (accuracy = 0.1, decimal.mark = &quot;,&quot;)) Figura6.57: Gráfico de barras com percentagens no eixo y 6.6.8.7 Mudando o nome e a ordem dos rótulos do eixo x No gráfico acima, temos a faixa etária dividida em &lt;20a, 20-35a e &gt;35a. Pode haver interesse em mudar para adolescentes, adultas jovens e gestante idosa (Figura 6.58). Para fazer isso, sem modificar o banco de dados, simplemente altera-se-se os rótulos no argumento labels da função scale_x_discrete(): gb + scale_y_continuous (expand = expansion(mult = c(0,0.05)), labels = percent_format (accuracy = 0.1, decimal.mark = &quot;,&quot;)) + scale_x_discrete (labels = c(&quot;Adolescente&quot;, &quot;Adulta jovem&quot;, &quot;Gestante idosa&quot;)) Figura6.58: Gráfico de barras com eixo x modificado Para mudar a ordem dos nomes do eixo x, usando a ordem decrescente da frequência, basta colocar na função scale_x_discrete os argumentos limits e labels com a ordem desejada. O resultado é a Figura 6.59. gb + scale_x_discrete (limits = c(&quot;20-35a&quot;, &quot;&lt;20a&quot;, &quot;&gt;35a&quot;), labels = c(&quot;Adulta jovem&quot;, &quot;Adolescente&quot;, &quot;Gestante idosa&quot;)) Figura6.59: Gráfico de barras com eixo x modificado 6.6.8.8 Moficação dos intervalos dos valores do eixo O gráfico de linha de mortes por COVID no RS, 2020-2022, visto anteriormente (Figura 6.38), será atribuído a um objeto gl: gl &lt;- ggplot(data = obitos) + geom_line(aes(x = data, y = obitos)) + labs(x = &quot;Ano (mês)&quot;, y = &quot;Nº de mortes&quot;) + theme_classic() gl Figura6.60: Mortes por COVID-19, 2020-2022, RS. Observe (Figura 6.60) que, no eixo y, os óbitos estão registrados a cada 2000 e, no eixo x, foram marcadas apenas 4 datas. Podemos modificar isso adicionando duas camadas, usando as funções scale_y_continuous() e scale_x_datetime(): gl + scale_y_continuous(n.breaks = 10) + scale_x_datetime(date_breaks = &quot;4 month&quot;, date_labels = &quot;%Y (%b)&quot;) Figura6.61: Mortes por COVID-19, 2020-2022, RS. O aspecto do gráfico mudou um pouco (Figura 6.61). Agora, existem marcações no eixo y a cada 1000 mortes e o registro do tempo aparece a cada 4 meses, conforme estabelecido no argumento date_breaks = \"4 month\" e o formato foi modificado com o argumento date_labels = \"%Y %b\". Neste, %Y significa o ano e %b significa o mês abreviado (Jan-Dec). Para ver como customizar as datas, veja aqui 6.6.9 Modificação das cores Voltando a usar um gráfico de barra, já visto anteriormente (Figura 6.36), da distribuição da idade da gestante por faixa etária (categIdade), onde o ggplot2 escolheu as cores das barras de acordo com o seu padrão. Entretanto, a cor ou cores das barras podem ser modificadas na estética relacionada à cor: color ou colour, fill e alpha. Estes parâmetros estéticos alteram a cor (colour e fill) e a transparência (alpha). A mudança nas cores é uma forma de aprimorar um gráfico, especialmente quando representa mais de duas variáveis. Uma cor pode ser especificada por seu nome em inglês (por exemplo, “red”, “steelblue”). O R possui 657 cores integradas 25 que permitem uma ampla escolha. Uma outra maneira de especificar as cores é usar o sistema RGB ou hexadecimal. O código hexadecimal da cor branca é #FFFFFFF, da “gray58” é #949494, da “yellow4” é #999900, etc. Opcionalmente, a cor pode ser transparente, usando o formato “#RRGGBBAA”. Alpha refere-se à opacidade de um geom. Os valores de alpha variam de 0 a 1, com valores mais baixos correspondendo a cores mais transparentes. Alpha também pode ser modificada por meio da estética de colour ou fill se qualquer uma das estéticas fornecer valores de cor usando uma especificação RGB. 6.6.9.1 Usando uma única cor A Figura 6.36 será modificada, usando a cor cinza-claro para o preenchimento das barras (fill = “gray80”) e vermelho-escuro para bordas das barras (color = “darkred”), colocados fora da aes. Após a execução dos códigos, o resultado será a Figura 6.62. ggplot(data = rnt) + geom_bar(aes(x = categIdade, y = after_stat(count/sum(count))), fill = &quot;gray80&quot;, colour = &quot;darkred&quot;, show.legend = FALSE) + labs(y = &quot;Frequência&quot;, x = &quot;Faixa Etária&quot;) + scale_y_continuous(expand = expansion(mult = c(0,0.05)), labels = percent_format (accuracy = 0.1, decimal.mark = &quot;,&quot;)) + theme_bw() Figura6.62: Frequência da faixa etária das parturientes da Maternidade do HCCS, 2008. 6.6.9.2 Cores diferentes de acordo com o grupo Continuando com os mesmos dados, o preenchimento (fill = categIdade) será colocado dentro da aes e o ggplot2 escolhe as cores do sua paleta, conforme o número de categorias. As bordas serão mantidas na mesma cor, color = darkred, permanecendo fora da aes. Saida da execução dos comandos resultará na Figura 6.63. ggplot(data = rnt) + geom_bar(aes(x = categIdade, y = after_stat(count/sum(count)), fill = categIdade), colour = &quot;darkred&quot;, show.legend = FALSE) + labs(y = &quot;Frequência&quot;, x = &quot;Faixa Etária&quot;) + scale_y_continuous(expand = expansion(mult = c(0,0.05)), labels = percent_format (accuracy = 0.1, decimal.mark = &quot;,&quot;)) + theme_bw() Figura6.63: Cores estabelecidas por fill = grupo 6.6.9.3 Mudanças das cores manualmente As cores da Figura 6.63 ficaram um pouco espalhafatosas para o gosto do autor e serão modificadas, usando a função scale_fill_manual(). As barras assumirão cores salmão em tons diferentes e as bordas serão pretas. A escolha das cores é pessoal. Assim, resultará em um gráfico como o da Figura 6.64. ggplot(data = rnt) + geom_bar(aes(x = categIdade, y = after_stat(count/sum(count)), fill = categIdade), colour = &quot;black&quot;, show.legend = FALSE) + labs(y = &quot;Frequência&quot;, x = &quot;Faixa Etária&quot;) + scale_y_continuous(expand = expansion(mult = c(0,0.05)), labels = percent_format (accuracy = 0.1, decimal.mark = &quot;,&quot;)) + scale_fill_manual(values = c(&quot;lightsalmon1&quot;, &quot;lightsalmon3&quot;, &quot;lightsalmon4&quot;)) + theme_bw() Figura6.64: Cores de escolha pessoal 6.6.9.4 Cores de acordo com uma determinada paleta O ggsci é um pacote que oferece uma coleção de paletas de alta qualidade inspiradas em cores usadas em revistas científicas, bibliotecas de visualização de dados, filmes de ficção científica e programas de TV. As paletas de cores no ggsci estão disponíveis como escalas ggplot2. Para todas usa-se as seguintes funções: scale_color_palname() e scale_fill_palname(). Por exemplo, para a paleta do Lancet, usa-se para o preenchimento: scale_fill_lancet() . O pacote ggsci deve ser instalado e carregado para usar estas paletas. A Figura 6.65 mostra a modificação das cores conforme a paleta do periódico Lancet. ggplot(data = rnt) + geom_bar(aes(x = categIdade, y = after_stat(count/sum(count)), fill = categIdade), show.legend = FALSE) + labs(y = &quot;Frequência&quot;, x = &quot;Faixa Etária&quot;) + scale_y_continuous(expand = expansion(mult = c(0,0.05)), labels = percent_format (accuracy = 0.1, decimal.mark = &quot;,&quot;)) + scale_fill_lancet() + theme_bw() Figura6.65: Cores conforme a paleta do Lancet 6.6.10 Exemplo final: Gráfico de barra de erro com colunas Um gráfico de barras de erro será usado para visualizar a influência do sexo e do tabagismo materno no peso do recém-nascido. Será incluído a representação das colunas (barras) e as barras de erro com intervalo de confiança de 95%, calculado usando média ± margem de erro, onde a margem de erro = 1.96 × erro padrão. Estes conceitos serão discutidos adiante no Capítulo 10. 6.6.10.1 Resumo dos dados usados Em primeiro lugar, faz-se um resumo dos dados que serão usados no gráfico: resumo &lt;- rnt %&gt;% group_by(sexo, fumo) %&gt;% dplyr::summarise(n = n(), media = mean(pesoRN, na.rm = TRUE), dp = sd(pesoRN, na.rm = TRUE), me = 1.96 * dp/sqrt(n)) resumo ## # A tibble: 4 × 6 ## # Groups: sexo [2] ## sexo fumo n media dp me ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 masc sim 122 3162. 464. 82.4 ## 2 masc não 470 3303. 453. 40.9 ## 3 fem sim 110 2998. 503. 94.1 ## 4 fem não 383 3190. 435. 43.6 Onde, dp = desvio padrão e me = margem de erro. O objeto resumo pertence a classe data.frame e será empregado na construção do gráfico. 6.6.10.2 Gráfico de barra de erro Inicialmente se constrói, usando o dataframe resumo, um gráfico de barra. A seguir, acrescenta-se outra camada para incluir a barra de erro. Em sequência, coloca-se os rótulos; a função scale_y_continous() coloca o início das barras em zero e as cores são manualmente colocadas conforme a escolha do autor (Figura 6.66). Observe que o rótulo da legenda, na função labs(), foi determinado com fill = \"Tabagismo\", porque as cores das barras foram estabelecidas na estética do ggplot com o mesmo argumento (fill = fumo). ggplot(resumo, aes(x=sexo, y=media, fill=fumo)) + geom_bar(stat=&quot;identity&quot;, color=&quot;black&quot;, position=position_dodge()) + geom_errorbar(aes(ymin=media-0, ymax=media+me), width=.2, position=position_dodge(.9)) + labs(x=&quot;Sexo&quot;, y = &quot;Peso do RN (g)&quot;, fill = &quot;Tabagismo&quot;, caption = &quot;RN = Recém-nascido&quot;)+ scale_y_continuous(expand = expansion(mult = c(0,0.05))) + theme_classic() + scale_fill_manual(values=c(&#39;gray80&#39;,&#39;darkslategray1&#39;)) Figura6.66: Gráfico de barra de erro no ggplot2 OBS.: Clique em ggplot2::cheat sheet para obter a planilha de dicas do ggplot2. Para maiores detalhes: https://haozhu233.github.io/kableExtra/awesome_table_in_pdf.pdf ou consulte a ajuda digitando no Console ?kable() ou help(kable).↩︎ Consulte https://cran.r-project.org/web/packages/kableExtra/kableExtra.pdf↩︎ Poderiam ser transformados em fatores sem trocar os rótulos e manter os números 1 e 2, como se fossem palavras. O autor prefere usar nomes.↩︎ O número de elementos pode variar dependendo de como você os agrupa e da pergunta a ser respondida.↩︎ Consulte https://plotly.com/ggplot2/layers/Stats/stat_summary/↩︎ Para maiores detalhes: https://rpkgs.datanovia.com/ggpubr/reference/ggerrorplot.html↩︎ Para outras opções do tipo de linha, pode-se usar a função show_lines_types() do pacote ggpubr↩︎ Para maiores detalhes consulte: https://tidyverse.github.io/ggplot2-docs/reference/theme.html.↩︎ As cores podem ser consultadas na folha de R Colors da Universidade de Columbia, criada pelo Dr. Ying Wei ( http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf↩︎ "],["introdução-à-teoria-das-probabilidades.html", "Capítulo 7 Introdução à Teoria das Probabilidades 7.1 Pacotes necessários neste capítulo 7.2 Introdução 7.3 Processo aleatório 7.4 Definição frequentista 7.5 Propriedades das probabilidades 7.6 Distribuição de Probabilidades 7.7 Distribuição Normal 7.8 Distribuição Binomial 7.9 Distribuição de Poisson", " Capítulo 7 Introdução à Teoria das Probabilidades 7.1 Pacotes necessários neste capítulo pacman::p_load(dplyr, readxl) 7.2 Introdução A teoria das probabilidades é a base sobre a qual a estatística é desenvolvida. Os jogos de azar deram um grande impulso ao conhecimento da moderna teoria das probabilidades, principalmente, pelo trabalho de Blaise Pascal (1623-1662), em parceria com Pierre de Fermat (1601-1665). Eles foram estimulados por um escritor francês e matemático amador, Antoine Gombaud (1607-1684), conhecido como Chevalier de Méré, que era muito interessado em jogos de azar(84). A Teoria das probabilidades permite que seja possível modelar populações, experimentos ou qualquer situação que possa ser considerada aleatória. Estes modelos possibilitam fazer inferência sobre populações a partir da observação de uma amostra dessa população. Ao usar apenas uma parte da população, inevitavelmente, é cometido um erro o erro amostral. Este erro amostral pode ser dimensionado pela teoria das probabilidades. Existem duas interpretações alternativas de probabilidades: a frequentista e a bayesiana (85). Neste livro, será discutida, basicamente, a definição de probabilidade frequentista. O processo bayesiano de formulação de um modelo probabilístico faz uso do conhecimento subjetivo, estabelecendo uma especificação a priori, combinado com a informação objetiva ou empírica. A teoria bayesiana é a estrutura integradora dessas duas fontes de informação, derivando como resultado a distribuição a posteriori dos parâmetros de interesse. Na seção 18.2, sobre análise de testes diagnósticos, será abordado alguns aspectos relacionados a teoria bayesiana em medicina. 7.3 Processo aleatório Um processo ou experimento é dito aleatório quando em uma situação se sabe quais os resultados que podem acontecer, mas não se sabe qual resultado particular irá acontecer. Por exemplo, quando uma moeda é lançada, se conhece que a probabilidade de o desfecho cara ocorrer é de 50%, mas se desconhece o que irá ocorrer até que a moeda esteja no chão. O número de caras que podem surgir em vários lançamentos da moeda é chamado de variável aleatória, ou seja, uma variável que pode assumir mais de um valor com determinadas probabilidades (86). Da mesma forma, um dado lançado pode mostrar seis faces, numeradas de um a seis, com igual probabilidade de 16,7%. Portanto, quando a probabilidade é associada a todos os conjuntos de valores possíveis de uma variável, diz-se que ela é aleatória. O conjunto de todos os possíveis resultados de um experimento aleatório é denominado espaço amostral. Na área da saúde, trabalha-se com uma infinidade de variáveis aleatórias, por exemplo, o número de filhos de uma mulher, o número de mortos diários em uma epidemia, o número de vacinados em uma campanha, etc. Essas variáveis são a variáveis aleatórias discretas, pois apenas permitem ser quantificadas por processo de contagem. Por outro lado, o peso, a altura de uma mulher são ditos variáveis aleatórias contínuas, pois podem assumir qualquer valor real entre uma medida e outra, dependendo da precisão do aparelho usado. Em geral, variáveis aleatórias são representadas por letras maiúsculas, como X, Y e Z e sua a probabilidade, por exemplo, pode ser denotada por: \\(P(X)\\). 7.4 Definição frequentista A probabilidade se relaciona a eventos futuros ou que ainda não ocorreram, desta forma a probabilidade pode ser entendida como uma medida de incerteza em relação ao evento. A probabilidade de um evento ocorrer, em determinadas circunstâncias, pode ser definida como a proporção de vezes que o evento é observado quando o experimento é repetido um número infinitamente grande de vezes (85). Pode-se dizer que a visão frequentista define a probabilidade como uma frequência de longo prazo. A chamada Lei dos Grandes Números diz que à medida que múltiplas observações são coletadas, a proporção observada de ocorrências de um determinado desfecho, após n ensaios, converge para a probabilidade real P desse desfecho. Ou seja, quanto mais vezes for repetido uma experiência, a melhor estimativa de probabilidade tende a ocorrer. Suponha que seja lançada uma moeda honesta repetidas vezes. Por definição, essa é uma moeda que tem \\(P(cara)=0,5\\). O que se observaria? O autor fez 20 lançamentos seguidos com uma mesma moeda e obteve o seguinte resultado, onde 1 = cara (Figura 7.1): Figura7.1: 20 lançamentos seguidos de uma moeda Neste caso, 10 (50%) desses lançamentos deram cara. Agora, suponha que foram feitos registros do número de caras (\\(n_1\\)) dos primeiros lançamentos (N) e calculadas as proporções de caras (\\(n_1⁄N\\)) todas as vezes. O resultado está na Figura 7.2. Figura7.2: Proporção em 20 lançamentos de moeda Observa-se, nessa sequência, que a proporção de caras flutua muito, variando de 0,17 a 0,75. Se o número de lançamentos for aumentando tem-se a sensação de que a proporção se aproxima da “correta”. Por exemplo, com 100 jogadas, obteve-se 53 caras (0,53); com 150 jogadas, 79 (0,53) e com 200 jogadas, 111 (0,56). Quando N se aproximar do infinito (\\(N \\to\\infty\\)) a proporção de caras convergirá para 0,50. A definição frequentista de probabilidade segue essa definição. Ninguém consegue um número infinito de lançamentos de moedas, mas um computador pode simular milhares de lançamentos. A Figura 7.3 mostra o que acontece com a proporção \\(n_1⁄N\\) à medida que N aumenta em lançamentos de moedas. As simulações foram repetidas 4 vezes somente para ter certeza de que o que aconteceu não foi obra do acaso. Figura7.3: Proporção à medida que N aumenta em lançamentos de moedas Embora nenhuma das simulações tenha realmente terminado com um valor exato de 0,5, elas se aproximaram, oscilando muito pouco em torno desse valor. 7.4.1 Aplicando a visão frequentista no dia a dia A definição frequentista também pode ser aplicada no cotidiano. Será utilizada a altura de mulheres, uma medida numérica contínua. No conjunto de dados dadosMater.xlsx (veja Capítulo 5), encontra-se o registro da altura de 1368 mulheres. Essas alturas serão selecionadas e colocadas em um objeto, denominado dadosAltura. dadosAltura &lt;- read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) %&gt;% select(altura) Usando a função summary(), será feito um resumo da variável altura: summary(dadosAltura$altura) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.400 1.550 1.600 1.598 1.650 1.850 A mediana da altura das gestantes é 1,60 m. Ou seja, metade dessas mulheres têm uma altura acima de 1,60 m. Em um longo conjunto de sorteios, a probabilidade de uma mulher ter altura acima de 1,60 m é 50%. O percentil 75 (3º quartil) é igual a 1,65 m, a probabilidade de estar acima deste valor, portanto, é 25%. É possível encontrar a probabilidade de a altura estar acima, abaixo ou entre quaisquer valores. Quando se faz a mensuração de uma variável contínua, fica-se limitado ao método usado, portanto, quando se diz que uma mulher tem 1,60 m, significa dizer que está entre 159,5 e 160,5 m, dependendo da precisão do instrumento de medição. Dessa maneira, o interesse está na probabilidade de a variável aleatória assumir valores entre certos limites. A probabilidade de encontrar um valor exatamente de 1,60 m é quase igual a zero. Como se verá adiante, isto pode ser verificado com bastante facilidade, no R, calculando a distância que esta medida (1,60 m) está da média em número de desvios padrão (escore z): z &lt;- (1.60 - mean(dadosAltura$altura))/sd(dadosAltura$altura) z ## [1] 0.03103551 Observe que a medida de 1,60 m está muito próxima da média e isto é um indicativo de que essa variável tem uma distribuição praticamente simétrica. Sabendo a distância, em números de desvios padrão, que 1,60 m está da média, qual a probabilidade de encontrar, na maternidade do HGCS, uma parturiente que tenha exatamente esta altura? Para responder a essa pergunta, será usada a função pnorm() (veja adiante na Seção (#sec-dnp)) que utiliza o escore z, a média e o desvio padrão para encontrar essa proporção que, multiplicada por 100, fornece a percentagem. p &lt;- pnorm (z, mean(dadosAltura$altura),sd(dadosAltura $altura)) p ## [1] 7.387473e-127 O R por padrão retorna números grandes como notação científica. O resultado dessa operação é um número tão grande que para escrevê-lo sem este tipo de notação, seriam necessários 127 dígitos decimais. O resultado não caberia em apenas uma linha. Ficaria assim, suprimindo a notação científica 26: options(scipen =999) p ## [1] 0.0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000007387473 options(scipen = 0) Ou seja, um número tão próximo de zero que poderia muito bem ser zero! 7.5 Propriedades das probabilidades As seguintes propriedades simples decorrem da definição de probabilidade. Sendo E um evento aleatório, a \\(P[E]\\) está entre 0 e 1, ou seja \\(0\\le P[E]\\le 1\\). Quando o evento certamente não ocorre, a probabilidade é 0, quando sempre ocorre a probabilidade é 1. Quando a probabilidade for igual a 0,50 tem-se máxima incerteza. Regra de adição (regra do “ou”) Dois eventos A e B são mutuamente exclusivos, ou seja, quando A acontece, B não pode acontecer. Então, a probabilidade de que um ou outro aconteça é a soma de suas probabilidades. Por exemplo, um dado lançado pode mostrar um ou dois, mas não ambos. A probabilidade de mostrar um ou dois é igual a \\(1/6 + 1/6 = 1/3\\). \\[ P[A ou B]=P[A]+P[B] \\] Se A e B não são mutuamente exclusivos, ou seja, quando A acontece pode também ocorrer B. Por exemplo, o nascimento de uma menina pode ser concomitante com o fato de ser branca. \\[ P[A ou B]=P[A]+P[B]-P[A \\space e \\space B] \\] Regra de multiplicação (regra do “e”) Suponha que dois eventos (A e B) sejam independentes, ou seja, saber que um aconteceu não nos diz nada sobre se o outro aconteceu. Então, a probabilidade de que ambos aconteçam é o produto de suas probabilidades. Por exemplo, suponha que jogamos duas moedas. Uma moeda não influencia a outra, portanto os resultados dos dois lançamentos são independentes e a probabilidade de ocorrerem duas caras é 050 × 0,50 = 0,25. \\[ P[A \\quad e\\quad B]=P[A]×P[B] \\] Se os eventos são dependentes, a probabilidade que ambos aconteçam é igual a: \\[ P[A \\quad e \\quad B]=P[A]×P[B \\rvert A] \\] Com essas propriedades simples e outras mais complexas, é possível construir algumas ferramentas matemáticas extremamente poderosas, mas isso não faz parte do objetivo deste livro e não se entrará em detalhes. 7.6 Distribuição de Probabilidades Um conjunto de eventos que são mutuamente excludentes e que inclui todos os eventos que podem acontecer, é chamado de exaustivo. A soma de suas probabilidades é 1. O conjunto dessas probabilidades constitui uma distribuição de probabilidade. Existem diversos modelos probabilísticos que procuram descrever vários tipos de variáveis aleatórias discretas ou contínuas. Estas distribuições também são chamadas de modelos probabilísticos estocástico que são definidas por duas funções matemáticas: a função de probabilidade (fp) para variáveis discretas, que atribui a cada valor a sua probabilidade de ocorrência (P(X=x)) e função densidade de probabilidade (fdp) para variáveis contínuas. A função de probabilidade é a função que atribui probabilidades a cada um dos possíveis valores da variável aleatória discreta, usando, em geral, as frequências relativas, apresentadas em uma tabela de frequência. O modelo de Bernoulli ou Binomial e o modelo de Poisson são exemplos de modelo probabilístico de variáveis discretas. A função densidade de probabilidade é a função que atribui probabilidade a qualquer intervalo de número reais, ou seja, um conjunto de valores não enumerável (infinito). Não é possível atribuir probabilidades para um determinado valor, é possível apenas para um intervalo. Por exemplo, o peso dos recém-nascidos. Para atribuir probabilidade a intervalos de valores é utilizada uma função e as probabilidades são representadas por áreas. Existem diversos modelos contínuos de probabilidade, mas o mais importante deles, é o modelo normal, também conhecido como modelo gaussiano. 7.7 Distribuição Normal O modelo probabilístico normal ou gaussiano é extremamente importante em estatística, pois serve como um fundamento para técnicas de inferência. Variáveis como os pesos dos recém-nascidos a termo, as alturas das mulheres adultas, a renda familiar em reais e muitas outras variáveis, na natureza, se ajustam ao modelo da distribuição normal. O modelo de distribuição normal sempre descreve uma curva simétrica, unimodal e em forma de sino (Figura 7.4). Figura7.4: Curva normal. Uma distribuição normal é descrita por meio de dois parâmetros: a média da distribuição \\(\\mu\\) e o desvio padrão da distribuição \\(\\sigma\\). Em função dessa informação, observe como a distribuição normal funciona se esses parâmetros forem alterados. Como é fácil prever, alterar a média desloca a curva de sino para a esquerda ou para a direita, enquanto a alteração do desvio padrão estende ou achata a curva, ou seja, muda a dispersão da distribuição. A Figura 7.5, mostra a distribuição normal com média 0 e desvio padrão 1, na curva à direita, a distribuição normal com média 1.5 e desvio padrão 1. Sobrepondo-se à curva da esquerda observa-se uma curva mais achatada (verde) que tem média 0 e desvio padrão 1.5. Observa-se, como mencionado, que modificando os parâmetros da curva, altera-se a posição ou o formato da curva. curve (dnorm (x, mean=0, sd=1), col=&quot;dodgerblue3&quot;, lty=1, lwd=2, ylim = c(0, 0.4), xlim = c(-4.5, 4.5), ylab = &quot;Densidade&quot;, xlab = &quot;X&quot;, bty = &quot;n&quot;) box(bty = &quot;L&quot;) abline (v= 0, lwd = 1, lty = 2, col = &quot;dodgerblue3&quot;) curve (dnorm (x, mean=0, sd=1.5), col=&quot;darkolivegreen3&quot;, lty=1, lwd=2, add=T) curve (dnorm (x, mean=1.5, sd=1), col=&quot;firebrick3&quot;, lty=1, lwd=2, add=T) abline (v= 0, lwd = 1, lty = 2, col = &quot;firebrick3&quot;) Figura7.5: Curvas normais com modificação dos parâmetros. 7.7.1 Características da distribuição normal A curva normal apresenta as seguintes características: A média e o desvio padrão descrevem exatamente uma distribuição normal, eles são chamados de parâmetros da distribuição. Se uma distribuição normal tem média \\(\\mu\\) e desvio padrão \\(\\sigma\\), pode-se escrever a distribuição como \\(N (\\mu,\\sigma)\\). As três distribuições dos gráficos da Figura 7.5 podem ser escritas como: Curva azul \\(\\to\\) \\(N(\\mu = 0,\\sigma = 1)\\) Curva verde \\(\\to\\) \\(N(\\mu = 0,\\sigma = 1.5)\\) Curva vermelha \\(\\to\\) \\(N(\\mu = 1.5,\\sigma = 1)\\) Na distribuição normal, a média, a mediana e a moda coincidem. A curva normal é simétrica em torno da média (\\(\\mu\\)). As extremidades da curva, em ambos os lados da média, se estendem cada vez mais próximas do eixo x (abscissa) sem jamais tocá-lo. É assintótica. Os pontos de inflexão da curva são \\(\\mu - \\sigma\\) e \\(\\mu + \\sigma\\). A área total sob a curva é 1 ou 100%. 7.7.2 Distribuição normal padronizada Cada variável aleatória contínua tem a sua média e seu desvio padrão e, portanto, a sua curva normal correspondente. Para facilitar a comparação entre variáveis, foi criado o conceito de curva normal padronizada, que é uma curva normal com média 0 e desvio padrão 1. A distribuição normal padrão também pode ser chamada de distribuição normal centrada ou reduzida. Para calcular probabilidades associadas a distribuição normal, costuma-se converter a variável aleatória original X, em unidades reduzidas ou padronizadas, denominadas de **escore *Z** ou escore padrão. Essa transformação é realizada pela equação que indica o número de desvios padrão envolvidos no afastamento do valor x em relação à média da população: \\[ z =\\frac{x-\\mu}{\\sigma} \\] onde: z \\(\\longrightarrow\\) escore z x \\(\\longrightarrow\\) valor qualquer da variável aleatória X \\(\\mu\\) \\(\\longrightarrow\\) média da variável X \\(\\sigma\\) \\(\\longrightarrow\\) desvio padrão da variável X Qualquer distribuição de uma variável aleatória normal pode ser padronizada, usando o escore z. Isto permite que se calcule a probabilidade de se encontrar determinados intervalos de valores (87). Como exemplo, será usada a altura das mulheres. É, praticamente, impossível saber o valor da média populacional, por isso. costuma-se usar a média aritmética como um estimador da média populacional. Dessa forma, variável altura do objeto dadosAltura poderá utilizada com estimativa da média populacional . Em primeiro lugar, se construirá um tibble de nome resumo: resumo &lt;- dadosAltura %&gt;% dplyr::summarise(n = n(), media = mean(altura, na.rm = TRUE), dp = sd(altura, na.rm = TRUE), min = min(altura, na.rm = TRUE), max = max(altura, na.rm = TRUE)) resumo ## # A tibble: 1 × 5 ## n media dp min max ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1368 1.60 0.0655 1.4 1.85 Assim, pode-se verificar quantos desvios padrão uma mulher, pertencente a essa amostra, com 1,725m está afastada da média.. Usando o R, pode-se calcular da seguinte maneira: z &lt;- (1.725 - resumo$media)/resumo$dp round(z, 2) ## [1] 1.94 Esta mulher está distante praticamente 2 desvios padrão acima da média da sua população. Portanto, ela é considerada alta. Por que? Para responder a essa pergunta, há necessidade de calcular a probabilidade de encontrar uma mulher com esta altura, nesta população. Para responder a essa pergunta, há necessidade de calcular a probabilidade de encontrar uma mulher com esta altura, nesta amostra. No R, existem as funções dnorm(), pnorm() e qnorm(), que permitem calcular a densidade de probabilidade, a distribuição cumulativa e a função quantílica da distribuição normal para um conjunto de valores. Além dessas, há a função rnorm() permite obter observações aleatórias que seguem uma distribuição normal(88). 7.7.2.1 Função pnorm() A função pnorm() fornece a Função de Distribuição Cumulativa (CDF) da distribuição Normal, que é a probabilidade de que a variável X contenha um valor menor ou igual a x. Argumentos: q \\(\\rightarrow\\) vetor de quantis mean \\(\\rightarrow\\) média sd \\(\\rightarrow\\) desvio padrão lower.tail \\(\\rightarrow\\) Se TRUE, as probabilidades são \\(P(X\\le x)\\), caso contrário \\(P(X &gt; x)\\) Se for usado \\(mean = 0\\) e \\(sd = 1\\), o valor de q = z, caso contrário, toma-se os valores da média, o desvio padrão da população e o valor de x. Com esta função, é possível responder a pergunta feita anteriormente em relação a probabilidade de encontrar uma mulher com mais de 1,725m, equivalente a 1.94 desvios padrão acima da média, em uma população com média = 1.5979678 e desvio padrão = 0.0654787. p &lt;- pnorm(z, mean = 0, sd = 1, lower.tail = FALSE) p ## [1] 0.02618654 Ou, usando os valores: pnorm(1.725, mean = resumo$media, sd = resumo$dp, lower.tail = FALSE) ## [1] 0.02618654 Observa-se que, nesta amostra, apenas 2.6% das mulheres têm acima de 1,725m, razão de ser considerada uma mulher alta. Ou seja, é pouco provável encontrar mulheres acima dessa altura, nesta amostra. Para representar graficamente essa pequena probabilidade, será construída uma curva com essa pequena área sombreada, colorida em vermelho. Para isso, será feito uso de uma função própria, denominada normal_area(). Ela pode ser obtida aqui para ser baixada em seu diretório de trabalho para uso posterior 27. A Figura 7.6 representa com clareza esta pequena probabilidade. Foi usada a função text() para escrever o valor da probabilidade. source(&quot;Arquivos/normal_area.R&quot;) normal_area(media = 0, dp = 1, linf = 1.94, lsup = 3, cor = &quot;tomato&quot;, lwd = 2 ) text(2.6, 0.05, &quot;2.6%&quot;) Figura7.6: Probabilidade de encontrar mulheres com mais de 1,725m 7.7.2.2 Função qnorm() A função qnorm() permite encontrar o quantil q para qualquer probabilidade p. Portanto, a função qnorm é o inverso da função pnorm(). Argumentos: p \\(\\rightarrow\\) vetor de probabilidades mean \\(\\rightarrow\\) média sd \\(\\rightarrow\\) desvio padrão lower.tail \\(\\rightarrow\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\) No exemplo anterior, a probabilidade de se encontrar mulheres, na maternidade, com mais de 1,725m foi de 2.6%. Poderia ser calculado com a função qnorm() qual o escore z correspondente: qnorm(p, mean = 0, sd = 1, lower.tail = FALSE) ## [1] 1.940054 7.7.2.3 Função dnorm() Essa função retorna o valor da função de densidade de probabilidade (pdf) da distribuição normal dada uma certa variável aleatória X, uma média populacional \\(\\mu\\) e o desvio padrão populacional \\(\\sigma\\). Argumentos: x \\(\\rightarrow\\) vetor de quantis mean \\(\\rightarrow\\) média sd \\(\\rightarrow\\) desvio padrão Embora x represente a variável independente da pdf para a distribuição normal, também é útil pensar em x como um escore z. Por exemplo, a densidade de probabilidade quando x = 0 é igual: dnorm(x = 0, mean = 0, sd = 1) ## [1] 0.3989423 Para se construir uma curva de densidade de probabilidades normal ( \\(X \\sim N(μ=0,σ=1)\\)), basta aplicar a função dnorm() a uma sequência contínua de escores z. O vetor de escores z é obtido com a função seq(), como mostrado a seguir: escores_z &lt;- seq(-3,3, by = 0.05) escores_z ## [1] -3.00 -2.95 -2.90 -2.85 -2.80 -2.75 -2.70 -2.65 -2.60 -2.55 -2.50 -2.45 ## [13] -2.40 -2.35 -2.30 -2.25 -2.20 -2.15 -2.10 -2.05 -2.00 -1.95 -1.90 -1.85 ## [25] -1.80 -1.75 -1.70 -1.65 -1.60 -1.55 -1.50 -1.45 -1.40 -1.35 -1.30 -1.25 ## [37] -1.20 -1.15 -1.10 -1.05 -1.00 -0.95 -0.90 -0.85 -0.80 -0.75 -0.70 -0.65 ## [49] -0.60 -0.55 -0.50 -0.45 -0.40 -0.35 -0.30 -0.25 -0.20 -0.15 -0.10 -0.05 ## [61] 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 ## [73] 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 1.05 1.10 1.15 ## [85] 1.20 1.25 1.30 1.35 1.40 1.45 1.50 1.55 1.60 1.65 1.70 1.75 ## [97] 1.80 1.85 1.90 1.95 2.00 2.05 2.10 2.15 2.20 2.25 2.30 2.35 ## [109] 2.40 2.45 2.50 2.55 2.60 2.65 2.70 2.75 2.80 2.85 2.90 2.95 ## [121] 3.00 Agora, usando a função dnorm(), será construído conjunto de valores de densidade de probabilidade correspondentes aos escores z obtidos anteriormente: valores_d &lt;- dnorm(escores_z, mean = 0, sd = 1) Estes valores serão plotados para construir a curva normal (Figura 7.7): plot(valores_d, type = &quot;l&quot;, lwd = 2, col = &quot;steelblue&quot;, xaxt = &quot;n&quot;, ylab = &quot;Densidade de Probabilidade&quot;, xlab = &quot;Escores z&quot;) # Rótulos do eixo x axis(side = 1, at = which(valores_d == dnorm(0)), labels = c(0)) axis(side = 1, at=which(valores_d == dnorm(1)), labels=c(-1, 1)) axis(side = 1, at=which(valores_d == dnorm(2)), labels=c(-2, 2)) axis(side = 1, at=which(valores_d == dnorm(3)), labels=c(-3, 3)) Figura7.7: Função densidade de probabilidade. Os argumentos básicos a serem informados da função axis() são: side=, at= e labels=. Esses argumentos determinam qual eixo será preenchido, qual a posição dos valores no eixo e a sequência de valores a ser preenchida, respectivamente. O argumento side= recebe valores que vão de 1 a 4: 1 = eixo inferior, 2 = eixo lateral esquerdo, 3 = eixo superior, 4= eixo lateral direito. Ou seja, partindo do eixo inferior (eixo x), os valores aumentam até 4 seguindo o sentindo horário para os quatros lados do gráfico. No exemplo, foi modificado o eixo x, logo side = 1. O argumento at = estabelece os pontos (densidades de probabilidade) do eixo x que receberão os rótulos, especificados no argumento label =. Como se pode ver, dnorm() fornece a “altura” do pdf da distribuição normal em qualquer escore z que se forneça como argumento. 7.7.2.4 Função rnorm() A função rnorm() gera n números aleatórios com distribuição normal com média \\(\\mu\\) e desvio padrão \\(\\sigma\\). Argumentos: n \\(\\rightarrow\\) número de observações a serem geradas mean \\(\\rightarrow\\) média sd \\(\\rightarrow\\) desvio padrão Com esta função é possível, por exemplo, gerar 10 observações de uma distribuição normal: rnorm(10) ## [1] 1.03581157 0.58535273 -0.79325083 -0.21254049 -0.09663871 -0.37835041 ## [7] -0.13180399 -0.29348458 2.13748109 0.28994796 No entanto, deve-se notar que, se uma “semente” (seed) não for especificada, a saída não será reproduzível, ou seja, cada vez que o comando for executado, retornará um novo conjunto de observações: rnorm(10) ## [1] 0.5476140 -0.5789843 0.2421454 0.2876592 0.3959830 0.1836206 ## [7] 1.9827846 -1.9667769 0.5447282 0.5297082 Cada vez que este comando for reproduzido, retornará uma nova série de 10 números diferentes do anterior. Para tornar o código reproduzível, retornando o mesmo conjunto de valores, deve-se usar uma “semente” (seed), usando a função set.seed(), cujo argumento é um número que identificará a série gerada, no exemplo, pela função rnorm(). O valor do número (“semente”) não é importante, é apenas um identificador. Para ilustrar, será construído dois conjuntos de 10 números que serão recebidos pelos objetos x e y. Para gerar o conjunto de números x, será usado o número 123 como “semente”. A “semente” funciona como uma espécie de marca. Para o y não será usado a função set.seed(): n &lt;- 10 set.seed (123) x &lt;- rnorm (n) x ## [1] -0.56047565 -0.23017749 1.55870831 0.07050839 0.12928774 1.71506499 ## [7] 0.46091621 -1.26506123 -0.68685285 -0.44566197 y &lt;- rnorm(n) y ## [1] 1.2240818 0.3598138 0.4007715 0.1106827 -0.5558411 1.7869131 ## [7] 0.4978505 -1.9666172 0.7013559 -0.4727914 Comparando os conjuntos com a função identical() do R base, observa-se que os conjuntos são diferentes: identical(x, y) ## [1] FALSE Agora, repetindo os mesmos comandos, mas usando antes a mesma “semente”, observa-se que os conjuntos são idênticos. set.seed (123) x &lt;- rnorm (n) x ## [1] -0.56047565 -0.23017749 1.55870831 0.07050839 0.12928774 1.71506499 ## [7] 0.46091621 -1.26506123 -0.68685285 -0.44566197 set.seed (123) y &lt;- rnorm(n) y ## [1] -0.56047565 -0.23017749 1.55870831 0.07050839 0.12928774 1.71506499 ## [7] 0.46091621 -1.26506123 -0.68685285 -0.44566197 identical(x, y) ## [1] TRUE Outros usos da função rnorm() A função rnorm()será usada para gerar três vetores diferentes de números aleatórios de uma distribuição normal. set.seed(1234) n10 &lt;- rnorm(10, mean = 0, sd = 1) n100 &lt;- rnorm(100, mean = 0, sd = 1) n10000 &lt;- rnorm(10000, mean = 0, sd = 1) Emn sequência, serão construídos histogramas (Figura 7.8), onde se pode observar que, aumentando o número de observações, tem-se gráficos que irão progressivamente se aproximando da verdadeira função de densidade normal. A função par(mfrow(1,3)) coloca os gráficos gerados em uma mesma linha e em três colunas. No final, se repete a função, restaurando as configurações basais de plotagem (uma linha e uma coluna). # Este comando coloca os gráficos em uma mesma linha, o argumento mfrow(c(1,3)) diz ao R para construir uma linha e três colunas: par(mfrow=c(1,3)) # Histogramas hist(n10, breaks = 5, main = &quot;n =10&quot;, ylab = &quot;Frequência&quot;) hist(n100, breaks = 20, main = &quot;n =100&quot;, ylab = &quot;Frequência&quot;) hist(n10000, breaks = 50, main = &quot;n =10000&quot;, ylab = &quot;Frequência&quot;) Figura7.8: Histogramas construídos com amostras geradas pela função rnorm. # Restaura as configurações basais de plotagem par(mfrow=c(1,1)) Observe também que à medida que n aumenta, a distribuição dos dados caracteriza-se como uma distribuição normal. Na Seção 9.2.2, este assunto voltará à cena. 7.7.3 Regra Empírica 68-95-99.7 A regra empírica diz que, se uma população de um conjunto de dados tem uma distribuição normal com média 0 e desvio padrão 1 (\\(X\\sim N(\\mu=0,\\sigma=1)\\)) pode-se afirmar que aproximadamente, 68%, 95% e 99,7% dos valores encontram-se, respectivamente, dentro de \\(\\pm\\) 1, 2 e 3 desvio padrão acima e abaixo média. Essa regra pode ser usada para descrever uma população e ajudar a decidir se uma amostra de dados veio de uma distribuição normal. Se uma amostra é grande o suficiente e a observação do histograma tem um formato parecido com um sino, é possível verificar se os dados seguem as especificações 68-95-99,7%. Se sim, é razoável concluir que os dados vieram de uma distribuição normal. Exemplo: Será gerado um vetor de 1000 números, gerados com a função rnorm() e um histograma com curva normal sobrposta: dados &lt;- rnorm(10000, mean = 0, sd = 1) media &lt;- mean(dados, na.rm =TRUE) dp &lt;- sd(dados, na.rm =TRUE) hist(dados, ylim = c (0, 0.4), xlim = c (-3, 3), main= NULL, ylab = &quot;Densidade de Probabilidade&quot;, xlab = &quot;Escore z&quot;, col =&quot;dodgerblue4&quot;, freq = FALSE, border = &quot;white&quot;) box (bty = &quot;L&quot;) curve (dnorm (x, mean=media, sd=dp), col=&quot;red&quot;, lty=1, lwd=2, add=TRUE) abline(v=c(-1, 1), col = &quot;green&quot;, lty = 2, lwd = 2) text(-1, 0.40, cex = 1.5, col = &quot;dodgerblue4&quot;, &quot;A&quot;) text(1, 0.40, cex = 1.5, col = &quot;dodgerblue4&quot;, &quot;B&quot;) Figura7.9: Histograma com curva de densidade de probabilidade sobreposta No histograma (Figura 7.9), a probabilidade entre os escores z -1 e 1 (entre as duas linhas tracejadas) é igual a aproximadamente 68%. Pode-se calcular isto facilmente, usando a função pnorm(): Probabilidade abaixo de z = 1, abaixo do ponto B B &lt;- pnorm (1, 0, 1) B &lt;- round(B, 3)*100 B ## [1] 84.1 Probabilidade abaixo de z = -1, abaixo do ponto A A &lt;- pnorm (-1, 0, 1) A &lt;- round(A, 3)*100 A ## [1] 15.9 Logo , a área abaixo da curva entre A e B é igual a: prob &lt;- B - A prob ## [1] 68.2 7.7.4 Calculando probabilidades em uma distribuição normal Como visto na Seção 7.4.1, a variável altura das mulheres (dadosAltura$altura) tem uma distribuição praticamente simétrica. Usando esses dados (\\(X\\sim N(\\mu=1,598,\\sigma=0,065)\\)), pode-se calcular probabilidades, dadas pela área sob a curva, usando o R. Por exemplo: Exemplo 1: Qual a probabilidade de se encontrar mulheres com altura entre 1,47 e 1,73 m? # Dados mu &lt;- 1.598 sigma &lt;- 0.065 x1 &lt;- 1.47 x2 &lt;- 1.73 # Solução z1 &lt;- (x1 - mu)/sigma z2 &lt;- (x2 - mu)/sigma p1 &lt;- pnorm(x1, mu, sigma) p2 &lt;- pnorm(x2, mu, sigma) p2 - p1 ## [1] 0.9543975 Figura7.10: Probabilidade de alturas entre 1,47 e 1,73m. A probabilidade de alturas entre 1,47 m e 1,73m é igual a 95,4% (Figura 7.10). Exemplo 2: Os dados de uma pesquisa mostram informações sobre o tempo de cirurgia para reconstrução do ligamento cruzado anterior (LCA). A distribuição de probabilidades se a justa à normal com o tempo médio de cirurgia de 129 minutos com um desvio padrão de 14 minutos. Qual a probabilidade de uma cirurgia de reconstrução do LCA requerer um tempo menor do que 100 minutos? # Dados mu &lt;- 129 sigma &lt;- 14 x &lt;- 100 # Solução z &lt;- (x - mu)/sigma p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE) p ## [1] 0.01915938 Figura7.11: Probabilidade do tempo de cirurgia de LCA menor ou igual a 100 minutos. De acordo com a distribuição, 1,92%% das cirurgias irão demandar quantidade de tempo menor do que 100 minutos (Figura 7.11). Se uma cirurgia demorar 160 minutos, o que se conclui em relação a essa informação? # Dados mu &lt;- 129 sigma &lt;- 14 x &lt;- 160 # Solução z &lt;- (x - mu)/sigma p &lt;- pnorm(x, mu, sigma, lower.tail = FALSE) p ## [1] 0.01340457 Figura7.12: Probabilidade do tempo de cirurgia de LCA maior ou igual a 160 minutos. De acordo com a distribuição, 1,34% das cirurgias irão demandar quantidade de tempo \\(\\ge 160\\) minutos (Figura 7.12). Ou seja, é uma probabilidade muito pequena! Exemplo 3: Suponha-se que em uma determinada ilha hipotética existam duas populações etnicamente diferentes onde as mulheres têm as seguintes medidas de altura: população 1 tem μ = 160 cm e σ = 6,6 cm e a população 2 tem μ = 140 cm e σ = 6,6 cm. As alturas de ambas as populações têm distribuição normal. Essas duas populações têm o mesmo aspecto físico, podendo ser distinguidas apenas geneticamente. Qual a probabilidade de uma mulher com 150 cm pertencer a população 1? # Dados mu &lt;- 160 sigma &lt;- 6.6 x &lt;- 150 # Solução z &lt;- (x - mu)/sigma p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE) p ## [1] 0.06486702 Figura7.13: Probabilidade de uma mulher com 150 cmm pertencer a uma população de média igual a 160 cmm. Na população 1, apenas 6.5% das mulheres tem altura \\(\\le 1,50\\) m (Figura 7.13). Em outras palavras, existe pouca probabilidade dessa mulher pertence à população 1. Qual a probabilidade de uma mulher com 150 cm pertencer a população 2? # Dados mu &lt;- 140 sigma &lt;- 6.6 x &lt;- 150 # Solução z &lt;- (x - mu)/sigma p &lt;- pnorm(x, mu, sigma, lower.tail = TRUE) p ## [1] 0.935133 Figura7.14: Probabilidade de uma mulher com 150 cmm pertencer a uma população de média igual a 140 cmm. Na população 2, 93,5% das mulheres tem altura \\(\\le 1,50\\) m (Figura 7.14). Concluindo, ela pode pertencer a qualquer uma das populações. Pode ser uma mulher alta da população 2 ou uma “baixinha” da população 1! 7.8 Distribuição Binomial A distribuição normal padrão é apenas um dos exemplos de distribuição de probabilidade. Uma boa parte das situações se ajustam a ela. Entretanto, diversas situações reais muitas vezes se aproximam de outras distribuições estocásticas definidas por algumas hipóteses. Daí a importância de se conhecer e manipular algumas destas distribuições. Entre elas, a distribuição binomial. Quando um experimento aleatório resulta em um de dois, mutuamente exclusivos, desfechos, tais como vivo/morto, positivo/negativo, sim/não, masculino/feminino é denominado de Ensaio de Bernoulli. Recebeu esta denominação em homenagem ao matemático suíço, Jacob Bernoulli (1654-1705), considerado fundador do cálculo e da teoria da probabilidade (89). A distribuição de frequências que descreve as proporções de um ensaio de Bernoulli, chama-se Distribuição Binomial. A probabilidade binomial dá a probabilidade de determinado desfecho ocorrer em determinado número de ensaios independentes. Uma sequência de ensaios de Bernoulli forma um Processo de Bernoulli. A distribuição binomial é importante para variáveis discretas. Existem poucas condições que precisam ser atendidas antes se considere uma variável aleatória para distribuição binomial: Cada ensaio resulta em um de dois desfechos, mutuamente exclusivos, denominados, arbitrariamente, de sucesso e fracasso; A probabilidade de sucesso é fixa, igual a p, constante em cada ensaio, e a probabilidade de fracasso é igual a 1 – p; O número de repetições n em um ensaio é fixo. Os ensaios são independentes A distribuição binomial é na verdade uma família de distribuições, cujos membros são definidos pelos valores de n e p (parâmetros da distribuição binomial). A probabilidade de sucesso 28, em uma distribuição binomial, é dada pela fórmula: \\[ P(X = x)= C \\times p^x \\times (1 - p)^{n-x} \\] onde n = ensaios, x = sucessos, p = probabilidade de um sucesso e C representa o número possível de combinações em um ensaio. O número de combinações, C de x sucessos entre n repetições podem ser computado pela fórmula: \\[ C = \\frac{n!}{x!(n - x)!} \\] ou, no R, com a função choose (n, x). O modelo de distribuição binomial trata de encontrar a probabilidade de sucesso de um evento que tem apenas dois resultados possíveis em uma série de experimentos. Usando dados de uma distribuição binomial, é possível calcular os valores esperados de uma variável aleatória conforme ela passa por tentativas independentes. Em outras palavras, é possível prever o número exato de caras ou coroas que se deve esperar ao jogar uma moeda um certo número de vezes. Também, pode-se usar a probabilidade binomial cumulativa para encontrar a probabilidade de obter um determinado intervalo de resultados. Por exemplo, saber a probabilidade do nascimento de até três meninos em 10 nascimentos consecutivos quando a probabilidade de nascer um menino é 0,50. O R tem quatro funções embutidas para gerar distribuição binomial. Ela são descritas a seguir. 7.8.1 Funções da distribuição binomial 7.8.1.1 Função pbinom() Esta função retorna o valor da função de densidade cumulativa (cdf) da distribuição binomial dada uma certa variável aleatória q, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). Argumentos: q \\(\\rightarrow\\) vetor de quantis size \\(\\rightarrow\\) numero de ensaios prob \\(\\rightarrow\\) probabilidade de sucesso em cada ensaio lower.tail \\(\\rightarrow\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\) Por exemplo, qual é a probabilidade de nascer até três meninos em cinco nascimentos, sabendo que a probabiliade de nascer um menino é igual a 0.50? pbinom (3, 5, 0.50) ## [1] 0.8125 Isso corresponde a soma das probabilidades de nascer nenhum menino, um menino, dois meninos e três meninos (Figura 7.15). Isto é calculado pela equação \\(P(X = x)\\), vista anteriormente. Colocando no R: n = 5 p = 0.50 x &lt;- 0:5 # Probabilidades de meninos Fx &lt;- (factorial(n)/(factorial(x)*factorial(n-x)))* p^x *(1-p)^(n-x) Fx ## [1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125 Figura7.15: Distribuição binomial, mostrando a P (x &lt; 4) com n = 5 e p = 0.50 ## [1] 0.8125 7.8.1.2 Função qbinom() Esta função retorna o valor da função de densidade cumulativa inversa (cdf) da distribuição binomial dada uma certa variável aleatória q, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). Com o uso desta função, podemos descobrir o quantil da distribuição binomial. Argumentos: p \\(\\rightarrow\\) probabilidade ou vetor de probabilidades size \\(\\rightarrow\\) numero de ensaios prob \\(\\rightarrow\\) probabilidade de sucesso em cada ensaio lower.tail \\(\\rightarrow\\) Se TRUE, as probabilidades são (\\(P \\le x\\)), caso contrário \\(P(X &gt; x)\\) Por exemplo, quantos meninos nascerão em 5 partos com 81.25% de probabilidade cumulativa? qbinom (0.8125, size = 5, prob = 0.50) ## [1] 3 7.8.1.3 Função rbinom() A função rbinom() permite extrair n observações aleatórias de uma distribuição binomial. Os argumentos da função são descritos abaixo: Argumentos: n \\(\\rightarrow\\) número de observações aleatórias a ser gerado size \\(\\rightarrow\\) numero de ensaios prob \\(\\rightarrow\\) probabilidade de sucesso em cada ensaio Para fazer uma simulação de 1000 amostras, aleatoriamente, de tamanho 5 e com probabilidade de nascer menino igual a 0,50, usa-se 29: set.seed(23) menino &lt;- rbinom(n = 1000, size = 5, prob = 0.5) Cada amostra de n = 5 exibe o número de meninos nascidos. Pode-se fazer a média que representa o valor esperado do número de sucessos (nascimento de menino, no exemplo) em um conjunto de ensaios independentes: mean(menino) ## [1] 2.515 Quanto maior o número de variáveis aleatória criadas, mais próximo a média do número de sucessos estará do número esperado de sucessos que é igual ao número de sucessos vezes a probabilidade de sucesso em cada ensaio (5 x 0,50 = 2,5). Estranho, não é? Dois meninos e meio, em média por ensaio! É, a média é assim, uma estimativa, expectativa matemática! Não é real… 7.8.1.4 Função dbinom() Essa função retorna o valor da função de densidade de probabilidade (pdf) da distribuição binomial dada uma determinada variável aleatória X, número de tentativas (size) e probabilidade de sucesso em cada tentativa (prob). A função tem a seguinte sintaxe: Argumentos: x \\(\\rightarrow\\) vetor de números size \\(\\rightarrow\\) numero de ensaios prob \\(\\rightarrow\\) probabilidade de sucesso em cada ensaio A função é usada para encontrar a probabilidade de um determinado valor para dados que seguem a distribuição binomial, ou seja, encontra \\(P(X=x)\\), probabilidade de x sucessos em tentativas de tamanho (size) n quando a probabilidade (p) de sucesso é prob. Obtém o mesmo resultado da fórmula: \\[ P(X = x)= C \\times p^x \\times (1 - p)^{n-x} \\] Por exemplo, no nascimento de uma criança, as duas possibilidades, menino ou menina, são mutuamente excludentes e esses são os únicos eventos que podem acontecer. A probabilidade de nascimento de menino, como visto, é 0,50, qual seria a probabilidade de nascerem 4 meninos em 5 partos consecutivos (Figura 7.16)? dbinom(4, size = 5, prob = 0.50) ## [1] 0.15625 As probabilidades de nascerem meninos em 5 nascimentos são: Fx &lt;- dbinom(0:5, 5, 0.50) Fx ## [1] 0.03125 0.15625 0.31250 0.31250 0.15625 0.03125 Figura7.16: Distribuição binomial para P (x = 4) com n = 5 e p = 0,50 7.8.2 Média e desvio padrão da distribuição binomial Quando o número de repetições é grande, geralmente há necessidade de resumir as probabilidades. A distribuição binomial pode ser descrita por sua média e variância. A média é o valor médio da variável aleatória em um longo número de repetições. É também chamada de valor esperado ou expectativa. A expectativa de uma variável aleatória X, geralmente, é denotada por \\(E(X)\\) e obtida pela multiplicação do número de ensaios independentes (n) pela probabilidade (p) de sucesso em cada ensaio: \\[ \\mu = E(X) = n \\times p \\] Portanto, a expectativa (esperança) de nascimento de meninos em 5 partos é \\(E(X)=5 \\times 0,50 = 2,5\\), como visto na função rbinom(). Observe que o valor esperado de uma variável aleatória discreta não tem um valor que a variável aleatória pode realmente assumir. Por exemplo, para o número médio de meninos em um parto, ou não se tem menino ou se tem 1 menino, cada uma possibilidade com probabilidade de 0,50 e o valor esperado é (0 × 0,50) + (1 × 0,50) = 0,50. O número de meninos deve ser 0 ou 1, mas o valor esperado é a metade, a média que se obteria no longo prazo. A variância de uma variável aleatória discreta X é igual a \\[ \\sigma^2=var(X) = n\\times p \\times (1-p) \\] Consequentemente, o desvio padrão é igual a \\[ \\sigma = \\sqrt{var(X)} = \\sqrt{n\\times p \\times (1-p)} \\] Para o exemplo de 5 nascimentos, a média foi de 2,5 meninos e o desvio padrão \\[ \\sigma =\\sqrt{5\\times 0.50 \\times (1-0.50)}=\\sqrt{2.5 \\times 0.50}= 1.12 \\] Portanto, se espera que ocorram em média 2,5 (\\(\\sigma\\) = 1,12) nascimentos de meninos em 5 partos. 7.9 Distribuição de Poisson A distribuição de Poisson é utilizada para descrever a probabilidade do número de ocorrências em um intervalo contínuo (de tempo ou espaço). No caso da distribuição binomial, a variável de interesse é o número de sucessos em um intervalo discreto (n ensaios de Bernoulli). A unidade de medida (tempo ou espaço) é uma variável contínua, mas a variável aleatória, o número de ocorrências, é discreta. Esta distribuição segue as mesmas premissas da distribuição binomial: as tentativas são independentes; a variável aleatória é o número de eventos em cada amostra; a probabilidade é constante em cada intervalo Ela é utilizada para modelar eventos discretos que ocorrem com pouca frequência no tempo ou espaço, por isso é algumas vezes denominada de distribuição de eventos raros. Pode-se usar a distribuição de Poisson como uma aproximação da distribuição Binomial quando n, o número de tentativas, for grande e p ou (1 – p) for pequeno (eventos raros). Um bom princípio básico é usar a distribuição de Poisson quando \\(n \\ge 20\\) e \\(n \\times p\\) ou \\(n \\times (1- p)\\) &lt; 5% (90). Nessas condições, a probabilidade que uma variável aleatória X adote um valor x é \\[ P(X = x) = \\frac {e^{-\\lambda} \\times \\lambda^x}{x!} \\] onde \\(\\lambda\\) (lambda) representa o número de ocorrências de um evento em um intervalo de tempo e é conhecida como parâmetro da distribuição de Poisson e é igual em média a \\(n \\times p\\). No R, essa probabilidade é dada pela função dpois(x, lambda). Exemplo: Suponha que a probabilidade de uma puérpera ter infecção congênita (rubéola) seja igual a 0,0009. Qual seria a probabilidade, em uma população de 6000 gestantes, de que 5 estejam infectadas? p &lt;- 0.0009 x &lt;- 5 n &lt;- 6000 lambda &lt;- n * p P &lt;- dpois(x, lambda) round (P, 3) ## [1] 0.173 Portanto, a probabilidade de se encontrar 5 mulheres com infecção congênita é de aproximadamente 17%. Para remover a notação científica, usar a função options (scipen = 999) e, para desfazer essa ação, trocar o 999 por 0.↩︎ Veja na Seção 4.8.1↩︎ Sucesso, aqui, não está no sentido de vitória, êxito, triunfo, glória e sim com a conotação de obter o desfecho esperado. Por exemplo, se uma moeda é lançada e se espera obter cara, sucesso significa um resultado igual a cara.↩︎ Deve ser especificado uma “semente” (seed) antes de executar a função, senão será obtido um conjunto diferente de observações aleatórias a cada execução. Teste para verificar↩︎ "],["assimetria-e-curtose.html", "Capítulo 8 Assimetria e Curtose 8.1 Pacotes necessários neste capítulo 8.2 Dados 8.3 Assimetria 8.4 Curtose 8.5 Exercício", " Capítulo 8 Assimetria e Curtose 8.1 Pacotes necessários neste capítulo pacman::p_load(dplyr, e1071, ggplot2, ggpubr, grDevices, moments, readxl, rstatix) 8.2 Dados Será usada a distribuição da variável altura, correspondente a altura em metros de 1368 parturientes da Maternidade do HGCS (dadosMater.xlsx), já usado anteriormente no Capítulo 7. 8.2.1 Exploração dos dados O resumo dos dados pode ser realizado, usando a função summarise() do pacote dplyr. A moda será calculada usando uma função própria moda()30 ,que deve ser ativada com a função source()31. dadosAltura &lt;- read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) %&gt;% select(altura) source(&quot;Arquivos/moda.R&quot;) resumo &lt;- dadosAltura %&gt;% summarise(n = n(), media = mean(altura, na.rm = TRUE), dp = sd(altura, na.rm = TRUE), mediana = median(altura, na.rm = TRUE), moda = moda(altura), Q1 = quantile (altura, 0.25), Q3 = quantile (altura, 0.75), CV = dp/media) resumo ## # A tibble: 1 × 8 ## n media dp mediana moda Q1 Q3 CV ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1368 1.60 0.0655 1.6 1.6 1.55 1.65 0.0410 Para a exploração visual dos dados, será construído um histograma com um boxplot sobreposto (Figura 8.1). A função layout() tem o formato layout(mat) onde mat é um objeto da classe matriz que permite dividir a janela de plotagem em áreas com tamanhos personalizados. Abaixo, cria-se uma matriz com uma coluna e duas linhas com uma relação de 1:8 entre as linhas. A função par() é utilizada para alterar as margens. Para mais detalhes acesse aqui. # Estruturação do layout do gráfico layout(matrix(c(1,2), nrow = 2 , ncol = 1, byrow = TRUE), heights = c(1, 8)) # Boxplot par (mar=c (0, 4.3, 1.1, 2)) boxplot (dadosAltura$altura, horizontal = TRUE, ylim = c (1.4, 1.9), xaxt = &quot;n&quot;, col = &quot;lightblue&quot;, frame = FALSE) #Histograma par (mar=c (4, 4.3, 1.1, 2)) hist (dadosAltura$altura, breaks=15, col = &quot;lightblue&quot;, border = &quot;black&quot;, main = &quot;&quot;, xlab = &quot;Altura (m)&quot;, ylab = &quot;Frequência&quot;, xlim = c(1.4,1.9), las = 1) box(bty = &quot;L&quot;) Figura8.1: Histograma da altura das gestantes com boxplot sobreposto. # Restauração do padrão par (mar = c(5, 4, 4, 2) + 0.1) 8.3 Assimetria A assimetria analisa a proximidade ou o afastamento de um conjunto de dados quantitativos em relação à distribuição normal. Mede o grau de afastamento de uma distribuição em relação a um eixo central (geralmente a média). Quando a curva é simétrica, a média, a mediana e a moda coincidem, num mesmo ponto, havendo um perfeito equilíbrio na distribuição. Quando o equilíbrio não acontece, isto é, a média, a mediana e a moda recaem em pontos diferentes da distribuição esta será assimétrica; enviesada a direita ou esquerda. podendo-se caracterizar como curvas assimétricas à direita ou à esquerda. Quando a distribuição é assimétrica à esquerda ou assimetria negativa, a cauda da curva localiza-se à esquerda, desviando a média para este lado (Figura 8.2). Na assimetria positiva, ocorre o contrário, a cauda está localizada à direita e da mesma forma a média (91). Figura8.2: Assimetria 8.3.1 Avaliação da assimetria O R dispões de diversas maneiras para o cálculo do coeficiente de assimetria. O coeficiente de assimetria é um método numérico estatístico para medir a assimetria da distribuição ou conjunto de dados. Ele fala sobre a posição da maioria dos valores de dados na distribuição em torno do valor central. 8.3.1.1 Cálculo do coeficiente de assimetria Várias medidas de coeficientes de assimetria amostrais foram propostas. O coeficiente de assimetria pode ser calculado no R, usando a função skewness() do pacote e1071. Esta função usa os seguintes argumentos: x \\(\\rightarrow\\) vetor numérico que contém os valores na.rm \\(\\rightarrow\\) um valor lógico que indica se os valores NA devem ser eliminados antes que o cálculo prossiga. type \\(\\rightarrow\\) número inteiro entre 1 e 3 selecionando um dos algoritmos para calcular assimetria detalhados abaixo. Os três tipos são os seguintes: Tipo 1, g1 \\(\\rightarrow\\) definição típica usada em muitos livros didáticos mais antigos. Dada pela fórmula: \\[ g_1=\\frac{m_3}{m_2^\\frac{3}{2}} \\] onde os momentos amostrais para amostras de tamanho n são dados por: \\[ m_r=\\frac{\\sum(x_i - \\overline{x})^r}{n} \\] Para o momento central amostral de ordem r = 3, tem-se: \\[ m_3=\\frac{\\sum(x_i - \\overline{x})^3}{n} \\] Para r = 2, \\[ m_2=\\frac{\\sum(x_i - \\overline{x})^2}{n} \\] Usando o resumo dos dadosAltura: m3 &lt;- (sum((dadosAltura$altura - (mean(dadosAltura$altura)))^3))/resumo$n m3 ## [1] 5.081924e-05 m2 &lt;- (sum((dadosAltura$altura - (mean(dadosAltura$altura)))^2))/resumo$n m2 ## [1] 0.004284321 Colocando os dados na fórmula do g1 no R, chega-se ao resultado: g1 &lt;- m3/(m2)^(3/2) g1 ## [1] 0.1812196 Usando a função skewness() do pacote e1071 32, chega-se ao mesmo resultado: e1071::skewness(dadosAltura$altura, type = 1) ## [1] 0.1812196 Tipo 2, G1 \\(\\rightarrow\\) Usado em vários pacotes estatísticos. É calculado com a seguinte fórmula: \\[ G_1=\\frac{g_1 \\sqrt{n(n-1)}}{n-2} \\] Colocando os dados na fórmula na linguagem do R, tem-se: G1 &lt;- (g1*sqrt((resumo$n*(resumo$n-1))))/(resumo$n-2) G1 ## [1] 0.1814186 Calculando com a função skewness() do pacote e1071: e1071::skewness(dadosAltura$altura, type = 2) ## [1] 0.1814186 Tipo 3, b1 \\(\\to\\) É o padrão da função skewness() do pacote e1071. Usa-se a seguinte fórmula para o cálculo: \\[ b_1= \\frac {m_3}{s^3} \\] onde s é o desvio padrão da amostra. Na linguagem R, tem-se: b1 &lt;- m3/(resumo$dp)^3 b1 ## [1] 0.1810209 Usando a função skewness() do pavote e1071: e1071::skewness(dadosAltura$altura, type = 3) ## [1] 0.1810209 Para amostras grandes, há muito pouca diferença entre as várias medidas (92). Todas as três medidas de assimetria são imparciais sob normalidade. Interpretação do coeficiente de assimetria Quando a \\(assimetria = 0\\), tem-se uma distribuição simétrica e a média, a mediana e a moda coincidem; quando a \\({assimetria} &lt; {0}\\), \\({média} &lt; {mediana} &lt; {moda}\\), a distribuição tem assimetria negativa e quando a \\({assimetria} &gt; {0}\\), \\({média} &gt; {mediana} &gt; {moda}\\), a distribuição tem assimetria positiva. A Tabela 8.1 sugere uma forma de interpretar o coeficiente de assimetria (93). Tabela8.1: Interpretação do Coeficiente de Assimetria Valor do Coeficiente Assimetria -1 a +1 leve -1 a -2 e +1 a +2 moderada -2 a -3 e +2 a +3 importante &lt; -3 ou &gt; +3 grave Observando o formato da distribuição no histograma e no boxplot, na Figura 8.1, e no resultado do coeficiente de assimetria, conclui-se que a variável altura tem uma assimetria positiva leve. 8.3.1.2 Avaliação da assimetria com o gráfico QQ Outra ferramenta gráfica que permite avaliar a simetria dos dados é o gráfico QQ (gráfico quantil-quantil). Ele permite observar se a distribuição se ajusta a distribuição normal. O gráfico QQ é um gráfico de dispersão que compara os quantis 33 da amostra com os quantis teóricos de uma distribuição de referência. Se os pontos do gráfico QQ formarem uma reta, isso indica que os dados têm a mesma distribuição da referência. Se os pontos se afastarem da reta, isso indica que os dados têm uma distribuição diferente da referência. Para construir um gráfico QQ, pode-se usar a função ggqqplot()do pacote ggpubr. Ele apresenta uma linha de referência, acompanhada de uma area sombreada, correspondente ao Intervalo de Confiança de 95% (Veja o Capítulo 10): ggqqplot(data = dadosAltura, x = &quot;altura&quot;, conf.int = TRUE, shape = 19, xlab = &quot;Quantis teóricos&quot;, ylab = &quot;Altura (m)&quot;, color = &quot;dodgerblue4&quot;) Figura8.3: Gráfico QQ A Figura 8.3 exibe que a linha formada pelos pontos, praticamente, formam uma linha reta. É mais uma informação mostrando que os dados têm uma distribuição simétrica aceitável. 8.3.1.3 Pesquisa de valores atípicos Os valores atípicos atraem as caudas da dispersão aumentando a possibilidade de assimetria. No boxplot da Figura 8.1, verifica-se a presença de outliers que devem ser avaliados. Para examinar os outliers, as estatísticas do boxplot são úteis, pois mostram a quantidade e os respectivos valores. A função boxplot.stats() do pacote grDevices, entregam as estatísticas dos 5 números (min, P25, mediana, P75 e max), o total de observações, o limite inferior e superior do intervalo de confiança de 95% e os valores atípicos (outliers):: boxplot.stats(dadosAltura$altura) ## $stats ## [1] 1.42 1.55 1.60 1.65 1.78 ## ## $n ## [1] 1368 ## ## $conf ## [1] 1.595728 1.604272 ## ## $out ## [1] 1.40 1.82 1.80 1.40 1.40 1.85 1.80 Outra maneira de identificar os outliers é através da função indentify_outliers() do pacote rstatix: dadosAltura %&gt;% rstatix::identify_outliers(altura) ## # A tibble: 7 × 3 ## altura is.outlier is.extreme ## &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 1.4 TRUE FALSE ## 2 1.82 TRUE FALSE ## 3 1.8 TRUE FALSE ## 4 1.4 TRUE FALSE ## 5 1.4 TRUE FALSE ## 6 1.85 TRUE FALSE ## 7 1.8 TRUE FALSE Ambas as funções identificaram 7 valores atípicos (acima ou abaixo 1,5 vezes o intervalo interquartil), mas, como mostra a função identify_outliers, eles exercem pouca influência, pois não são extremos, ou seja, acima de três vezes o intervalo interquartil. 8.4 Curtose É o grau de achatamento de uma distribuição, em relação a distribuição normal. A curtose indica como o pico e as caudas de uma distribuição diferem da distribuição normal. A assimetria mede essencialmente a simetria da distribuição, enquanto a curtose determina o peso das caudas da distribuição. Portanto, é uma medida dos tamanhos combinados das duas caudas; mede a quantidade de probabilidade nas caudas. A curtose pode ser de três tipos (Figura 8.4): Mesocúrtica \\(\\rightarrow\\) quando a distribuição é normal; Leptocúrtica \\(\\rightarrow\\) quando a distribuição é mais pontiaguda e concentrada que a normal, mostrando caudas pesadas em ambos os lados; Platicúrtica \\(\\rightarrow\\) quando a distribuição é mais achatada e dispersa que a normal, com caudas planas. Uma curtose em excesso é uma medida que compara a curtose de uma distribuição com a curtose de uma distribuição normal. A curtose de uma distribuição normal é igual a 3. Portanto, o excesso de curtose é determinado subtraindo 3 da curtose: \\[ Excesso \\space de \\space curtose = curtose - 3 \\] A distribuição normal tem uma curtose de zero e é chamada de mesocúrtica. Uma distribuição com curtose maior que zero (ou três) é mais alta e concentrada que a normal, mostrando caudas pesadas em ambos os lados, e é chamada de leptocúrtica. Uma distribuição com curtose menor que zero é mais achatada e dispersa que a normal, com caudas planas, e é chamada de platicúrtica. Os dados que seguem uma distribuição mesocúrtica mostram um excesso de curtose de zero ou próximo de zero. Isso significa que se os dados seguem uma distribuição normal, eles seguem uma distribuição mesocúrtica. A distribuição leptocúrtica mostra caudas pesadas em ambos os lados, indicando grandes valores discrepantes. Uma distribuição leptocúrtica manifesta uma curtose excessiva positiva. Uma distribuição platicúrtica mostra uma curtose excessiva negativa, revela uma distribuição com cauda plana. Figura8.4: Assimetria 8.4.1 Avaliação da curtose 8.4.1.1 Cálculo do coeficiente de curtose O coeficiente de curtose pode ser calculado no R usando a função kurtosis() do pacote e1071. Esta função usa os mesmos argumentos da função skewness(), vista acima. Calcula três tipos de coeficientes: Tipo 1, g2 \\(\\rightarrow\\) definição típica usada em muitos livros didáticos mais antigos. Dada pela fórmula: \\[ g_2=\\frac{m_4}{m_2^2} - 3 \\] onde os momentos amostrais para amostras de tamanho n são dados por: \\[ m_r=\\frac{\\sum(x_i - \\overline{x})^r}{n} \\] Para o momento central amostral de ordem r = 4, tem-se: \\[ m_4=\\frac{\\sum(x_i - \\overline{x})^4}{n} \\] Para r = 2, \\[ m_2=\\frac{\\sum(x_i - \\overline{x})^2}{n} \\] Usando o resumo dos dadosAltura: m4 &lt;- (sum((dadosAltura$altura - (mean(dadosAltura$altura)))^4))/resumo$n m4 ## [1] 5.734699e-05 m2 &lt;- (sum((dadosAltura$altura - (mean(dadosAltura$altura)))^2))/resumo$n m2 ## [1] 0.004284321 Colocando os dados na fórmula do g2 no R, chega-se ao resultado: g2 &lt;- (m4/(m2)^2)-3 g2 ## [1] 0.1242567 Usando a função do pacote e1071, chega-se ao mesmo resultado: e1071::kurtosis(dadosAltura$altura, type = 1) ## [1] 0.1242567 Tipo 2, G2 \\(\\rightarrow\\) Usado em vários pacotes estatísticos. É calculado com a seguinte fórmula: \\[ G_2=\\left (\\left (n + 1 \\right )g_2 + 6 \\right )\\frac{\\left (n - 1 \\right)}{\\left ( \\left(n-2 \\right)\\left (n-3 \\right) \\right )} \\] Colocando os dados na fórmula na linguagem do R, tem-se: G2 &lt;- ((resumo$n+1)*g2 + 6)*(resumo$n-1)/((resumo$n-2)*(resumo$n-3)) G2 ## [1] 0.1291109 Com a função kurtosis() do pacote e1071: e1071::kurtosis(dadosAltura$altura, type = 2) ## [1] 0.1291109 Tipo 3, b2 \\(\\to\\) É o padrão da função kurtosis() do pacote e1071. Usa-se a seguinte fórmula para o cálculo: \\[ b_2=\\frac{m_4}{s^4}-3 \\] onde s é o desvio padrão da amostra. Na linguagem R, tem-se: b2 &lt;- m4/(resumo$dp)^4 - 3 b2 ## [1] 0.1196907 Com a função kurtosis(): e1071::kurtosis(dadosAltura$altura, type = 3) ## [1] 0.1196907 Novamente, para amostras grandes, há muito pouca diferença entre as várias medidas, principalmente entre G2 e b2 (92). Interpretação do coeficiente de curtose Os coeficientes calculados pela função do pacote e1071 retornam um resultado equivalente ao excesso de curtose. A curva normal tem um excesso de curtose próximo a zero e a curva é dita mesocúrtica. Se o coeficiente for positivo, os dados são leptocúrticos e se for negativo, os dados são platicúrticos. O resultado do exemplo aponta para uma distribuição leptocúrtica, pois existe um pequeno excesso de curtose (g2 = 0.1242567). Os valores que contribuem para a curtose são aqueles fora da região do pico, ou seja, ou outliers. A curva mesocúrtica tem um coeficiente de 3. Portanto, os valores calculados anteriormente referem-se ao excesso de curtose. O resultado da g2 = 0,1242567 pode ser escrito como b2 = 3,1242567. Daí o termo excesso de curtose. A função kurtosis() do pacote moments retorna um resultado ao redor de 3, para o coeficiente tipo 1. Para chegar ao mesmo resultado do coeficiente tipo 1 da função do pacote e1071, deve-se subtrair 3 do resultado. moments::kurtosis(dadosAltura$altura) ## [1] 3.124257 8.5 Exercício Criar um conjunto de dados com distribuição normal com média 0 e desvio padrão 1 e n = 10000 que será atribuído ao um objeto denominado meusDados. meusDados &lt;- rnorm(100000, mean = 0, sd = 1) Construa um histograma (Figura 8.5) com curva normal sobreposta: library(ggplot2) ggplot() + geom_histogram(aes(x = meusDados, y =after_stat(density)), bins = 20, fill=&#39;tomato&#39;, col=alpha(&#39;gray40&#39;,0.5)) + geom_function(fun=dnorm, args=list(mean=0,sd=1), col=&#39;dodgerblue4&#39;, lwd=1, lty=2) + labs(x=&#39;X&#39;, y=&#39;Densidade de probabilidade&#39;)+ scale_x_continuous(limits = c(-3, 3), n.breaks = 6) + theme_bw() Figura8.5: Histograma com curva normal Observe a skewness e a kurtosis e1071::skewness(meusDados) ## [1] 0.0007815895 e1071::kurtosis(meusDados) ## [1] 0.03225656 Como era de se esperar, usando a rnorm(), a distribuição é um exemplo de distribuição normal, \\(skewness \\approx 0\\) e \\(kurtosis \\approx 0\\). Observe que a cada vez que os comandos forem executados, os resultados serão discretamente diferentes. Para evitar isso, deve-se usar set.seed(), veja a seção 7.7.2. Faça o teste! A função pode ser obtida no link para se baixada no seu diretório de trabalho.↩︎ Veja a seção sobre criação de funções no Capítulo 4↩︎ https://cran.r-project.org/web/packages/e1071/e1071.pdf↩︎ Sobre os quantis, veja a seção específica no Capítulo 6↩︎ "],["distribuições-amostrais.html", "Capítulo 9 Distribuições Amostrais 9.1 Pacotes necessários para este capítulo 9.2 Distribuições populacional e amostral 9.3 Erros amostrais e não amostrais 9.4 Média e desvio padrão da média 9.5 Teorema do Limite Central 9.6 Proporções populacional e amostral", " Capítulo 9 Distribuições Amostrais 9.1 Pacotes necessários para este capítulo pacman::p_load(dplyr, e1071, ggplot2, ggpubr, kableExtra, knitr, readxl) 9.2 Distribuições populacional e amostral Estatísticas da amostra, como a média, a mediana, a moda e o desvio padrão, são medidas numéricas de resumo calculadas para dados de uma amostra. Por outro lado, as mesmas medidas numéricas de resumo calculadas para dados populacionais são chamadas de parâmetros populacionais. Um parâmetro populacional é sempre uma constante, enquanto uma estatística de amostra é sempre uma variável aleatória. Como cada variável aleatória deve possuir uma distribuição de probabilidade, cada estatística de amostra possui uma distribuição de probabilidade. A distribuição de probabilidade de uma estatística de amostra é mais comumente chamada de distribuição amostral. Os conceitos abordados neste capítulo são a base da estatística inferencial. 9.2.1 Distribuição populacional A distribuição populacional é a distribuição de probabilidade derivada das informações sobre todos os elementos de uma população. Para fins de raciocínio didático, o conjunto de dados de 1368 observações de puérperas e recém-nascidos da Maternidade-escola do Hospital Geral de Caxias do Sul, RS, será considerado a população. O gráfico da Figura 8.1, da Seção 8.2.1, mostra a distribuição da altura das puérperas dessa ‘população’. Os parâmetros (\\(\\mu\\) e \\(\\sigma\\)) dessa ‘população’ são: dadosAltura &lt;- read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) %&gt;% select(altura) media = mean(dadosAltura$altura, na.rm =TRUE) round(media, 3) ## [1] 1.598 dp = sd(dadosAltura$altura, na.rm =TRUE) round(dp, 3) ## [1] 0.065 9.2.2 Distribuição amostral Conforme mencionado no início deste capítulo, o valor de um parâmetro da população é sempre constante. Por exemplo, para qualquer conjunto de dados populacionais, há apenas um valor para a média populacional, \\(\\mu\\). No entanto, não se pode dizer o mesmo sobre a média amostral. Amostras diferentes do mesmo tamanho, retiradas da mesma população, produzem valores diferentes da média amostral, \\(\\bar{x}\\). O valor da média amostral, para qualquer amostra, dependerá dos elementos incluídos nessa amostra. Em decorrência, a média amostral é uma variável aleatória. Portanto, como outras variáveis aleatórias, a média amostral possui uma distribuição de probabilidade, que é mais comumente chamada de distribuição amostral da média. Outras estatísticas de amostra, como mediana, moda e desvio padrão, também possuem distribuições amostrais. Em geral, a distribuição de probabilidades de uma amostra é denominada de distribuição amostral. Voltando à variável altura das puérperas da Maternidade do HGCS, convencionada a priori como a população de interesse. Isso raramente acontece na vida real. Reunir informação sobre uma população inteira costuma ser muito custoso ou impossível. Por essa razão, a prática é selecionar apenas uma amostra da população e a usar para compreender as suas características. Usando a função slice_sample() do pacote dplyr, será extraída uma amostra de n = 30 da população e calculada a média e o desvio padrão: set.seed(234) amostra1 &lt;- dadosAltura %&gt;% dplyr::slice_sample(n = 30) media1 &lt;- mean(amostra1$altura, na.rm =TRUE) dp1 &lt;- sd(amostra1$altura, na.rm =TRUE) print(c(media1, dp1)) ## [1] 1.59266667 0.06073875 Se este processo for repetido várias vezes, a cada amostra aleatória, serão gerados médias e desvios padrão diferentes. set.seed(236) amostra2 &lt;- dadosAltura %&gt;% dplyr::slice_sample(n = 30) media2 &lt;- mean(amostra2$altura, na.rm =TRUE) dp2 &lt;- sd(amostra2$altura, na.rm =TRUE) print(c(media2, dp2)) ## [1] 1.60633333 0.06960397 À medida que o número de amostras possíveis forem aumentando, elas constituem uma distribuição cuja média, média das médias, \\(\\bar{x}_{\\bar{x}}\\), é igual a média populacional, \\(\\mu\\). Essa distribuição, no caso da média, recebe o nome de distribuição amostral das médias. Agora, para exemplificar este conceito, serão geradas 5000 amostras e calculada a média de cada uma das amostras de n = 30 que constituirão a distribuição, mostrada no gráfico da Figura 9.1. # extraindo 5000 amostras amostras5000 &lt;- rep (0, 5000) for (i in 1:5000) { amostra &lt;- dadosAltura %&gt;% dplyr::slice_sample (n = 30) amostras5000 [i] &lt;- mean(amostra$altura) } Media e desvio padrão das 5000 amostras mu &lt;- round (mean (amostras5000), digits = 3) sigma &lt;- round (sd (amostras5000), digits = 3) print(c(mu, sigma)) ## [1] 1.598 0.012 Figura9.1: Distribuição amostral das médias de 5000 amostras de n = 30 Se a média, \\(\\bar{x}_{\\bar{x}}\\), dessas 5000 amostras de n = 30, for comparada com a média populacional, \\(\\mu\\), observa-se que até 3 dígitos decimais não há uma diferença. Entretanto, o desvio padrão é bem menor (0.012) que o da população (0.065). 9.3 Erros amostrais e não amostrais Amostras diferentes selecionadas da mesma população darão resultados diferentes porque contêm elementos diferentes. Isso é evidente nas medias das amostra1 e amostra2, 1.593m e 1.606m, respectivamente, comparadas com a média da população igual a 1.598m . erro1 &lt;- abs(mean(amostra1$altura, na.rm =TRUE) - mean(dadosAltura$altura, na.rm =TRUE)) erro2 &lt;- abs(mean(amostra2$altura, na.rm =TRUE) - mean(dadosAltura$altura, na.rm =TRUE)) print(c(erro1, erro2), digits = 2) ## [1] 0.0053 0.0084 Se outras amostras forem extraídas, o resultado obtido de qualquer amostra geralmente será diferente do resultado obtido da população correspondente. A diferença entre o valor de uma estatística amostral obtida de uma amostra e o valor do parâmetro populacional correspondente, é chamada de erro amostral. Observe que essa diferença representa o erro amostral apenas se a amostra for aleatória e não houver nenhum erro não amostral. Caso contrário, apenas uma parte dessa diferença será devido ao erro amostral. \\[ erro \\quad amostral = \\bar{x}_{i} - \\mu \\] É importante lembrar que o erro amostral ocorre devido ao acaso. Não é possível evitar o erro amostral. É possível limitar o seu valor através da seleção de uma amostra adequada. Os erros que ocorrem por outros motivos, como erros cometidos durante a coleta, registro e tabulação dos dados, são chamados de erros não amostrais. Esses erros ocorrem, em geral, por causa de erros humanos e não por acaso. 9.4 Média e desvio padrão da média A média e o desvio padrão calculados para a distribuição amostral da média são chamados de média (\\(\\mu_{\\bar{x}}\\)) e desvio padrão (\\(\\sigma_{\\bar{x}}\\)) da média. Na verdade, a média e o desvio padrão da média são, respectivamente, a média e o desvio padrão das médias de todas as amostras do mesmo tamanho selecionadas de uma população. O desvio padrão da média é, comumente, chamado de erro padrão da média (\\(\\sigma_{\\bar{x}}\\)). A média amostral, \\(\\bar{x}\\), é chamada de estimador da média da população, \\(\\mu\\). Quando o valor esperado (ou média) de uma estatística amostral é igual ao valor do parâmetro populacional correspondente, essa estatística amostral é considerada um estimador não enviesado, consistente. Para a média amostral \\(\\bar{x}\\), \\(\\mu_{\\bar{x}} = \\mu\\). Logo, \\(\\bar{x}\\), é um estimador imparcial de \\(\\mu\\). Esta é uma propriedade muito importante que um estimador deve possuir. No entanto, o desvio padrão da média, \\(\\sigma_{\\bar{x}}\\), não é igual ao desvio padrão, \\(\\sigma\\), da distribuição populacional (a menos que n = 1). O desvio padrão da média amostral é igual ao desvio padrão da população dividido pela raiz quadrada do tamanho amostral: \\[ \\sigma_{\\bar{x}} = \\frac {\\sigma}{\\sqrt{n}} \\] A dispersão da distribuição amostral da média é menor do que dispersão da distribuição populacional correspondente, como mostrado acima. Em outras palavras, \\(\\sigma_{\\bar{x}} &lt; \\sigma\\). Isso é visível na fórmula do \\(\\sigma_{\\bar{x}}\\) . Quando n é maior que 1, o que geralmente é verdadeiro, o denominador em \\(\\frac {\\sigma}{\\sqrt{n}}\\) é maior que 1. Desta forma, \\(\\sigma_{\\bar{x}}\\) é menor que \\(\\sigma\\). O desvio padrão da distribuição amostral da média diminui à medida que o tamanho amostral aumenta. Sempre que o n for grande, em geral &gt; 30 (94), pode ser assumido que a distribuição será uma curva normal e que o desvio padrão da amostra (s) é um estimador não enviesado do desvio padrão populacional (\\(\\sigma\\)). Então, o erro padrão da média (\\(\\sigma_{\\bar{x}}\\)) pode ser estimado pelo \\(EP_{\\bar{x}}\\): \\[ EP_{\\bar{x}} = \\frac {s}{\\sqrt{n}} \\] 9.5 Teorema do Limite Central Na maioria das vezes, a população da qual as amostras são extraídas não é normalmente distribuída. Em tais casos, a forma da distribuição amostral de X é inferida de um teorema muito importante chamado teorema do limite central. De acordo com este teorema para um grande tamanho de amostra (&gt; 30), a distribuição amostral da média é aproximadamente normal, independentemente da forma da distribuição da população (94). Esta aproximação tornar-se-á mais acurada à medida que aumenta o tamanho amostral: a média da distribuição amostral, \\(\\mu_{\\bar{x}}\\), é igual a média populacional, \\(\\mu\\); desvio padrão da distribuição amostral, \\(\\sigma_{\\bar{x}}\\), é igual a \\(\\frac {\\sigma}{\\sqrt{n}}\\); o erro padrão da média, \\(\\sigma_{\\bar{x}}\\), é sempre menor que o desvio padrão populacional, \\(\\sigma\\) (Figura 9.2). Figura9.2: Erro padrão versus desvio padrão. Agora, será tomado como exemplo a variável renda, do conjunto de dados dadosMater.xlsx, que representa a renda familiar em salários mínimos (sm). Como foi feito anteriormente, suponha que essa variável seja a população de estudo. Ela tem as seguintes medidas resumidoras e de assimetria: mater &lt;- readxl::read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) resumo &lt;- mater %&gt;% select (renda) %&gt;% dplyr::summarise (media.sm = mean (mater$renda, na.rm = TRUE), dp.sm = sd(mater$renda, na.rm = TRUE), mediana.sm = median(mater$renda, na.rm = TRUE), assimetria = e1071::skewness(mater$renda), curtose = e1071::kurtosis(mater$renda)) resumo ## # A tibble: 1 × 5 ## media.sm dp.sm mediana.sm assimetria curtose ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.22 1.23 1.92 2.22 8.21 O desvio padrão é grande em relação à média, com um coeficiente de variação de 55.1% e uma mediana &lt; média. Estas métricas junto com os coeficientes de assimetria e curtose apontam para a assimetria positiva da variável renda familiar. O gráfico da Figura 9.3 confirma esta afirmação: Figura9.3: Distribuição assimétrica positiva Os valores da média e do desvio padrão calculados para a distribuição de probabilidade dessa população fornecem os valores dos parâmetros populacionais \\(\\mu\\) e \\(\\sigma\\). Esses valores são \\(\\mu\\) =2.22sm e \\(\\sigma\\) =1.23sm. Se extrairmos múltiplas amostras dessa população, observa-se a modificação do formato da distribuição à medida que aumenta o tamanho amostral, se aproximando progressivamente do modelo normal, com um número grande de amostras. Extração de múltiplas amostras(1000) amostras1000 &lt;- rep (0, 1000) for (i in 1:1000) { amostra.sm &lt;- sample (mater$renda, 30) amostras1000 [i] &lt;- mean(amostra.sm) } Media e desvio padrão das 1000 amostras mu &lt;- round (mean (amostras1000), digits = 3) sigma &lt;- round (sd (amostras1000), digits = 3) md &lt;- round (median(amostras1000), digits = 3) print(c(mu, sigma, md)) ## [1] 2.231 0.236 2.224 Assimetria e curtose b1 &lt;- e1071::skewness(amostras1000) b2 &lt;- e1071::kurtosis(amostras1000) print(c(b1, b2)) ## [1] 0.4357889 0.2241975 Figura9.4: Distribuição praticamente normal Ou seja, extraindo-se 1000 amostras de n = 30 e calculando as mesmas métricas anteriores, observa-se que, embora a distribuição populacional original seja assimétrica, a distribuição amostral da média se aproxima bastante da distribuição gaussiana (Figura 9.4). 9.6 Proporções populacional e amostral O conceito de proporção é o mesmo que o conceito de frequência relativa e o conceito de probabilidade de sucesso em um experimento binomial, discutidos anteriormente, na distribuição binomial. A frequência relativa de uma categoria ou classe dá a proporção da amostra ou população que pertence a essa categoria ou classe. Da mesma forma, a probabilidade de sucesso em um experimento binomial representa a proporção da amostra ou população que possui uma determinada característica. A proporção populacional, representada por p, é obtida considerando a razão entre o número de elementos em uma população com uma característica específica e o número total de elementos na população. A proporção amostral, denotada por \\(\\hat{p}\\) (pronuncia-se p-chapéu), fornece uma proporção semelhante para uma amostra. \\[ p = \\frac{X}{N} \\quad e \\quad \\hat{p}= \\frac{x}{n} \\] onde, N \\(\\to\\) número total de elementos em uma população n \\(\\to\\) número total de elementos em uma amostra X \\(\\to\\) número de elementos na população que possui determinada característica x \\(\\to\\) número de elementos na amostra que possui determinada característica Como no caso da média, a diferença entre a proporção amostral e a proporção populacional correspondente, determina o erro amostral, assumindo que a amostra é aleatória e nenhum erro não amostral foi cometido. Ou seja, \\[ erro \\quad amostral = \\hat{p} - p \\] A distribuição amostral de uma proporção é a distribuição das proporções de todas as amostras possíveis de tamanho n retiradas de uma população. De acordo com o Teorema Central do Limite: * Considerando m o número de vezes que o processo de repetição das amostras de tamanho n, a média das proporções, quando m $m$, tende para a verdadeira proporção populacional; * A distribuição amostral das proporções segue aproximadamente uma distribuição normal. Assim, \\[E(\\hat{p})=\\mu_\\hat{p}\\] \\[Var(\\hat{p})=\\sigma^2_\\hat{p}=\\frac{\\hat{p}(1-\\hat{p})}{n}\\] Logo, \\[E(\\hat{p})=\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\] Dessa forma, a distribuição amostral de \\(\\hat{p}\\) será: \\[ \\hat{p} \\sim N(\\hat{p}, \\frac{\\hat{p}(1-\\hat{p})}{n}) \\] Quando não conhecemos a proporção populacional p, pode-se usar \\(\\hat{p}\\) como estimativa dessa proporção, desde que as seguintes condições sejam satisfeitas: \\(n \\times p ≥5\\) \\(n \\times(1-p) ≥5\\) Dessa forma, pode-se calcular probabilidades aproximadas por uma distribuição normal com média \\(μ = n \\times p\\) e \\(σ = \\sqrt{(n×p(1-p))}\\) (veja também Seção 7.8.2). Considerando a ‘população’ que está sendo usada neste capítulo, o conjunto de dados dadosMater.xlsx, será verificado a proporção de mulheres fumantes. Inicialmente, a variável fumo, que está como variável numérica, será transformada em fator, pois, na realidade, é categórica: mater$fumo &lt;- factor (mater$fumo, levels = c (1,2), label = c (&quot;sim&quot;, &quot;não&quot;)) A proporção de fumantes, frequência relativa (fr) é: fumo &lt;- with(mater, table(fumo)) fr.fumo &lt;- prop.table(fumo) fr.fumo ## fumo ## sim não ## 0.2200292 0.7799708 A saída retorna que a proporção de fumantes entre as mulheres desse arquivo é 0.22. Esta será considerada a proporção p da ‘população’. Agora, imagine que esse resultado fosse desconhecido. Então, para saber a qual a proporção de fumantes dessa ‘população’, seria necessário extrair uma amostra adequada. Foi selecionada uma amostra de n = 100 da ‘população’ alvo: set.seed(134) amostra.fumo &lt;- mater %&gt;% dplyr::slice_sample(n = 100) Usando a amostra.fumo, calcula-se a proporção de fumantes: tabagismo &lt;- with(amostra.fumo, table(fumo)) fr &lt;- prop.table(tabagismo) fp &lt;- fr*100 tab.fumo &lt;- cbind(n = tabagismo, fr = round(fr, 2), fp = round(fp, 2)) tab.fumo ## n fr fp ## sim 20 0.2 20 ## não 80 0.8 80 A proporção de uma amostra é uma variável aleatória: varia de amostra para amostra de uma forma que não pode ser prevista com certeza. O Teorema Central do Limite se aplica em proporções. À medida que novas amostras forem extraídas, o valor da proporção amostral \\(\\hat{p}\\) se aproxima da proporção populacional p. Na ‘população’ p = 0,22; na amostra de n = 100, \\(\\hat{p}\\) = 0.2. Para amostras grandes, a proporção amostral tem distribuição aproximadamente normal com as seguinte características mencionadas acima em relação a \\(\\mu_\\hat{p}\\) e \\(\\sigma_\\hat{p}\\). Como verificar se uma amostra é grande? Uma amostra é grande se o intervalo \\[ [\\hat{p}-3 \\times \\sigma_\\hat{p} , \\quad \\hat{p}-3 \\times \\sigma_\\hat{p}] \\] estiver totalmente dentro do intervalo [0,1]. Na prática, p não é conhecido, portanto, \\(\\sigma_\\hat{p}\\) também não é. Nesse caso, para verificar se a amostra é suficientemente grande, substitui-se o valor de p pelo valor conhecido de \\(\\hat{p}\\). Isso significa verificar se o intervalo \\[ \\hat{p}-3\\times\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}},\\quad \\hat{p}+3\\times\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\] encontra-se totalmente dentro do intervalo [0,1]. Transportando os dados da amostra de gestantes, para a fórmula e usando o R para o cálculo, tem-se: p.chapeu &lt;- tab.fumo[1,2] n &lt;- tab.fumo[1,1] + tab.fumo[2,1] li &lt;- p.chapeu - 3*sqrt((p.chapeu*(1-p.chapeu))/n) ls &lt;- p.chapeu + 3*sqrt((p.chapeu*(1-p.chapeu))/n) print(c(li, ls), digits = 3) ## [1] 0.08 0.32 Como os limites ficam no intervalo [0, 1], chega-se à conclusão de que a amostra de n = 100 é aceitável para estimar a proporção populacional. Como exercício, verificar se uma amostra de n = 40 é aceitável. "],["sec-estimacao.html", "Capítulo 10 Estimação 10.1 Pacotes necessários neste capítulo 10.2 Dados 10.3 Introdução 10.4 Estimativa Pontual e Intervalo de Confiança 10.5 Estimação da média populacional: \\(\\sigma\\) conhecido 10.6 Estimação da média populacional: \\(\\sigma\\) desconhecido 10.7 Intervalo de Confiança para uma proporção populacional", " Capítulo 10 Estimação 10.1 Pacotes necessários neste capítulo pacman::p_load(DescTools, dplyr, ggplot2, kableExtra, knitr, readxl, Rmisc, tidyr) 10.2 Dados Serão carregados os dados do arquivo dadosMater.xlsx, já usados em outras seções. Para maiores detalhes deste conjunto de dados, consulte a Seção 5.3: mater &lt;- read_excel(&quot;./Arquivos/dadosMater.xlsx&quot;) 10.2.1 Exploração dos dados Será novamente considerado, para fins didáticos, como se fosse uma ‘população’. Dessa população serão filtrados os recém-nascidos a termo (idade gestacional \\(\\ge 37\\) sem e &lt;42 sem) que constituirá um novo conjunto que será atribuído ao objeto rnt: rnt &lt;- mater %&gt;% filter(ig&gt;=37 &amp; ig&lt;42) Também será selecionada, a partir da convencionada ‘população’, mater, a variável altura para constituir-se no objeto dadosAltura: dadosAltura &lt;- mater %&gt;% select(altura) 10.3 Introdução A estatística inferencial é a parte da estatística que usa os resultados da amostra para tomar decisões e tirar conclusões sobre a população de onde a amostra foi retirada. A estimação e o teste de hipóteses, tomados em conjunto, constituem a inferência estatística. Estimação é um procedimento pelo qual um valor ou valores numéricos são atribuídos a um parâmetro populacional com base nas informações de uma amostra. Na estatística inferencial, \\(\\mu\\) é chamada de média populacional e p é chamada de proporção populacional. Existem muitos outros parâmetros populacionais, como mediana, moda, variância e desvio padrão. Se houvesse possibilidade de realizar um censo (pesquisa incluindo toda a população de interesse), não haveria necessidade dos procedimentos de estimação. Seria equivalente ao que ocorre em uma eleição, basta contar os votos, para declarar os vencedores da eleição. No entanto, em saúde, realizar censo é um procedimento caro, demorado ou virtualmente impossível. Portanto, geralmente é utilizada uma amostra da população e calculada o valor das estatísticas da amostra apropriada. Baseado nessas estatísticas, é atribuído valores ao parâmetro. A estatística usada para estimar um parâmetro é chamada de estimador. Assim, a média da amostra, \\(\\bar{x}\\), é um estimador da média da população, \\(\\mu\\); e a proporção da amostra, \\(\\hat{p}\\), é um estimador da proporção da população, p. Estimativa é um valor que a função estimador assume. 10.4 Estimativa Pontual e Intervalo de Confiança A partir do conjunto de dados rnt, construído na Seção anterior, serão calculados a média e o desvio padrão da variável pesoRN (peso dos recém-nascidos em g) que, para fins didáticos, serão considerados os parâmetros dessa ‘população’: mu &lt;- round(mean(rnt$pesoRN, na.rm = TRUE)) sigma &lt;- round(sd(rnt$pesoRN, na.rm = TRUE)) print(x = c(mu, sigma)) ## [1] 3216 462 A seguir, será extraída, dessa população, uma amostra de n = 30 34 e calculado os mesmas medidas resumidoras, que se constituirão nas estimativas da amostra : set.seed (1234) amostra &lt;- rnt %&gt;% select(pesoRN) %&gt;% slice_sample(n = 30) # Média amostral x_barra &lt;- round(mean(amostra$pesoRN, na.rm = TRUE)) # Desvio padrão amostral s &lt;- round(sd(amostra$pesoRN, na.rm = TRUE)) print(x = c(x_barra, s)) ## [1] 3222 407 O valor de 3222g é a média amostral, \\(\\bar{x}\\), usada como um estimativa da \\(\\mu\\), é denominada de estimativa pontual. Como já mencionado anteriormente, espera-se que cada amostra selecionada produza um valor diferente da estatística amostral. Assim, o valor atribuído a uma média populacional, \\(\\mu\\), com base em uma estimativa pontual depende de qual das amostras está sendo usada. Consequentemente, a estimativa pontual atribui um valor a \\(\\mu\\) que quase sempre difere da mesma. Para melhorar a precisão, usa-se uma estimativa de intervalo. Em vez de atribuir um único valor para o parâmetro populacional, é construído um intervalo, acrescentando ou subtraindo um valor, chamado de margem de erro, à estimativa pontual. Este procedimento é conhecido como estimação por intervalo e o intervalo construído, estabelecendo um limite inferior e um limite superior em torno da estimativa amostral, é denominado de intervalo de confiança. Desta forma, é possível afirmar que o intervalo de confiança, provavelmente, contém o parâmetro populacional correspondente (Figura 10.1). Figura10.1: Intervalo de Confiança. A construção do intervalo de confiança depende da obtenção da margem de erro. Este processo necessita de dois fatores: do desvio padrão da distribuição amostral, \\(\\sigma_{\\bar{x}}=\\frac{\\sigma }{\\sqrt{n}}\\), que em decorrência do Teorema do Limite Central, pode ser escrito \\(EP_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\\); do nível de confiança atribuído ao intervalo. Primeiro, quanto maior for o desvio padrão de \\(\\bar{x}\\), maior será a margem de erro subtraída e adicionada à estimativa pontual. Consequentemente, o intervalo de confiança se modifica de acordo com a margem de erro. Quanto maior a margem de erro mais amplo o intervalo de confiança. Em segundo lugar, a quantidade subtraída e adicionada à estimativa se modifica de acordo o nível de confiança. Para ter uma maior confiança, deve-se aumentar a margem de erro, de acordo com a probabilidade declarada. Quanto maior o nível de confiança (NC), maior a probabilidade. O nível de confiança é mostrado como \\((1 - \\alpha) \\times 100\\)%, onde \\(\\alpha\\) é o nível de significância. Tradicionalmente, o valor de \\(\\alpha\\) é igual a 0,05, mas qualquer outro valor pode ser usado. 10.5 Estimação da média populacional: \\(\\sigma\\) conhecido A margem de erro para a estimativa da média populacional, \\(\\mu\\), quando se conhece o desvio padrão populacional,\\(\\sigma\\), e \\(n \\ge 30\\) ou, mesmo que \\(n &lt; 30\\), mas a população de onde amostra foi selecionada tem distribuição normal, é a quantidade que é subtraída ou adicionada ao valor da média da amostra, \\(\\bar{x}\\), para obter o intervalo de confiança para \\(\\mu\\). Desta forma, a margem de erro é igual a: \\[ margem \\quad de\\quad erro\\quad(me)= z_{(1-\\frac{\\alpha}{2})} \\times \\sigma_{\\bar{x}} \\] Ou, \\[ me = z_{(1-\\frac{\\alpha}{2})} \\times \\frac{\\sigma }{\\sqrt{n}} \\] Logo, o intervalo de confiança para a média populacional, \\(\\mu\\), para um nível de confiança (1 - \\(\\alpha\\))100%, é igual a: \\[ IC_{(1-\\alpha)}(\\mu) \\rightarrow \\bar{x} \\pm me \\] Se objetivo é construir um intervalo de confiança de 95%, a última equação passa a ser: \\[ IC_{(1-\\alpha)}(\\mu) \\rightarrow \\bar{x} \\pm z_{(0,975)} \\times me \\] Onde z é o valor crítico para o nível de confiança escolhido, obtido da tabela de distribuição normal padrão, e me é a margem de erro (\\(z_{0,975} \\times erro \\quad padrao\\)). Um intervalo de confiança de 95% significa que a área total sob a curva normal entre dois pontos em torno da média populacional, \\(\\mu\\), é igual a 95%, ou 0,95. A área das caudas é \\(\\alpha\\), ou seja, cada cauda á igual a \\(\\frac{\\alpha}{2}\\) (Figura ??). Para encontrar o valor de z para um nível de confiança de 95%, primeiro encontram-se as áreas à esquerda desses dois pontos, \\(z_1\\) e \\(z_2\\). Esses dois valores de z serão iguais, mas com sinais opostos. A área total sob a curva é igual a 1. A área entre \\(z_1\\) e \\(z_2\\) é igual a \\(1 - \\alpha = 0,95\\). A área a esquerda de \\(z_1\\) é igual a 0,025 e a área a esquerda de \\(z_2\\) é igual a 1 – 0,025 = 0,975. No R, o valor \\(z_1\\) e \\(z_2\\) podem facilmente ser obtidos com a função qnorm(): print(c(qnorm(0.025),qnorm(0.975)), 3) ## [1] -1.96 1.96 Dessa maneira, para uma confiança de 95%, é usado um \\(z = 1.96\\), onde: \\[ p(-1,96 \\le z \\le 1,96) = 0,95 \\] Logo, \\[ IC_{95\\%}(\\mu) \\rightarrow \\bar{x} \\pm (1.96 \\times \\sigma_{\\bar{x}}) \\] ou \\[ IC_{95\\%}(\\mu) \\rightarrow \\bar{x} \\pm (1.96 \\times \\frac{\\sigma}{\\sqrt{n}}) \\] 10.5.1 Cálculo do intervalo de confiança com \\(\\sigma\\) conhecido Usando a média da amostra dos recém-nascidos de n = 30, \\(\\bar{x}\\)= 3222 g, e o desvio padrão populacional conhecido, \\(\\sigma\\)= 462 g, tem-se que o intervalo de confiança de 95% (IC95%), para o peso dos recém-nascidos a termo na ‘população’ de onde esta amostra é proveniente: Dados do exemplo para o cálculo n &lt;- 30 x_barra &lt;- 3222 sigma &lt;- 462 Com 95% de confiança a margem de erro é igual a 1,96 vezes o erro padrão da média: n &lt;- 30 me &lt;- 1.96 * sigma/sqrt(n) round(me,2) ## [1] 165.32 Basta, agora, adicionar e subtrair a margem de erro da média: lim_inf &lt;- x_barra - me lim_sup &lt;- x_barra + me ic95 &lt;- c(lim_inf, lim_sup) round(ic95, 1) ## [1] 3056.7 3387.3 Assim, tem-se uma confiança de 95% de que a verdadeira média, esteja incluída no intervalo. O nome para isso é intervalo de confiança de 95% para a média populacional. 10.5.2 Função para calcular IC com \\(\\sigma\\) conhecido O cálculo manual é simples, mas enfadonho, nos tempos dos computadores. Em decorrência, como o R não tem uma função para encontrar os intervalos de confiança para a média de dados com distribuição normal quando o desvio padrão da população é conhecido, foi criada uma função para cumprir essa ação. Ela necessita dos seguintes argumentos: x \\(\\rightarrow\\) conjunto de números da amostra s \\(\\rightarrow\\) desvio padrão populacional nc \\(\\rightarrow\\) nível de confiança. Padrão: nc = 0.95 IC_z &lt;- function (x, s, nc = 0.95) { `%&gt;%` &lt;- dplyr::`%&gt;%` n &lt;- length(x) me &lt;- abs(qnorm((1-nc)/2))* sigma/sqrt(n) df_out &lt;- data.frame( tamanho_amostral = n, media_amostral = mean(x), margem_erro = me, &#39;IC limite inferior&#39;=(mean(x) - me), &#39;IC limite superior&#39;=(mean(x) + me)) %&gt;% tidyr::pivot_longer(names_to = &quot;Medidas&quot;, values_to =&quot;valores&quot;, 1:5 ) return(df_out) } IC_z(x = amostra$pesoRN, s = sigma, nc = 0.95) ## # A tibble: 5 × 2 ## Medidas valores ## &lt;chr&gt; &lt;dbl&gt; ## 1 tamanho_amostral 30 ## 2 media_amostral 3222. ## 3 margem_erro 165. ## 4 IC.limite.inferior 3056. ## 5 IC.limite.superior 3387. Essa função pode ser salva no seu diretório e, quando necessária, pode ser ativada com a função source(), como visto na Seção 4.8.1. Com essa função fica fácil alterar o nível de confiança, por exemplo, para 99%. Isso mudará o z crítico para: alpha &lt;- 0.01 p &lt;- 1-(alpha/2) p ## [1] 0.995 z_critico &lt;- qnorm(p) round(z_critico, 2) ## [1] 2.58 Com a função IC_z(): IC_z(x = amostra$pesoRN, s = sigma, nc = 0.995) ## # A tibble: 5 × 2 ## Medidas valores ## &lt;chr&gt; &lt;dbl&gt; ## 1 tamanho_amostral 30 ## 2 media_amostral 3222. ## 3 margem_erro 237. ## 4 IC.limite.inferior 2985. ## 5 IC.limite.superior 3458. Observando o IC95% e o IC99%, verifica-se que a amplitude do intervalo aumentou com o crescimento da confiança de 95% para 99%, porque houve um aumento na margem de erro Figura 10 3. 10.5.3 Interpretação do intervalo de confiança Se fossem extraídas todas as possíveis amostras de tamanho 30 da população de recém-nascidos a termo e construído para cada uma delas um intervalo de confiança de 95% em torno de cada média amostral, espera-se que 95% desses intervalos incluirão a média populacional e 5% não incluirão. O IC95% informa sobre a precisão com que a média amostral estima a média populacional desconhecida 35. Na Figura 10.2, são mostradas 20 amostras diferentes de tamanho n = 30, retiradas da mesma população de recém-nascidos a termo da Maternidade do HGCS. Junto aparecem os intervalos de confiança de 95% construídos em torno dessas amostras. Observa-se que apenas uma amostra (em vermelho) não inclui a média populacional (linha tracejada vertical em azul). Pode-se afirmar com 95% de confiança que se forem extraídas muitas amostras do mesmo tamanho de uma população e construído intervalos de confiança de 95% em torno das médias dessas amostras, 95% desses intervalos de confiança incluirão a média populacional. Figura10.2: Intervalos de confiança de 95% que mostra 20 replicações simuladas de amostras de n = 30 do peso do recém-nascido. Apenas um intervalo (em vermelho) não inclui a média populacional (linha vertical azul). 10.6 Estimação da média populacional: \\(\\sigma\\) desconhecido Com amostras pequenas, usar o modelo normal para construir intervalos de confiança, gera um erro, pois os pressupostos do teorema do limite central não são respeitados. Quando o desvio padrão populacional, \\(\\sigma\\), é desconhecido e o tamanho amostral é pequeno (&lt; 30), a estimação da média populacional é feita usando a distribuição t. 10.6.1 Distribuição t A distribuição t, desenvolvida por William Sealy Gosset, em 1908, é semelhante à distribuição normal. Como a curva de distribuição normal, a curva de distribuição t é unimodal, simétrica (em forma de sino) em torno da média e nunca encontra o eixo horizontal. A área total sob uma curva de distribuição t é 1 ou 100%. A curva da distribuição t é mais plana do que a curva de distribuição normal padrão. Em outras palavras, ela é mais achatada e mais espalhada. No entanto, conforme o tamanho da amostra aumenta, a distribuição t aproxima-se da distribuição normal padrão. O formato de uma curva de distribuição t particular depende do número de graus de liberdade. O número de graus de liberdade (gl) para uma distribuição t é igual ao tamanho da amostra menos um, ou seja, \\(gl=n-1\\), veja seção 6.3.4.3. O número de graus de liberdade é o único parâmetro da distribuição t. Há uma diferente distribuição t para cada número de graus de liberdade, portanto, a distribuição t se constitui em uma família de distribuições (Figura 10.3). Figura10.3: Curvas de distribuição t conforme o grau de liberdade e a distribuição normal. Da mesma maneira que a distribuição normal padrão, a média da distribuição padrão t é 0. Entretanto, ao contrário da distribuição normal padrão, cujo desvio padrão é 1, o desvio padrão de uma distribuição t é \\(\\sqrt{\\frac{gl}{gl-2}}\\) , para gl &gt; 2, que sempre é maior do que 1. Assim, o desvio padrão de uma distribuição t é maior do que o desvio padrão da distribuição normal padrão. Os valores de \\({t}_{crítico}\\) podem ser obtidos usando a função qt() que usa os seguintes argumentos: p \\(\\to\\) probabilidade, igual a \\(1 - \\frac{\\alpha}{2}\\), considerando-se bicaudal e \\(1 - \\alpha\\) quando unicaudal; df \\(\\to\\) graus de liberdade; lower.tail \\(\\to\\) lógico; se TRUE, informa a probabilidade da cauda inferior. O padrão é TRUE. Assim, o valor do \\({t}_{crítico}\\) para \\(gl=10\\) é: alpha &lt;- 0.05 p &lt;- 1 - (alpha/2) gl = 10 t &lt;- qt(p = p, df = 10, lower.tail = TRUE) round(t, digits = 2) ## [1] 2.23 A área compreendida entre \\(\\pm 2.23\\) é igual a 95% (Figura 10.4): \\[ p(-2,23\\le t\\le 2,23)=0,95 \\] Figura10.4: Distribuição t com gl = 10, bilateral. Quando se considera apenas uma das caudas (unicaudal ou unilateral), o valor do \\({t}_{crítico}\\) para \\(gl=10\\) é t1 &lt;- qt(p = 0.95, df = 10, lower.tail = TRUE) round(t1, digits = 2) ## [1] 1.81 Assim, a área abaixo de 1.81 é igual a 95% (Figura 10.5). \\[ p(t\\le 1,81)=0,95 \\] Figura10.5: Distribuição t com gl = 10, unilateral. 10.6.2 Cálculo do intervalo de confiança com \\(\\sigma\\) desconhecido Serão utilizados nesta seção, os dados da altura de mulheres (dadosAltura), obtidos na Seção 10.2.1. Estes dados da ‘população’ convencionada contêm informações de 1368 mulheres. Suponha-se que os parâmetros sejam desconhecidos. Para estimar esses parâmetros, selecionou-se uma amostra de n = 30 desse conjunto dados. Tomando essa amostra, calcula-se a sua média e o seu desvio padrão: set.seed(2345) amostra1 &lt;- dadosAltura %&gt;% select (altura) %&gt;% slice_sample(n = 30) x_barra1 &lt;- mean(amostra1$altura, na.rm = TRUE) s1 &lt;- sd(amostra1$altura, na.rm = TRUE) print(round(c(x_barra1, s1),3)) ## [1] 1.599 0.051 A maneira mais intuitiva de estimar a média da população com base na amostra, é, simplesmente, calcular a média e o desvio padrão. Entretanto, para uma maior precisão, é sempre importante calcular o intervalo de confiança. 10.6.2.1 Cálculo manual do IC Quando o desvio padrão da população (\\(\\sigma\\)) não é conhecido, pode-se usar o seu estimador que é o desvio padrão da amostra (s), respeitando os pressupostos (95). Então, o erro padrão da média (\\(\\sigma_{\\bar{x}}\\)) pode ser estimado pelo \\(EP_{\\bar{x}}\\). \\[ EP_{\\bar{x}}=\\frac{s}{\\sqrt{n}} \\] O intervalo de confiança para a \\(\\mu\\) para um nível de confiança (NC) de \\((1 – \\alpha) \\times100\\)% é igual a: \\[ IC_{NC}(\\mu)\\rightarrow x\\pm (t_{({1-\\frac{alpha}{2})} } \\times \\frac {s}{\\sqrt{n}}) \\] Quando o tamanho amostral é grande, o valor de t se aproxima do valor de z, portanto, em situações em que não se conhece o desvio padrão populacional, não há muita diferença se houver uma aproximação de t para z (Tabela 10.1). Tabela10.1: Comparação dos valores z e t(gl). z n gl t 1,96 5 4 2,57 1,96 10 9 2,23 1,96 30 29 2,04 1,96 50 49 2,01 1,96 100 99 1,98 1,96 200 199 1,97 1,96 500 499 1,96 1,96 1000 999 1,96 A amostra1 de n = 30, \\(\\overline x\\) = 1.5986667m e \\(s\\) = 0.0511073m. Essas estimativas servirão para o cálculo do intervalo de confiança, usando uma distribuição t bicaudal e um nível de significância \\(\\alpha = 0,05\\). n1 &lt;- length(amostra1$altura) alpha &lt;- 0.05 p &lt;- 1 - alpha/2 # Graus de liberdade gl &lt;- n1 - 1 # Valor t crítico tc &lt;- qt(p, gl, lower.tail = TRUE) # Erro padrão EP1 &lt;- round(s1/sqrt(n1),3) print(round(c(tc, EP1),3)) ## [1] 2.045 0.009 Com esses dados, calcula-se o intervalo de confiança de 95%: me1 &lt;- tc*EP1 lim_inf &lt;- x_barra1 - me1 lim_sup &lt;- x_barra1 + me1 ic95 &lt;- c(lim_inf, lim_sup) round(ic95, 2) ## [1] 1.58 1.62 10.6.2.2 Cálculo usando uma função do R O R possui algumas funções que calculam o intervalo de confiança para variáveis numéricas, baseadas na distribuição t. Entre elas, a função CI(), incluída no pacote Rmisc. Esta função tem dois argumentos: x ⟶ vetor de dados; ci ⟶ intervalo de confiança a ser calculado IC95 &lt;- CI(amostra1$altura, ci = 0.95) round(IC95, 2) ## upper mean lower ## 1.62 1.60 1.58 10.7 Intervalo de Confiança para uma proporção populacional 10.7.1 Dados para estimar a proporção populacional Aqui, será utilizada uma amostra aleatória de n = 60 da variávelfumo para estimar a proporção de mulheres fumantes no conjunto de dados dadosMater.xlsx. set.seed(2346) dados &lt;- read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) %&gt;% select (fumo) %&gt;% slice_sample(n = 60) str(dados) ## tibble [60 × 1] (S3: tbl_df/tbl/data.frame) ## $ fumo: num [1:60] 2 2 1 2 2 2 2 2 2 1 ... A seleção mostra que temos 60 observações da variável fumo e que a mesma está classificada como numérica (num), 1 e 2. Onde 1 representa as mulheres fumantes. A variável é categórica e deve ser transformada para fator. dados$fumo &lt;- factor (dados$fumo, levels = c (1,2), label = c (&quot;sim&quot;, &quot;não&quot;)) 10.7.2 Cálculo da estimativa pontual da proporção Nessa amostra, a proporção de fumantes é: tab &lt;- table(dados$fumo) tab ## ## sim não ## 14 46 tabFumo &lt;- round (prop.table (tab), 3) tabFumo ## ## sim não ## 0.233 0.767 10.7.3 Cálculo do intervalo de confiança para a proporção Cálculo manual com aproximação normal 1ª etapa: verificar a premissa de que quando a proporção populacional é desconhecida a proporção pontual (\\(\\hat p\\)) e o seu complemento (\\(\\hat q = 1 - \\hat p\\)) multiplicados, cada um, por \\(n\\), devem ser maior do que 5. n &lt;- length(dados$fumo) (tabFumo) * n ## ## sim não ## 13.98 46.02 Como se observa, ambos os valores são maiores do que 5. 2ª Etapa: O intervalo pode ser estimado pela distribuição normal e é necessário calcular o z_crítico: alpha &lt;- 0.05 p &lt;- 1 - alpha/2 zc &lt;- qnorm (p, mean = 0, sd = 1) round(zc, 2) ## [1] 1.96 3ª Etapa: Cálculo do erro padrão da proporção (\\(\\sqrt \\frac {\\hat p \\times \\hat q}{n}\\)) e da margem de erro (veja também a Seção 9.6: # Extração da proporção amostral do tabFumo prop &lt;- tabFumo [1] # Cálculo do EP amostral EP &lt;- sqrt((prop * (1 - prop))/n) # Cálculo da margem de erro(me) me &lt;- zc * EP # dados necessários para o cálculo do IC95% print(c(prop, me), digits = 3) ## sim sim ## 0.233 0.107 4ª Etapa: Intervalo de confiança ic_prop &lt;- c((prop - me), (prop + me)) round(ic_prop, 3) ## sim sim ## 0.126 0.340 Cálculo usando uma função O chamado Intervalo de Confiança Exato corrigem as deficiências da aproximação normal. O R tem uma função para este cálculo: BinomCI() do pacote DescTools(96). É preferível usar o método de Clopper e Pearson que fornece o IC exato. Os argumentos da função BinomCI() são: x \\(\\to\\) é o número de desfechos, sucessos; n \\(\\to\\) é o tamanho da amostra, número de ensaios; p \\(\\to\\) probabilidade, hipótese nula; se ignorada o padrão é 0,50; conf.level \\(\\to\\) nível de confiança, o padrão é 0.95; method \\(\\to\\) possui vários métodos para calcular intervalos de confiança para uma proporção binomial como: “clopper-pearson” (exact interval), “wilson”, “wald”, “agresti-coull”, “jeffreys”, “modified wilson”, “modified jeffreys”, “arcsine”, “logit”, “witting”, “pratt”. O método padrão é o de “wilson”. Qualquer outro método, há necessidade de solicitar; sides \\(\\to\\) hipótese alternativa padrão “two.sided” (bilateral), mas pode ser “right” ou “left” (unilateral a direita ou a esquerda, respectivamente). x &lt;- tab[1] IC &lt;- BinomCI (x, n, conf.level = 0.95, method = &quot;clopper-pearson&quot;) round(IC, 3) ## est lwr.ci upr.ci ## [1,] 0.233 0.134 0.36 Observe que existe uma pequena diferença entre os valores da aproximação normal e o exato, com o método de “clopper-pearson”. É importante lembrar que toda vez que for extraída uma nova amostra de tamanho n = 30, o resultado será um conjunto de números diferentes e, em consequência, a média será diferente. Por isso, se for importante repetir o mesmo resultado, deve-se usar a função set.seed(). Consulte a Seção 7.7.2.4↩︎ Anteriormente, mostrou-se a media populacional por uma questão didática. A regra é não se conhecer a média populacional, razão da importância do intervalo de confiança↩︎ "],["teste-de-hipóteses.html", "Capítulo 11 Teste de Hipóteses 11.1 Pacotes necessários neste capítulo 11.2 Dados do exemplo 11.3 Introdução 11.4 Hipótese nula e alternativa 11.5 Escolha do teste estatítico e regra de decisão 11.6 Valor P do teste 11.7 Poder do teste", " Capítulo 11 Teste de Hipóteses 11.1 Pacotes necessários neste capítulo pacman::p_load(dplyr, lsr, pwr, readxl, rstatix) 11.2 Dados do exemplo Considere o exemplo dos recém-nascidos a termo da Maternidade do HGCS, extraído do arquivo dadosMater.xlsx (veja Seção 5.3). rnt &lt;- readxl::read_excel(&quot;Arquivos/dadosMater.xlsx&quot;) %&gt;% dplyr::filter(ig&gt;=37 &amp; ig&lt;42) %&gt;% select(sexo, pesoRN) 11.2.1 Exploração e transformação dos dados Inicialmente, para ter uma visão da estrutura dos dados, usa-se: str(rnt) ## tibble [1,085 × 2] (S3: tbl_df/tbl/data.frame) ## $ sexo : num [1:1085] 1 1 1 1 1 1 1 1 1 1 ... ## $ pesoRN: num [1:1085] 3285 3100 3100 2800 3270 ... A seguir, transformar a variável sexo em fator: rnt$sexo &lt;- factor(rnt$sexo, levels = c(1, 2), labels = c(&quot;masc&quot;, &quot;fem&quot;)) Este conjunto de dados fica, portanto, fica restrito a 1085 casos, contendo duas variáveis sexo e pesoRN, necessárias neste capítulo e assim resumidas: resumo &lt;- rnt %&gt;% dplyr::group_by(sexo) %&gt;% dplyr::summarise (n = n(), media = mean(pesoRN, na.rm = TRUE), sigma = sd(pesoRN, na.rm = TRUE)) resumo ## # A tibble: 2 × 4 ## sexo n media sigma ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 masc 592 3274. 458. ## 2 fem 493 3147. 458. Suponha que esses 1085 casos sejam considerados a ‘população-alvo’ e que se faça uma afirmação de que existe uma diferença significativa entre os pesos ao nascer de meninos e meninas. Para testar essa afirmação, será extraída uma amostra de 200 casos do conjunto rnt, sem reposição. 36 set.seed (123) dados &lt;- rnt %&gt;% slice_sample(n = 200) Este conjunto de dados contém 200 observações de 2 variáveis, assim resumidas. resumo.dados &lt;- dados %&gt;% dplyr::group_by(sexo) %&gt;% dplyr::summarise(n = n(), media = mean (pesoRN, na.rm = TRUE), dp = sd (pesoRN, na.rm = TRUE)) resumo.dados ## # A tibble: 2 × 4 ## sexo n media dp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 masc 109 3333. 407. ## 2 fem 91 3169. 478. Esta amostra de 109 meninos e 91 meninas, informa que os meninos têm, em média, 3333 g ao nascer e as meninas 3169 g. Esta diferença de peso entre os sexos pode ter ocorrido devido ao acaso. Portanto, há necessidade de realizar um teste de hipóteses para tomar uma decisão sobre o parâmetro populacional. Esta diferença é grande o suficiente para rejeitar a hipótese de igualdade entre os pesos e concluir que existe uma diferença real entre eles? 11.3 Introdução No capítulo anterior, foi discutido aspectos relacionados à estimação, que se constitui, junto com o teste de hipótese, em procedimentos básicos da estatística inferencial. Em um teste de hipóteses, testa-se uma teoria ou crença sobre um parâmetro populacional (97). Na maioria das vezes, como mencionado anteriormente, obtém-se informações a partir de uma amostra em função da impossibilidade ou dificuldade de se conseguir essas informações a partir da população. Portanto, extrapolar ou estender os resultados, obtidos de uma amostra, para a população, significa aceitá-los como representações adequadas da mesma. Sabe-se que as estimativas amostrais diferem dos valores reais (populacionais) e o objetivo dos testes de hipóteses é estabelecer a probabilidade de essa diferença ser explicada pelo acaso. O teste de hipóteses fornece um sistema referencial para a tomada de decisão sobre a adequação ou não dos dados amostrais serem representativos de uma população. Este sistema referencial é a distribuição de probabilidade do evento observado(98). Inicialmente é importante fazer uma distinção entre hipótese de pesquisa e hipótese estatística. Uma hipótese de pesquisa é uma afirmação que expressa a relação esperada entre as variáveis de um estudo científico. Ela é baseada em uma pergunta de pesquisa e serve para orientar a coleta e análise dos dados. Uma hipótese de pesquisa pode ser confirmada ou refutada pelos resultados do estudo. Um exemplo de hipótese de pesquisa é: “O tabagismo durante a gestação interfere sobre o peso dos conceptos”. Uma hipótese de pesquisa corresponde àquilo que se quer acreditar sobre o mundo. Uma hipótese estatística é uma afirmação relacionada aos parâmetros de uma população. Baseia-se em uma hipótese de pesquisa e serve para testar a validade da mesma usando técnicas estatísticas. Uma hipótese estatística pode ser aceita ou rejeitada com um certo nível de confiança. A hipótese estatística deve ter uma relação clara com as hipóteses de pesquisa Por exemplo: “A média de peso dos recém-nascidos de mães fumantes é menor do que o das não fumantes”; “A média de peso dos recém-nascidos masculinos é igual ao peso dos recém-nascidos femininos”, ou ainda, “A média de peso dos recém-nascidos masculinos é diferente do peso dos recém-nascidos femininos”. Todos esses exemplos são legítimos de uma hipótese estatística porque são afirmações sobre um parâmetro populacional e estão significativamente relacionados à hipótese de pesquisa. 11.4 Hipótese nula e alternativa Em função da hipótese de pesquisa, mencionada anteriormente, foram gerados os dados do exemplo. A hipótese de pesquisa corresponde ao que se quer acreditar, “o tabagismo na gestação interfere no peso dos neonatos”. Para refutar ou não essa afirmação constrói-se um teste de hipótese para verificar se ela é compatível ou não com os dados disponíveis (99). No teste de hipóteses (TH), existem dois tipos de hipóteses, definidas como: Hipótese nula(\\(H_{0}\\)): hipótese que afirma a não existência de diferença entre os grupos e, portanto, a diferença observada é atribuível ao acaso. É a hipótese a ser testada, aquela que se busca afastar, demonstrando que é, provavelmente 37, falsa, não válida. É denotada como: \\[ H_{0}: \\mu_{1}= \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2}=0 \\] Hipótese alternativa (\\(H_{1} \\quad ou \\quad H_{a}\\)): é a hipótese contrária, como o nome diz, alternativa à \\(H_{0}\\). Representa a posição de uma nova perspectiva, a conclusão que será apoiada se \\(H_{0}\\) for rejeitada. Ela supõe que realmente exista uma diferença entre os grupos. É a hipótese que o pesquisador pretende comprovar. É denotada, em geral, simplesmente como havendo uma diferença entre os grupos, sem indicar uma direção, hipótese bilateral ou bicaudal: \\[ H_{1}: \\mu_{1} \\neq \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} \\neq 0 \\] Ou, se houver uma suspeita, através de um conhecimento prévio, apontar uma direção para a diferença, ou seja, usar uma hipótese unilateral ou monocaudal. Neste caso existe duas possibilidade: Unilateral à direita: \\[ H_{1}: \\mu_{1} &gt; \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} &gt; 0 \\] Consequentemente, \\[ H_{0}: \\mu_{1} \\le \\mu_{2} \\quad ou \\quad \\mu_{1}- \\mu_{2} \\le 0 \\] 2) Unilateral à esquerda: \\[ H_{1}: \\mu_{1} &lt; \\mu_{2} \\quad ou \\quad \\mu_{1} - \\mu_{2} &lt; 0 \\] Consequentemente, \\[ H_{0}: \\mu_{1} \\ge \\mu_{2} \\quad ou \\quad \\mu_{1}- \\mu_{2} \\ge 0 \\] A \\(H_{0}\\) e \\(H_{1}\\) são opostas e mutuamente exclusivas. No teste de hipótese calcula-se a probabilidade de obter os resultados encontrados caso não haja efeito na população, ou seja, caso a \\(H_{0}\\) seja verdadeira. Portanto, o TH é um teste de significância para a \\(H_{0}\\). 11.4.1 Exemplo Voltando à hipótese de pesquisa, usando os dados da Seção 11.2, as hipóteses estatísticas seriam escritas da seguinte maneira, considerando uma hipótese alternativa bilateral. \\[ H_{0}: \\mu_{peso_{masc}} = \\mu_{peso_{fem}} \\quad ou \\quad \\mu_{peso_{masc}} - \\mu_{peso_{fem}}=0 \\] \\[ H_{1}: \\mu_{peso_{masc}} \\neq \\mu_{peso_{fem}} \\quad ou \\quad \\mu_{peso_{masc}} - \\mu_{peso_{fem}} \\neq 0 \\] 11.5 Escolha do teste estatítico e regra de decisão 11.5.1 Teste estatístico Usa-se um teste estatístico para testar as hipóteses estabelecidas. Este depende do tipo de distribuição da variável, por exemplo, teste z, teste t, teste F, qui-quadrado (\\(\\chi^2\\)). Cada teste fornece um valor para dirigir a decisão de rejeitar ou não a hipótese nula. Essa decisão depende da magnitude do teste valor. O nome para esse indicador, calculado para orientar a escolha, é estatística de teste. Para fazer isso, há necessidade de determinar qual seria a distribuição amostral da estatística de teste se a hipótese nula fosse realmente verdadeira. Depois de analisar esse valor, decide-se se a hipótese nula está correta ou, caso contrário, ela é rejeitada em favor da alternativa. É fundamental lembrar que cada teste estatístico tem suas características e seus pressupostos que devem ser analisados para garantir a validade das estatísticas de teste. Para uma boa parte deles, por exemplo, deve-se verificar se os dados se ajustam à distribuição normal (normalidade), a igualdade das variâncias (homocedasticidade), independência entre os grupos, tipo de correlação, etc. 11.5.2 Regra de decisão Realizado o teste estaístico, para rejeitar ou não rejeitar a \\(H_{0}\\), partindo do pressuposto de que ela é verdadeira, há necessidade de determinar uma regra de decisão que permita uma declaração fundamentada. Essa regra de decisão cria duas regiões, uma região de rejeição e uma região de não rejeição da \\(H_{0}\\), demarcadas por um valor crítico. Este valor de referência é determinado pelo nível de significância, \\(\\alpha\\), e deve ser explicitamente mencionado antes de se iniciar a pesquisa, pois é baseado nele que se fundamentam as conclusões da mesma. O nível de significância corresponde a probabilidade de rejeitar uma hipótese nula verdadeira. Quando a hipótese alternativa não tem uma direção definida, a área de rejeição, \\(\\alpha\\), é colocada nas duas caudas (Figura 11.1, superior), dividindo a probabilidade (\\(\\frac {\\alpha}{2}\\)); quando houver indicação prévia de um sentido, a área de rejeição ficará a direita (Figura 11.1, inferior) ou a esquerda dependendo da direção escolhida. Figura11.1: Regiões bicaudais (acima) e monocaudal à direita (abaixo) de rejeição e não rejeição da hipótese Quais valores exatos da estatística de teste deve-se associar à hipótese nula e quais valores exatos devem ser associados à hipótese alternativa? Para encontrar a região de rejeição, deve-se levar em consideração: * A estatística do teste deve ser muito grande ou muito pequena para que a hipótese nula seja rejeitada; * Distribuição da variável de teste, que depende da distribuição da população em estudo e do tamanho da amostra; * Nível se significância adotado, em geral, usa-se um \\(\\alpha\\) = 0,05, o que equivale a dizer que a região de rejeição abrange 5% da distribuição. É importante entender bem este último ponto. A região de rejeição corresponde aos valores da estatística de teste para os quais se rejeita a hipótese nula e a distribuição amostral em questão descreve a probabilidade de obtermos um determinado valor da estatística de teste se a hipótese nula for efetivamente verdadeira. Agora, suponha-se que foi escolhido uma região de rejeição que cobre 10% da distribuição amostral e que a hipótese nula é realmente verdadeira. Qual seria a probabilidade de rejeitar incorretamente a hipótese nula? Obviamente, a resposta é 10%! E o teste usado teria um nível \\(\\alpha\\) = 0,10. Ou seja, se a hipótese nula é verdadeira e for rejeitada, foi cometido um erro. 11.5.2.1 Erros de decisão Como se observa, ao se tomar uma decisão existe a possibilidade de se cometer erros. O primeiro erro é denominado de erro tipo I e ocorre quando, baseado na regra de decisão escolhida, uma hipótese nula verdadeira é rejeitada. Nesse caso, tem-se um resultado falso positivo. Há uma conclusão de que existe um efeito quando na verdade ele não existe. A probabilidade de cometer esse tipo de erro é \\(\\alpha\\), o mesmo usado como nível de significância no estabelecimento da regra de decisão. \\[ P(rejeitar \\quad H_{0}|H_{0} \\quad verdadeira) = \\alpha \\] Qual o valor de \\(\\alpha\\) que pode representar forte evidencia contra \\(H_{0}\\), reduzindo a possibilidade de erro tipo I? O valor de \\(\\alpha\\) escolhido, apesar de arbitrário, deve corresponder a importância do que se pretende demonstrar, quanto mais importante, menor deve ser o valor de \\(\\alpha\\). Nesses casos, não se quer rejeitar incorretamente \\(H_{0}\\) mais de 5% das vezes. Isso corresponde ao nível de significância mais usado de 0,05 (\\(\\alpha = 0,05\\)). Em algumas situações também são utilizados 0,01 e 0,10. Como mencionado, o valor de \\(\\alpha\\) deve ser escolhido antes de iniciar o estudo. Existe uma outra possibilidade de erro, denominado de erro tipo II, que ocorre quando a hipótese nula é realmente falsa, mas com base na regra de decisão escolhida, não se rejeita essa hipótese nula. Nesse caso, o resultado é um falso negativo; não se conseguiu encontrar um efeito que realmente existe. A probabilidade de cometer esse tipo de erro é chamada de \\(\\beta\\). \\[ P(não \\quad rejeitar \\quad H_{0}|H_{0} \\quad falsa) = \\beta \\] Na construção de um teste de hipótese, o erro tipo II é considerado menos grave que o erro tipo I. Entretanto, ele é bastante importante. Tradicionalmente, adota-se o limite de 0,10 a 0,20 para o erro tipo II. Na (Figura 11.2) estão resumidas as possíveis consequências na tomada de decisão em um teste de hipótese (100). Figura11.2: Tomada de decisão e erros. 11.5.3 Exemplo (continuação) Continuando com o exemplo da Seção 11.4.1, aceita-se que os pesos dos recém-nascidos de ambas as amostras tenham distribuição normal e que as variâncias são semelhantes. Apesar de o desvio padrão (\\(\\sigma\\)) da população-alvo, rnt, ser conhecido, será suposto que ele é desconhecido. Portanto, o teste t de amostras independentes será o teste escolhido como o teste estatístico. A hipótese alternativa é bilateral e o \\(\\alpha\\) = 0,05. A distribuição t é dependente dos grau de liberdade, que para duas amostras independentes é igual \\(gl=n_1+n_2-2\\). Para os dados em uso, tem-se: n1 &lt;- resumo.dados$n[1] n2 &lt;- resumo.dados$n[2] gl &lt;- n1 + n2 - 2 gl ## [1] 198 Para o nível de significância escolhido, o valor crítico de t para gl = 198 e uma hipótese alternativa bilateral pode ser obtido da seguinte maneira: alpha &lt;- 0.05 p &lt;- 1 - alpha/2 tc &lt;- round(qt(p, gl),3) tc ## [1] 1.972 A partir do cálculo do valor crítico de t, podemos estabelecer a regra de decisão para as hipóteses estatísticas: \\[ |t_{calculado}| &lt; |t_{crítico}| \\to não \\quad se \\quad rejeita \\quad H_{0} \\\\ |t_{calculado}| \\ge |t_{crítico}| \\to rejeita-se \\quad H_{0} \\] O teste t pode ser calculado no R, usando a função t_teste() do pacote rstatix. Esta função usa, entre outros, os seguintes argumentos: data \\(\\to\\) dataframe contendo as variáveis da formula; formula \\(\\to\\) uma fórmula da forma x ~ grupo onde x é uma variável numérica que fornece os valores dos dados e grupo é um fator; paired \\(\\to\\) lógico; indicando se o teste é pareado. Padrão é FALSE; var.equal \\(\\to\\) lógico: se TRUE, uma variância combinada é usada; caso contrário, a aproximação de Welch dos graus de liberdade é usada alternative → two.sided (padrão) ou greater ou less. teste &lt;- rstatix::t_test(data = dados, formula = pesoRN~sexo, alternative = &quot;two.sided&quot;, detailed = TRUE) teste ## # A tibble: 1 × 15 ## estimate estimate1 estimate2 .y. group1 group2 n1 n2 statistic p ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 164. 3333. 3169. pesoRN masc fem 109 91 2.58 0.0106 ## # ℹ 5 more variables: df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, method &lt;chr&gt;, ## # alternative &lt;chr&gt; A saída do teste mostra uma estatística de teste 38 igual a 2.583. Esta é maior do que o t_crítico = 1.972, consequentemente, rejeita-se a hipótese nula e conclui-se, com uma confiança de 95%, que existe uma diferença estatisticamente significativa no peso dos recém-nascidos entre os sexos. Esta diferença é em média igual a 164 g (IC95%: 39, 289), \\(peso_{meninos} &gt; peso_{meninas}\\). 11.6 Valor P do teste Nas seções anteriores, foi discutido um procedimento onde se encontrou o valor de probabilidade tal que uma dada hipótese nula é rejeitada ou não é rejeitada, de acordo com o nível de significância, \\(\\alpha\\), fixado, pelo pesquisador, no início da pesquisa. Essa abordagem do valor de probabilidade, mais comumente chamada de abordagem do valor P, fornece esse valor. Uma vez realizada a pesquisa, o pesquisador calcula a probabilidade de obter um resultado tão ou mais extremo que o observado, uma vez que a hipótese nula é verdadeira. O valor P também é conhecido como nível descritivo do teste (101). O objetivo de um teste estatístico é transformar em probabilidade a magnitude do desvio verificado em relação ao valor esperado, fornecendo o valor P. A partir daí pode-se, também, definir a regra de decisão, usando esse valor P. Toma-se o valor predeterminado (em geral, 0,05) de \\(\\alpha\\) e, então, compara-se o valor P com \\(\\alpha\\) e toma-se a decisão. Usando essa abordagem, rejeita-se a \\(H_{0}\\) se o valor P &lt; \\(\\alpha\\) e não se rejeita se o valor P &gt; \\(\\alpha\\). Costuma-se dizer que se o valor P &lt; \\(\\alpha\\), o resultado é significativo e não significativo quando P &gt; \\(\\alpha\\). Uma boa parte dos pesquisadores, principalmente no início da carreira, ficam empolgados pelo conhecimento do valor P. Entretanto, deve ser sempre lembrado que encontrar o valor P não é o único foco da pesquisa. O foco deve estar dirigido ao tamanho do efeito (effect size). O valor P obtido pelo teste estatístico, vai informar apenas sobre a probabilidade de se cometer erro ao rejeitar ou não rejeitar a hipóteses nula. 11.6.1 Exemplo (continuação) O teste realizado, t_test(), fornece o valor P = 0.0106. Este valor é menor do que \\(\\alpha\\) e leva as mesmas conclusões da Seção 11.5.3. 11.7 Poder do teste O poder do teste estatístico é a probabilidade de que um teste de hipótese rejeite corretamente a hipótese nula quando uma hipótese alternativa específica é verdadeira. É denotado comumente por \\(1 - \\beta\\) e representa a capacidade de um teste para detectar um efeito, se esse efeito realmente existir. O poder varia de 0 a 1 e, à medida que o poder do teste aumenta, a probabilidade \\(\\beta\\) de cometer um erro tipo II diminui. \\[ Poder \\quad do \\quad teste = P(rejeitar \\quad H_{0}|H_{0} \\quad falsa) \\] Na Figura 11.3, visualiza-se o poder em verde mais escuro. Em um teste de hipótese, o valor  sempre é estabelecido com antecedência, que geralmente é definido como 0,05, de modo que a taxa de erro do Tipo I é definida antes mesmo de se iniciar o teste. Em seguida, pode-se calcular o valor crítico mínimo necessário para rejeitar \\(H_0\\). É possível traçar uma linha da distribuição da hipótese nula até a distribuição da hipótese alternativa e separar a área sob a curva em duas partes. Se o valor t calculado cair à esquerda da linha tracejada, não se consegue rejeitar \\(H_0\\) quando \\(H_1\\) for verdadeira e é cometido um erro do Tipo II. Se o valor calculado cair à direita, rejeita-se \\(H_0\\) quando \\(H_1\\) é verdadeira e a decisão é correta. Portanto, a área à direita da curva é o poder. Figura11.3: Nível de significância, probabilidade de erro tipo II, poder e nível de confiança em um teste de hipótese e a região de rejeição da hipótese nula (à direita da linha vertical tracejada). O poder do teste depende de vários fatores, como: O nível de significância do teste, que é a probabilidade de rejeitar a hipótese nula quando ela é verdadeira (erro tipo I). A magnitude do efeito, que é a diferença entre o valor real do parâmetro e o valor considerado na hipótese nula. A variabilidade da população, que é medida pelo desvio padrão ou pela variância dos dados. O tamanho da amostra, que é o número de observações coletadas para o teste. Em geral, quanto maior o nível de significância, maior o poder do teste. Quanto maior a magnitude do efeito, maior o poder do teste. Quanto menor a variabilidade da população, maior o poder do teste. Quanto maior o tamanho da amostra, maior o poder do teste. Existem diferentes métodos para calcular o poder do teste, dependendo do tipo de teste e da distribuição dos dados. Por exemplo, para um teste de uma média com variância desconhecida, usa-se a distribuição t de Student com \\(n - 1\\) graus de liberdade. Para um teste de duas proporções, usa-se a distribuição normal aproximada. A análise de poder é uma ferramenta útil para planejar um estudo e determinar o tamanho da amostra necessário para obter um poder desejado. Ela também pode ser usada para avaliar a qualidade de um estudo realizado e verificar se o teste foi capaz de detectar um efeito relevante. 11.7.1 Exemplo (continuação) O teste t retornou um resultado significativo, com valor de t = 2.583 &gt; 1.972, com P = 0.0106. Um resultado significativo não informa sobre a magnitude do efeito. Para isso, lançamos mão do teste d de Cohen que pode ser calculado, usando a função cohensD() do pacote lsr: d &lt;- lsr::cohensD (data = dados, formula = pesoRN ~ sexo) d ## [1] 0.3721228 Na Seção 12.2.6.1, se entrará em maiores detalhes, por enquanto, será assumido que a magnitude do efeito é pequena. De posse do valor do d de Cohen, é possível calcular, através da função pwr.t.test() do pacote pwr, o poder do teste estatístico. Os argumentos dessa função são: n \\(\\to\\) número de observações por amostra; d \\(\\to\\)magnitude do efeito, d de Cohen; sig.level \\(\\to\\)nível de significância (padrão = 0.05); power \\(\\to\\)poder do teste; type \\(\\to\\)tipo de teste (one- , two- ou paired-samples); alternative \\(\\to\\)hipótese alternativa, deve ser “one-sided” ou “two-sided (padrão). O parâmetro que se quer calcular deve ser passado como NULL. Assim, o poder do teste estatístico do exemplo é: poder &lt;- pwr::pwr.t.test(n = 150, d = d, sig.level = 0.05, power = NULL, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;) poder ## ## Two-sample t test power calculation ## ## n = 150 ## d = 0.3721228 ## sig.level = 0.05 ## power = 0.8947719 ## alternative = two.sided ## ## NOTE: n is number in *each* group A saída mostra que no lugar do NULL, aparece o poder do teste estatístico. Ou seja, o poder foi de 0.895 , consequentemente, como \\(\\beta = 1 – Poder\\), então, \\(\\beta\\) = 0.105. O poder geralmente é definido em 0,80 (ou 0,90). Isto significa que se existirem efeitos verdadeiros a serem encontrados em 100 estudos diferentes com 80% de poder, apenas 80 em 100 testes estatísticos irão realmente detectá-los. Se não for garantido poder suficiente, é possível que nenhum efeito seja detectado, por isso, deve-se calcular o tamanho amostral necessário, antes de iniciar qualquer estudo, para garantir o poder pretendido. Conhecer os parâmetros da população, é muito raro. Aqui isto aconteceu, artificialmente, para fins didáticos.↩︎ Ter em mente que nunca se pode saber com total certeza se existe um efeito na população.↩︎ Para ver todas as estatísticas do teste, basta escrever teste$ e apertar a tecla TAB do teclado e surgirá um menu para escolha.↩︎ "],["sec-testet.html", "Capítulo 12 Comparação entre duas médias 12.1 Pacotes necessários para este capítulo 12.2 Teste t para amostras independentes 12.3 Teste t para grupos pareados", " Capítulo 12 Comparação entre duas médias 12.1 Pacotes necessários para este capítulo pacman::p_load(car, dplyr, ggplot2, ggpubr, ggsci, kableExtra, knitr, lsr, readxl, rstatix, tidyr) 12.2 Teste t para amostras independentes O teste t de amostras independentes é usado para comparar duas médias de amostras de grupos não relacionados. Isso significa que há pessoas diferentes fornecendo pontuações para cada grupo. O objetivo desse teste é determinar se as amostras são diferentes uma da outra 12.2.1 Dados usados nesta seção Suponha-se que em uma determinada ilha hipotética existam duas populações etnicamente diferentes. Foram coletadas aleatoriamente a altura de 30 mulheres de cada população. Os dados, dadosPop.xlsx, contém três variáveis: id \\(\\to\\) identificação dos participantes altura \\(\\to\\) medida da altura em metros; pop \\(\\to\\) variável categórica que identifica os grupos: população 1 e população 2. Estes dados podem ser obtidos aqui. Faça o download e salve no seu diretório de trabalho. 12.2.1.1 Leitura dos dados Para a leitura dos dados, será usado a função read_excel() incluído no pacote readxl que deve ser instalado e carregado. Os dados serão recebidos por um objeto que será denominado de dados: dados &lt;- readxl::read_excel(&quot;Arquivos/dadosPop.xlsx&quot;) Para visualizar os dados, pode-se usar a função glimpse() do pacote dplyr: dplyr::glimpse(dados) ## Rows: 60 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, … ## $ altura &lt;dbl&gt; 1.50, 1.56, 1.63, 1.66, 1.60, 1.65, 1.49, 1.60, 1.56, 1.58, 1.5… ## $ pop &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … Observa-se que existem 60 mulheres, sendo 30 moradoras na região 1 e 30 na região 2. A variável id é a variável de identificação das mulheres (variável numérica), altura é uma variável numérica que corresponde a medida da altura em metros e pop é uma variável categórica, onde 1 são as mulheres da população 1 e 2 as da população 2. No conjunto de dados, esta variável encontra-se como uma variável numérica e deverá ser transformada em fator. 12.2.1.2 Exploração e resumo dos dados Inicialmente, a variável pop será transformada em fator: dados$pop &lt;- as.factor(dados$pop) A seguir, calcular a média e o desvio padrão da variável altura de acordo com pop, usando a função group_by () e summarise do pacote dplyr resumo &lt;- dados %&gt;% dplyr::group_by(pop) %&gt;% dplyr:: summarise(n = n(), media = mean(altura, na.rm = TRUE), dp = sd(altura, na.rm = TRUE), mediana = median(altura, na.rm = TRUE), me = 1.96 * dp/sqrt(n)) resumo ## # A tibble: 2 × 6 ## pop n media dp mediana me ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 30 1.60 0.0621 1.6 0.0222 ## 2 2 30 1.39 0.0736 1.38 0.0263 Além do resumo numérico, é interessante construir um gráfico do tipo boxplot (Figura 12.1), usando o pacote ggplot2 (veja Seção 6.6): ggplot2::ggplot(data = dados, aes(x = pop, y = altura, fill = pop)) + geom_errorbar(stat = &quot;boxplot&quot;, width = 0.1) + geom_boxplot() + labs (x = &quot;Populações&quot;, y = &quot;Altura (m)&quot;) + theme_bw() + theme(legend.position=&quot;none&quot;) Figura12.1: Boxplot dos dados Os boxplot sugerem que a altura das mulheres nessas populações são diferentes. 12.2.2 Definição das hipóteses estatísticas As hipóteses comparam as médias dos dois grupos. Para um teste bicaudal, considerando pop1 a população 1 e pop2 a população 2, as hipóteses são escritas como: \\[ H_{0}: \\mu_{pop1} = \\mu_{pop2} \\] \\[ H_{1}: \\mu_{pop1} \\neq \\mu_{pop2} \\] 12.2.3 Definição da regra de decisão O nível significância, \\(\\alpha\\), escolhido é igual a 0.05. A distribuição t é dependente dos graus de liberdade, dados por: No exemplo, n1 &lt;- resumo$n[1] n2 &lt;- resumo$n[2] gl &lt;- n1 + n2 - 2 gl ## [1] 58 Para um \\(\\alpha = 0,05\\), o valor crítico de t para gl =58 para uma hipótese alternativa bicaudal é obtido com a função qt (p, df), onde \\(df = gl\\) e \\(p = 1 - \\alpha/2\\) alpha &lt;- 0.05 p &lt;- 1 - alpha/2 tc &lt;- round (qt((1-alpha/2), gl), 3) tc ## [1] 2.002 Portanto, se \\[ |t_{calculado}| &lt; |t_{crítico}| \\to não \\quad se \\quad rejeita \\quad H_{0} \\\\ t_{calculado}| \\ge t_{crítico}| \\to rejeita-se \\quad H_{0} \\] 12.2.4 Teste estatístico Para determinar se existe uma diferença estatisticamente significativa entre as médias das alturas das duas populações não relacionadas, será usado o teste t duas amostras independentes, também conhecido como teste t de Student, baseado na distribuição de mesmo nome. 12.2.4.1 Lógica do teste t O teste t compara as médias de duas amostras independentes, usando o erro padrão como métrica da diferença entre essas médias. Quanto maior o valor de t , maior a probabilidade de que as amostras pertençam a populações diferentes, ocorrendo nessas circunstâncias a rejeição da hipótese nula (102). Calcula-se o teste t com a seguinte equação: \\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2) - (\\mu_1 - \\mu_2)}{EP_{d}} \\] Onde \\(EP_d\\) é o erro padrão da diferença entre a médias \\(\\bar{x}_1 - \\bar{x}_2\\). Se a hipótese nula for verdadeira, as amostras foram retiradas da mesma população e, portanto, \\(\\mu_1 - \\mu_2 = 0\\). Assim, a equação fica: \\[ t = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{EP_d} \\] O erro padrão da diferença \\(\\bar{x}_1 - \\bar{x}_2\\) é calculado de maneiras diferentes: Se a variâncias nos dois grupos forem iguais, usa-se: \\[ EP_d = \\sqrt{s_o^2(\\frac{1}{n_1}+\\frac{1}{n_2})} \\] Onde \\(s_o^2\\) é a variância combinada ou conjugada que é, simplesmente, a média ponderada das variância dos grupos: \\[ s_0^2 = \\frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{(n_1 -1)+ (n_2-1)} \\] Quando os grupos têm o mesmo tamanho (\\(n_1 = n_2\\)), \\(s_o^2\\) é simplesmente a média aritmética da variância dos grupos: \\[ s_0^2 = \\frac {s_1^2 + s_2^2}{2} \\] \\[ EP_d = \\sqrt{\\frac{2 s_o^2}{n}} \\] Se as variâncias dos dois grupos forem diferentes: \\[ EP_d = \\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}} \\] Esta explicação da lógica e dedução da estatística de teste serve para uma melhor compreensão de como o teste funciona, mas para executar um teste t não há necessidade disso, basta saber como encaminhar ao R e como interpretar o resultado fornecido por ele. 12.2.4.2 Pressupostos do teste t O teste t assume que: As amostras são independentes; Deve haver distribuição normal. Entretanto, quando as amostras são grandes (teorema do limite central), isso não é muito importante; Exista homocedasticidade, ou seja, as variâncias dos grupos devem ser iguais. Violar o pressuposto de número 3 tem importância se os tamanhos dos grupos forem diferentes. Se os grupos tiverem o mesmo tamanho e a amostra for grande, este pressuposto torna-se menos importante, não importando muito se esta hipótese foi violada (103). O pressuposto tem mais importância em grupos pequenos e desiguais. Existe um teste, denominado teste t de Welch que corrige essa violação. É possível portanto, esquecer esse pressuposto e fazer o teste de Welch sempre. Avaliação da normalidade Uma boa parte dos procedimentos estatísticos são testes paramétricos 39 com base na distribuição normal. Ou seja, se assume que a distribuição dos dados segue o modelo da distribuição normal. Se essa suposição não for atendida, a lógica por trás do teste de hipóteses pode ser violada. Pode-se verificar a normalidade de maneira visual, observando o comportamento dos dados através de gráficos como o histograma (Figura 12.2) e o gráfico Q-Q (Figura 12.3). É útil também sobrepor uma distribuição normal no histograma, para fins de comparação com a distribuição normal. Além disso, nos histogramas, pode-se observar como as duas populações se sobrepõem. mu1 &lt;- resumo$media[1] dp1 &lt;- resumo$dp[1] mu2 &lt;- resumo$media[2] dp2 &lt;- resumo$dp[2] pop1 &lt;- dados %&gt;% filter (pop == 1) pop2 &lt;- dados %&gt;% filter (pop == 2) ggplot(dados) + geom_histogram(aes(x = altura, fill = pop, y = after_stat(density)), col= &quot;white&quot;, alpha = 0.5, bins = 15) + stat_function(data = pop1, fun = dnorm, color = &quot;red&quot;, lty = &quot;dashed&quot;, lwd = 1, args = list(mean = mu1, sd = dp1)) + stat_function(data = pop2, fun = dnorm, color = &quot;darkred&quot;, lty = &quot;dashed&quot;, lwd = 1, args = list(mean = mu2, sd = dp2)) + labs(x=&quot;Altura (m)&quot;, y=&quot;Densidade&quot;) + scale_fill_manual(values = c(&quot;pink2&quot;, &quot;pink3&quot;)) + theme_bw() Figura12.2: Histogramas da altura das mulheres O gráfico QQ (ou gráfico quantil-quantil) desenha a correlação entre uma determinada amostra e a distribuição normal. Uma linha de referência de 45 graus também é plotada. Um gráfico Q-Q é um gráfico de dispersão criado plotando dois conjuntos de quantis um contra o outro. Se ambos os conjuntos de quantis vierem da mesma distribuição, observa-se os pontos formando uma linha aproximadamente reta. Se os valores caírem na diagonal do gráfico, a variável é normalmente distribuída. Os desvios da diagonal mostram desvios da normalidade. Para desenhar um gráfico Q-Q pode ser usado a função ggqqplot ()40 do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%. ggqqplot(dados, x = &quot;altura&quot;, color = &quot;pop&quot;) + labs(y = &quot;Altura (m)&quot;, x = &quot;Quantis teóricos&quot;) Figura12.3: Gráficos Q-Q Observando os gráficos, verifica-se que a variável altura tem uma distribuição visualmente normal aceitável, pois o histograma se ajusta à curva normal e os gráficos Q-Q mostram que os dados seguem aproximadamente a linha diagonal. Outra maneira de analisar a normalidade é verificar se a distribuição como um todo se desvia de uma distribuição normal comparável. Para isso, usam-se testes estatísticos de normalidade. Os dois principais são o teste de Shapiro-Wilk e o teste de Kolmogorov-Smirnov (K-S). Esses testes comparam os dados da amostra com um conjunto de valores normalmente distribuídos com a mesma média e desvio padrão. Se o teste não for significativo (P &gt; 0,05), informa-se que a distribuição da amostra não é significativamente diferente de uma distribuição normal. Se, no entanto, o teste for significativo (P \\(\\le\\) 0,05), a distribuição em questão será significativamente diferente de uma distribuição normal. O método de Shapiro-Wilk é amplamente recomendado para teste de normalidade (104), (105), (106). sw &lt;- dados %&gt;% dplyr::group_by(pop) %&gt;% rstatix::shapiro_test(altura) sw ## # A tibble: 2 × 4 ## pop variable statistic p ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 altura 0.971 0.553 ## 2 2 altura 0.948 0.154 A saída mostra que ambos valores P do teste, 0.553 e 0.154, estão acima de 0,05, corroborando com a não rejeição da normalidade dos dados. Homogeneidade da Variância Na visualização da Figura 12.4 nos dois grupos de mulheres, observa-se que há, entre os limites inferior e superior, uma dispersão das medidas em torno da região central que vai progressivamente diminuindo. Esta dispersão parece ser semelhante nos grupos. Isto sugere que haja homogeneidade das variâncias. Portanto, homogeneidade da variância é o pressuposto de que a dispersão das medidas é aproximadamente igual em diferentes grupos de casos, ou que a dispersão dos valores são aproximadamente iguais em pontos diferentes da variável preditora. ggplot2::ggplot(dados, aes(x = pop, y = altura)) + geom_errorbar(stat = &quot;boxplot&quot;, width = 0.1) + geom_boxplot(outlier.shape = NA) + geom_jitter(aes(color = pop), position = position_jitter(0.08), size = 2, shape = 20) + labs(x = &quot;População&quot;, y = &#39;Altura (m)&#39;) + theme_bw() + scale_fill_nejm() + theme(legend.position=&quot;none&quot;) Figura12.4: Gráfico mostrando a dispersão dos dados Além do aspecto visual, a homogeneidade da variância pode ser testada com o teste de Levene. Neste teste, a \\(H_{0}\\) é todas as variâncias são iguais. No R, a função que calcula o teste é leveneTest() do pacote car (107). Os argumentos são: y \\(\\to\\) variável de resposta para o método padrão ou um objeto lm ou fórmula. Se y for um objeto de modelo linear ou uma fórmula, as variáveis do lado direito do modelo devem ser todas fatores e devem ser completamente cruzadas; group \\(\\to\\) fator que define os grupos; center \\(\\to\\) O nome de uma função para calcular o centro de cada grupo; mean fornece o teste de Levene original; o padrão, median, fornece um teste mais robusto; data \\(\\to\\) conjunto de dados para avaliar a formula. levene &lt;- car::leveneTest(altura~pop, center = mean, data = dados) levene ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 1 0.1599 0.6907 ## 58 A saída do teste de Levene retorna um valor P &gt; 0,05, confirma a impressão visual dos boxplots de que os grupos têm homogeneidade das variâncias, portanto a hipótese nula de igualdade das variâncias não pode ser rejeitada. Um outro teste que compara duas variância poderia ser usado. É o teste F que pode ser calculado com a função var.test() do pacote stats, incluído no R base. Seus argumentos pode ser consultados na ajuda do R. var.test(altura~pop, alternative = &quot;two.sided&quot;, data = dados) ## ## F test to compare two variances ## ## data: altura by pop ## F = 0.71253, num df = 29, denom df = 29, p-value = 0.3667 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.3391409 1.4970303 ## sample estimates: ## ratio of variances ## 0.7125337 A saída do teste permite uma conclusão igual ao teste de Levene, pois o valor P = 0,3667 &gt; 0,05. 12.2.4.3 Execução do teste t de Student Os pressupostos do teste não foram violados, portanto ele pode ser realizado com confiança. Será utilizado a função t_test() do pacote rstatix ((108)) para calcular o teste t para amostras independentes. Ele fornece uma estrutura compatível com operador pipe %&gt;% (pipe-friendly) para executar testes t de uma e duas amostras. Para consultar os argumentos, consulte a Seção 11.5.3 ou a ajuda do RStudio. teste &lt;- dados %&gt;% rstatix::t_test(formula = altura ~ pop, detailed = TRUE, var.equal = TRUE) teste ## # A tibble: 1 × 15 ## estimate estimate1 estimate2 .y. group1 group2 n1 n2 statistic ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0.212 1.60 1.39 altura 1 2 30 30 12.1 ## # ℹ 6 more variables: p &lt;dbl&gt;, df &lt;dbl&gt;, conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;, ## # method &lt;chr&gt;, alternative &lt;chr&gt; A saída, retorna a estimativa da diferença média (0.212), as estimativas das médias dos grupos (arredondadas), a estatística do teste (12.0557002) o valor P (1.95^{-17}), graus de liberdade (58)41 e outras métricas. Também é possível ver os resultados do teste t , usando o objeto teste que os recebeu. Por exemplo, os limites inferior (conf.low) e superior (conf.high) do intervalo de confiança de 95% da estimativa da diferença entre as médias. IC95 &lt;- round(c(teste$conf.low, teste$conf.high),3) IC95 ## [1] 0.177 0.247 12.2.5 Conclusão Como \\(|t_{calculado}|\\) = 12.056 &gt; \\(|t_{0,05;58}|\\) = 2.002, rejeita-se \\(H_{0}\\). Observa-se que o valor P é muito pequeno (1.95^{-17}) e, portanto, a diferença observada nas médias dos dois grupos deve ser assumida como significativa. Assim, as duas populações são diferentes e deve-se admitir que as amostras são procedentes de populações com médias de altura diferentes, com probabilidade de erro extremamente pequena. A estimativa da diferença média (\\(\\mu_1 - \\mu_2\\)) é fornecida pelo intervalo de confiança de 95% (0.177, 0.247). Observe que o valor zero não está contido no intervalo e isto confirma a não significância estatística da diferença. Concluindo, a altura das mulheres da população 1 e a altura das mulheres da população 2 são diferentes, a diferença (\\(\\mu_1 - \\mu_2\\)) encontrada é estatisticamente significativa (t = 12.056, gl = 58, P = 1.95^{-17}), com uma confiança de 95%. Esta conclusão pode ser visualizada em um gráfico (Figura 12.5) que exibirá a saída do teste t: Construir dois boxplots, usando o ggplot2 com cores do New England Journal of Medicine (NEJM), do pacote ggsci . Atribuir a um objeto bp: bp &lt;- ggplot(dados, aes(x=pop, y=altura)) + geom_errorbar(stat = &quot;boxplot&quot;, width = 0.1) + geom_boxplot(aes(fill = pop), color = &quot;black&quot;) + scale_color_nejm() + theme_bw() + theme(legend.position=&quot;none&quot;) Adicionar ao boxplot novos rótulos e os testes realizados: bp + labs(x = &quot;População&quot;, y = &#39;Altura (m)&#39;, title = &#39;Altura de Mulheres&#39;, subtitle = rstatix::get_test_label(stat.test = teste, correction = &quot;none&quot;, detailed = TRUE, type = &quot;expression&quot;, p.col = &quot;p&quot;)) Figura12.5: Boxplots comparando os dois grupos 12.2.6 Tamanho do Efeito A significância estatística deve ter uma atenção relativa do pesquisador, pois ela apenas mede a probabilidade de rejeitar uma hipótese nula, uma vez que ela seja verdadeira. Ajudam a determinar, em uma pesquisa, a significância dos resultados encontrados em relação à hipótese nula, mas não informam nada em relação a magnitude do efeito. Por exemplo, mostra se determinado tratamento afeta as pessoas, mas não dizem quanto isso as afeta. O tamanho do efeito (effect size) é uma medida quantitativa da magnitude do efeito. Quanto maior o tamanho do efeito, mais forte é a relação entre duas variáveis. É possível observar o tamanho do efeito ao comparar dois grupos quaisquer para ver quão substancialmente diferentes eles são. Normalmente, em ensaios clínicos tem-se um grupo de tratamento e um grupo de controle. O grupo de tratamento é uma intervenção que se espera efetue um resultado específico. O valor do tamanho do efeito mostrará se a terapia teve um efeito pequeno, médio ou grande. Isso tem mais relevância do que simplesmente informar o tamanho do valor P. 12.2.6.1 d de Cohen Também conhecida como diferença média padronizada, o d de Cohen (109) (110) é uma medida adequada e bastante popular para encontrar a magnitude do efeito na comparação entre duas médias. Para calcular a diferença média padronizada se verifica a diferença entre as médias dos dois grupos e se divide pelo desvio padrão conjugado: \\[ d = \\frac{(\\bar{x}_1 - \\bar{x}_2)}{s_{o}} \\] Onde, \\[ s_o =\\sqrt \\frac{(n_1 - 1)s_1^2 + (n_2 -1)s_2^2}{n_1 + n_2 - 2} \\] Voltando ao exemplo da altura das mulheres em duas populações, o d de Cohen é calculado, usando a função cohensD() do pacote lsr que usa os seguintes argumentos: x \\(\\to\\) um vetor numérico de valores de dados, variável preditora; y \\(\\to\\) um vetor numérico de valores de dados, variável resposta; formula \\(\\to\\) Fórmula na forma variável resposta ~ grupo; data \\(\\to\\) dataframe ou matriz; method \\(\\to\\) Qual versão da estatística d devemos calcular? Os valores possíveis são pooled(padrão), x.sd, y.sd, corrected, raw, paired e unequal.; mu \\(\\to\\) O valor “nulo” contra o qual o tamanho do efeito deve ser medido. Quase sempre é 0 (padrão); raramente especificado. Assim, o d de Cohen pode ser obtido da seguinte forma: d &lt;- lsr::cohensD (altura ~ pop, data = dados) d ## [1] 3.112768 Bastante simples! Agora, como interpretar este resultado de d = 1,1 (arredondado)? Sua interpretação não é intuitiva, recomenda-se usar a Tabela 12.1 para interpretar (109). Tabela12.1: Tamanho do Efeito d Interpretação &lt; 0,2 insignificante 0,2 &lt; 0.5 pequeno 0.5 &lt; 0.8 médio &gt;= 0,8 grande Assim, as alturas das mulheres diferem significativamente (P &lt; 0,0001) de acordo com a população, sendo que as mulheres da população 1 são bem mais altas do que as da população 2 e a magnitude dessa diferença é grande (d = 3.1). 12.3 Teste t para grupos pareados Um teste t pareado é usado para estimar se as médias de duas medidas relacionadas são significativamente diferentes uma da outra. Esse teste é usado quando duas variáveis contínuas são relacionadas porque são coletadas do mesmo participante em momentos diferentes (antes e depois), de locais diferentes na mesma pessoa ao mesmo tempo ou de casos e seus controles correspondentes. 12.3.1 Dados usados nesta seção O banco de dados é constituído por uma amostra de 15 escolares portadores de asma não controlada. Fizeram avaliação da sua função pulmonar no início do uso de um novo corticoide inalatório. Após 60 dias, repetiram a avaliação da função pulmonar. Para baixar o banco de dados, clique aqui. Faça o downloado para o seu diretório de trabalho. 12.3.1.1 Leitura e transformação dos dados Leia o arquivo dadosPar.xlsx a partir do diretório de trabalho, usando a função read_excel() do pacote readxl. Atribuir os dados a um objeto com o nome dados. dados &lt;- readxl::read_excel(&quot;Arquivos/dadosPar.xlsx&quot;) A estrutura dos dados podem ser visualizada, usando a função str(): str(dados) ## tibble [15 × 3] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:15] 1 2 3 4 5 6 7 8 9 10 ... ## $ basal: num [1:15] 1.3 1.47 2.06 1.95 1.47 1.13 1.48 0.94 1.05 0.87 ... ## $ final: num [1:15] 1.53 1.63 2.35 2.7 2.01 1.53 1.66 1.59 1.5 1.61 ... O dataframe dados encontra-se no formato amplo (wide), ou seja, com as colunas basal e final colocadas lado a lado como se fossem duas variáveis distintas, quando, na realidade, constituem-se em apenas uma variável contendo as medidas de VEF1 (Volume Expiratório Forçado no primeiro segundo). A função pivot_longer() do pacote tidyr fará a transformação do formato amplo para o longo (long). Este processo não é obrigatório, mas será realizado para fins de treinamento. O novo banco de dados será atribuído ao objeto dadosL. A função pivot_longer() necessita dos seguintes argumentos: dados \\(\\to\\) dataframe a ser pivotado, tranformado; cols \\(\\to\\) colunas a serem transformadas no formato longo; names_to \\(\\to\\) Especifica o nome da coluna a ser criada a partir dos dados armazenados nos nomes das colunas de dados; values_to \\(\\to\\) Especifica o nome da coluna a ser criada a partir dos dados armazenados nos valores das células; … \\(\\to\\) possui outros argumento. Ver ajuda. dadosL &lt;- dados %&gt;% tidyr::pivot_longer(c(basal, final), names_to = &quot;momento&quot;, values_to = &quot;medidas&quot;) str(dadosL) ## tibble [30 × 3] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:30] 1 1 2 2 3 3 4 4 5 5 ... ## $ momento: chr [1:30] &quot;basal&quot; &quot;final&quot; &quot;basal&quot; &quot;final&quot; ... ## $ medidas: num [1:30] 1.3 1.53 1.47 1.63 2.06 2.35 1.95 2.7 1.47 2.01 ... 12.3.1.2 Medidas Resumidoras Para resumir as variáveis, serão usadas as funções group_by() e summarise() do pacote dplyr, aplicadas ao formato longo dadosL: resumo &lt;- dadosL %&gt;% dplyr::group_by(momento) %&gt;% dplyr::summarise(n = n (), media = mean(medidas, na.rm = TRUE), dp = sd (medidas, na.rm = TRUE), mediana = median (medidas, na.rm = TRUE), IIQ = IQR (medidas, na.rm =TRUE), ep = dp/sqrt(n), me = ep * qt(1 - (0.05/2), n - 1)) resumo ## # A tibble: 2 × 8 ## momento n media dp mediana IIQ ep me ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 basal 15 1.31 0.427 1.26 0.48 0.110 0.236 ## 2 final 15 1.69 0.471 1.59 0.38 0.122 0.261 12.3.1.3 Visualização dos dados Tabela É possível exibir os dados, tanto o banco de dados dados como o dadosL, de uma maneira mais elegante, usando a função kable() do pacote knitr 42 e a função kable_styling() do pacote kableExtra. A função kable () usa a função head() embutida. Ao executar os códigos, se não for especificado, é mostrado apenas 6 linhas. Será mostrado o formato amplo e todas as suas 15 linhas: knitr::kable(head(dados, 15), booktabs = TRUE, col.names = c(&quot;Id&quot;, &quot;Basal&quot;, &quot;Final&quot;), caption = &quot;Função pulmonar de 15 escolares asmáticos antes-e-depois \\\\ do uso de um corticoide inalatório&quot;) %&gt;% kableExtra::kable_styling(position = &quot;center&quot;, latex_options = &quot;hold_position&quot;) %&gt;% kableExtra::kable_classic(full_width = F, html_font = &quot;Cambria&quot;) %&gt;% kableExtra::column_spec(2, width = &quot;3.5cm&quot;) %&gt;% kableExtra::column_spec(3, width = &quot;3.5cm&quot;) %&gt;% kableExtra::row_spec(0, bold = TRUE) Tabela12.2: Função pulmonar de 15 escolares asmáticos antes-e-depois  do uso de um corticoide inalatório Id Basal Final 1 1.30 1.53 2 1.47 1.63 3 2.06 2.35 4 1.95 2.70 5 1.47 2.01 6 1.13 1.53 7 1.48 1.66 8 0.94 1.59 9 1.05 1.50 10 0.87 1.61 11 0.75 1.17 12 1.26 1.30 13 1.21 1.41 14 0.78 1.00 15 1.99 2.37 Gráficos Apenas, por uma questão didática, serão apresentadas várias maneiras de mostrar os dados visualmente. Podem ser usados qualquer um dos tipos a seguir, pois todos dão, praticamente, a mesma informação. Gráfico de barra de erro resumo %&gt;% ggplot2::ggplot(aes(x=momento, y=media, fill=momento)) + geom_bar(stat=&quot;identity&quot;, width = 0.4, color=&quot;black&quot;) + geom_point() + geom_errorbar(aes(ymin=media, ymax=media+me), width=0.1, position=position_dodge(.9)) + labs(title=&quot;Avaliação de um corticoide inalatório&quot;, x=&quot;Momento&quot;, y = &quot;Volume Forçado em 1 seg (L)&quot;)+ theme_classic() + theme(legend.position=&quot;none&quot;) + scale_fill_manual(values=c(&quot;cyan4&quot;,&quot;cyan3&quot;)) Figura12.6: Gráfico de barra de erro comparando o grupo antes-e-depois Nesse gráfico (Figura 12.6), a altura da barra representa a média do Volume Forçado em 1 seg (VEF1) nos diferentes momentos (basal e final).O erro corresponde a margem de erro (me) a partir do ponto (média), ou seja, é o intervalo de confiança de 95%. O limite inferior do IC95% foi suprimido. Boxplot dadosL %&gt;% ggplot2::ggplot(aes(x = momento, y = medidas, fill = momento)) + geom_errorbar(stat = &quot;boxplot&quot;, width = 0.1) + geom_boxplot (outlier.color = &quot;red&quot;, outlier.shape = 1, outlier.size = 1) + scale_fill_manual(values = c(&quot;cyan4&quot;,&quot;cyan3&quot;)) + ylab(&quot;Volume Forçado em 1 seg (L)&quot;) + xlab(&quot;Momento&quot;) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 19, size = 2, color=&quot;red&quot;) + theme_bw() + theme(text = element_text(size = 12)) + theme(legend.position = &quot;none&quot;) Figura12.7: Boxplots comparando o grupo antes-e-depois A altura da caixa dos boxplots (Figura 12.7) é o intervalo interquartil (IIQ) e corresponde a 50% dos dados. A linha que corta horizontalmente a caixa é a mediana. Os bigodes da caixa (whiskers) em suas extremidades são os limites inferior e superior dos dados, excluindo os valores atípicos (outliers), representado no boxplot final por um ponto vermelho, acima do limite superior. Os pontos em vermelho (dentro das caixas) representam as médias. Gráfico de linha resumo %&gt;% ggplot2::ggplot(aes(x=momento, y=media, group=1)) + geom_line(linetype =&#39;dashed&#39;) + geom_errorbar(aes(ymin=media - me, ymax=media + me), width=0.1, linewidth = 1, col = c(&quot;cyan4&quot;,&quot;cyan3&quot;)) + geom_point(size = 3, color = c(&quot;cyan4&quot;, &quot;cyan3&quot;)) + theme_classic()+ labs(x=&#39;Momento&#39;, y=&#39;Volume Forçado em 1 seg (L)&#39;) Figura12.8: Gráfico de linha comparando o grupo antes-e-depois Este gráfico de linha (Figura 12.8) com representação da margem de erro tem a mesma interpretação do gráfico de barra de erro. A escolha do tipo de gráfico depende da ênfase do autor sobre os dados. 12.3.1.4 Criação de uma variável que represente a diferença entre as médias A diferença entre as média basal e final será atribuída ao nome D. Esta ação será realizada, utilizando o banco de dados amplo (dados): dados$D &lt;- dados$basal - dados$final head (dados) ## # A tibble: 6 × 4 ## id basal final D ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.3 1.53 -0.23 ## 2 2 1.47 1.63 -0.16 ## 3 3 2.06 2.35 -0.29 ## 4 4 1.95 2.7 -0.75 ## 5 5 1.47 2.01 -0.54 ## 6 6 1.13 1.53 -0.4 Atenção, agora, o banco de dados apresenta uma nova variável D, pois o foco do teste t pareado é essa diferença entre as médias, basal e final, a média das diferenças. Resumo da variável D Ao resumo será atribuído ao nome sumario (sem acento): resumoD &lt;- dados %&gt;% dplyr::summarise(media = mean (D), dp = sd (D), mediana = median (D), IIQ = IQR (D), min = min (D), max = max (D)) resumoD ## # A tibble: 1 × 6 ## media dp mediana IIQ min max ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.377 0.218 -0.38 0.285 -0.75 -0.0400 Existe uma diferença de 0.38L entre o VEF1 basal e o final. A pergunta que se faz é: Esta diferença tem significância estatística? Os gráficos sugerem que sim! 12.3.2 Definição das hipóteses estatísticas Será usado um teste bicaudal. Se a intervenção não produz efeito, então: \\[ H_0: \\mu_D = 0 \\] Se a intervenção produz efeito, então: \\[ H_1: \\mu_D \\neq 0 \\] 12.3.3 Regra de decisão O nível significância, \\(\\alpha\\), escolhido é igual a 0,05. A distribuição da estatística do teste, sob a \\(H_{0}\\), é a distribuição t que é dependente dos graus de liberdade. O número de graus de liberdade á igual ao número de observações menos 1, neste caso são o número de pares menos 1. n &lt;- length(dados$D) gl &lt;- n - 1 gl ## [1] 14 Para um \\(\\alpha = 0,05\\), o valor crítico de t para gl = 14 para uma hipótese alternativa bicaudal: alpha &lt;- 0.05 p &lt;- 1 - alpha/2 round(qt(p, 14), 3) ## [1] 2.145 Portanto, se \\[ \\mid t_{calculado}\\mid &lt; \\mid t_{crítico}\\mid -&gt; não \\quad rejeitar \\quad H_{0} \\\\ \\mid t_{calculado}\\mid &gt; \\mid t_{crítico}\\mid -&gt; rejeitar \\quad H_{0} \\] 12.3.4 Teste estatístico 12.3.4.1 Lógica do teste A estatística do teste t dependente é a mesma do teste t independente r dada por: \\[ T = \\frac{\\bar{D} - \\mu_{D}}{EP_{D}} \\] Como na equação do teste t para amostras independentes, sob a hipótese nula igual a zero, \\(\\mu_{D} = 0\\), assim, a equação fica: \\[ T = \\frac{\\bar{D}}{EP_{D}} \\] A estimativa do erro padrão das diferenças é dada por: \\[ EP_{D}=\\frac{s_{D}}{\\sqrt{n}} \\] O desvio padrão das diferenças, \\(s_{D}\\) , é dado por: \\[ s_{D}=\\sqrt\\frac{\\Sigma(D_{i} - \\bar{D})^2}{n - 1} \\] Onde \\(D_{i}\\) são as diferença individuais (\\(x_1 - y_1, x_2 - y_2, ..., x_n - y_n\\)). Da mesma maneira que no teste t para grupos independentes, essa demonstração serve para uma melhor compreensão de como o teste funciona, mas para executar este teste t não há necessidade disso, basta saber como encaminhar ao R, como será visto adiante. 12.3.4.2 Pressupostos do teste O teste t pareado assume que os seguintes pressupostos devem ser atendidos: Os dados devem ser dependentes; A variável desfecho deve estar em uma escala contínua; As diferenças entre os pares devem ter distribuição normal. Ao usar um teste t pareado, a variação entre os pares de medidas é a estatística mais importante e a variação entre os participantes, como no teste t de duas amostras independentes, é de pouco interesse, não havendo necessidade de se verificar se as variâncias dos grupos são iguais. Para testar o pressuposto de normalidade das diferenças, usa-se a variável criada da diferença entre os pares, D. Verifica-se a normalidade dessa variável com o teste Shapiro-Wilk, usando a função shapiro_test() do pacote rstatix, já usada no teste t de amostras independentes. shapiro &lt;- dados %&gt;% rstatix::shapiro_test(D) shapiro ## # A tibble: 1 × 3 ## variable statistic p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 D 0.942 0.410 O teste de Shapiro-Wilk retorna um valor P &gt; 0,05, mostrando que a variável D que não se pode rejeitar a hipóteses nula de sua normalidade. Além disso, um gráfico Q-Q (Figura 12.9) pode ser usado para avaliar a normalidade, com a função ggqqplot() do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95% ggpubr::ggqqplot (dados$D, color = &quot;steelblue&quot;) + labs(y = &quot;Diferença Basal-Inicial&quot;, x = &quot;Quantis teóricos&quot;) + theme_bw() Figura12.9: Gráfico Q-Q para avaliar a normalidade Os resultados do teste de Shapiro-Wilk e ográfico QQ, mostram que a \\(H_{0}\\) de normalidade da variável D não é rejeitada, apesar de haver uma pequena assimetria à esquerda que não impede o prosseguimento da análise. 12.3.4.3 Execução do teste estatístico O cálculo do teste t pareado pode usar a mesma função do teste t para amostras independentes, t_test(), do pacote rstatix, mudando o argumento paired =FALSE(padrão) por paired =TRUE. Assim: teste_par &lt;- dadosL %&gt;% rstatix:: t_test(formula = medidas ~ momento, paired = TRUE, detailed = TRUE) teste_par ## # A tibble: 1 × 13 ## estimate .y. group1 group2 n1 n2 statistic p df conf.low ## * &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.377 medidas basal final 15 15 -6.70 0.0000102 14 -0.497 ## # ℹ 3 more variables: conf.high &lt;dbl&gt;, method &lt;chr&gt;, alternative &lt;chr&gt; Observe que foi usado o conjunto de dados de formato longo (dadosL) para usar a fórmula (x ~ grupo). Da mesma maneira do que o teste t para amostras independentes, é possível ver os resultados do teste t , usando o objeto teste_par que os recebeu. Por exemplo, os limites inferior (conf.low) e superior (conf.high) do intervalo de confiança de 95% da estimativa de diferença (D) entre as médias IC95 &lt;- round(c(teste_par$conf.low, teste_par$conf.high),3) IC95 ## [1] -0.497 -0.256 12.3.5 Conclusão Conclui-se que o VEF1 dos escolares asmáticos se modificou significativamente entre o início e após 60 dias do uso de um novo medicamento com uma confiança de 95%. A diferença (\\(\\mu_{basal} - \\mu_{final}\\)) encontrada é estatisticamente significativa (t = -6.6969, gl = 14, P = 1.02^{-5}), com uma confiança de 95%. Observe que o intervalo de confiança de 95% da diferença de -0.38 está todo abaixo de zero (-0.497, -0.256), confirmando a significância. 12.3.6 Tamanho do Efeito O tamanho do efeito pode ser determinado, também, com o teste d de Cohen, usando a função cohensD() do pacote lsr: d_par &lt;- lsr::cohensD (dados$basal, dados$final) d_par ## [1] 0.8379499 Dessa forma, o uso do novo corticoide inalatório modificou significativamente o VEF1 dos escolares asmáticos com o uso de um novo corticoide inalatório (P = 1.02^{-5}), mostrando um aumento deste e que a magnitude dessa diferença é grande (d = 0.84). Os resultados podem ser apresentados usando um gráfico de linha (Figura 12.10), aproveitando o resultado da função t_test(). resumo %&gt;% ggplot2::ggplot(aes(x=momento, y=media, group=1)) + geom_line(linetype =&#39;dashed&#39;) + geom_errorbar(aes(ymin=media - me, ymax=media + me), width=0.1, size = 1, col = c(&quot;cyan4&quot;,&quot;cyan3&quot;)) + geom_point(size = 2) + labs(title=&quot;Avaliação do Uso de Corticosteroide Inalatório&quot;, subtitle = rstatix::get_test_label(stat.test = teste_par, correction = &quot;none&quot;, detailed = TRUE, type = &quot;expression&quot;), x=&quot;Momento&quot;, y = &quot;Volume Expiratório Forçado em 1 seg (L)&quot;, caption = &quot;d Cohen = 0,84&quot;)+ theme_bw() + theme(legend.position=&quot;none&quot;) Figura12.10: Gráfico de linha comparando um grupo de escolares asmáticos antes e depois do uso de um corticosteroide inalatório Teste paramétricos são testes estatísticos que se baseiam nos padrões da distribuição populacional da variável em estudo, por exemplo, a distribuição normal é descrita por dois parâmetros – média e desvio padrão – que são suficientes para se conhecer as probabilidades. Os testes que não requerem a especificação da forma de distribuição da população, ou seja, têm distribuição livre, são denominados de não paramétricos.↩︎ Veja também a Seção 8.3.1.2.↩︎ Se as variâncias forem diferentes (var.equal = FALSE), o teste calcula os graus de liberdade pela fórmula de Welch, bem mais complicada.↩︎ Veja também a Seção 6.4.2.2↩︎ "],["análise-de-variância.html", "Capítulo 13 Análise de Variância 13.1 Pacotes necessários para este capítulo 13.2 Por que realizar uma ANOVA? 13.3 ANOVA de um fator 13.4 ANOVA de dois fatores", " Capítulo 13 Análise de Variância 13.1 Pacotes necessários para este capítulo pacman::p_load(car, dplyr, effectsize, emmeans, fastGraph, ggplot2, ggpubr, ggsci, kableExtra, knitr, readxl, rstatix) 13.2 Por que realizar uma ANOVA? Inicialmente, para analisar os grupos, se ficaria tentado a fazer comparações por pares usando um teste t de amostras independentes. Com existem quatro grupos, é possível compará-los realizando seis testes, grupo 1 versus grupo 2, grupo 1 versus grupo 3, grupo 1 versus grupo 4, grupo 2 versus grupo 3, grupo 2 versus grupo 4 e grupo 3 versus grupo 4. Se os dados têm k grupos são necessários \\(\\frac {k!}{2!(k-2)!}\\) testes. A probabilidade de um erro do tipo I não ocorrer para cada teste t é de 0,95 (isto é, 1 – 0,05), supondo um \\(\\alpha\\) = 0,05. Os três testes são independentes; portanto, a probabilidade de um erro do tipo I não ocorrer nos seis testes é de \\((0,95)^6 = 0,735\\). Dessa maneira, a probabilidade de ocorrer pelo menos um erro do tipo I nos seis testes t de duas amostras é de 1 – 0,735 ou 0,265 (26,5%), o que é mais alto do que o nível de significância definido de 0,05 (111). Logo, uma ANOVA de um fator é usada para verificar as diferenças entre vários grupos dentro de um fator, reduzindo assim o número de comparações em pares e a probabilidade de ocorrer um erro tipo I. 13.2.1 Lógica do Modelo da ANOVA O procedimento de ANOVA é utilizado para testar a hipótese nula de que as médias de três 43 ou mais populações são as mesmas contra hipótese alternativa de que nem todas as médias são iguais. Na Seção 12.2.4.2, foram comparadas duas variâncias, usando um teste, denominado de teste F. Este teste, é uma razão entre duas variâncias e recebeu este nome em homenagem a Sir Ronald Aylmer Fisher. A variância é uma medida de dispersão que mensura como os dados estão espalhados em torno da média. Quanto maior o seu valor, maior a dispersão. Considere a Figura 13.1, onde está representada a distribuição de uma variável X em três grupos independentes. Pode-se, claramente, distinguir observações provenientes dessas distribuições, pois a sobreposição delas é pequena. Cada uma dela se dispersa pouco em torno da média. Figura13.1: Três distribuições diferentes Agora, observe o Figura 13.2, onde a distribuição da variável X é mostrada, mantendo as mesmas médias, mas com variâncias maiores. Isto torna claro que se o objetivo é distinguir observações provenientes desses grupos não basta avaliar suas médias, há necessidade de comparar a variação entre os grupos com a variação dentro de cada grupo (112). Figura13.2: Distribuições com mesmas médias da figura anterior, mas variâncias maiores Se a variação entre os grupos for grande quando comparada à variação dentro de cada grupo, aumenta a probabilidade de reconhecer a proveniência das observações (Figura 13.1). Entretanto, se a variação entre os grupos for pequena comparada à variação dentro do grupo, torna difícil a distinção de observações provenientes dos grupos (Figura 13.2). Portanto, usar o teste F para determinar se as médias de grupo são iguais é apenas uma questão de incluir as variâncias corretas na razão. Na ANOVA com um fator, a estatística F é a razão dos estimadores das variância entre e dentro dos grupos. \\[ F = \\frac{variância \\quad ENTRE \\quad os \\quad grupos}{variância \\quad DENTRO \\quad dos \\quad grupos} \\] Quando o valor de F fica próximo de 1, significa que as variâncias são muito próximas; quando F é significativamente maior do que 1, é possível distinguir os indivíduos de diferentes grupos. Ou seja, se o objetivo for mostrar que as médias são diferentes, será bom que a variância dentro dos grupos seja baixa. Pode-se pensar na variância dentro do grupo como o ruído que pode obscurecer a diferença entre os sons (as médias). No gráfico da Figura 13.1, o valor de F seria alto, no da Figura 13.2 seria baixo. Como saber se o valor de F é alto o suficiente? Um único valor F é difícil de interpretar sozinho. Há necessidade de colocá-lo em um contexto maior antes que seja possível interpretá-lo. Para fazer isso, usa-se a distribuição F para calcular as probabilidades. 13.2.2 Distribuição F A razão entre a variabilidade entre os grupos e a variabilidade dentro do grupo segue uma distribuição F quando a hipótese nula é verdadeira. Quando se realiza uma ANOVA com um fator obtém-se um valor F. No entanto, se forem extraídas várias amostras aleatórias do mesmo tamanho da mesma população e fosse repetida a mesma análise, o resultado seriam muitos valores F diferentes, constituindo uma distribuição amostral, denominada de distribuição F. Dessa forma, como a distribuição F assume que a hipótese nula é verdadeira, é possível colocar o resultado de qualquer valor F, resultante do teste de ANOVA, e determinar quão consistente ele é com a hipótese nula e calcular a probabilidade. A probabilidade que se quer calcular é a probabilidade de observar uma estatística F que é pelo menos tão alta quanto o valor que o estudo obteve. Essa probabilidade permite determinar quão comum ou raro é o valor F, sob a suposição de que a hipótese nula é verdadeira. Se a probabilidade for pequena o suficiente, pode-se concluir que dados são inconsistentes com a hipótese nula. Como já foi mostrado em outros momentos, essa probabilidade é o valor P. O formato de uma curva de distribuição F depende do número de graus de liberdade. No entanto, a distribuição F tem dois números de graus de liberdade: graus de liberdade para o numerador (variância entre) e graus de liberdade para o denominador (variância dentro). Esses dois graus de liberdade são os parâmetros da distribuição F. Cada combinação de graus de liberdade fornece uma curva de distribuição F diferente. As unidades de uma distribuição F são denotadas por F, que assume apenas valores positivos. Como as distribuições normal, t e qui-quadrado (veja 16.2), a distribuição F é uma distribuição contínua. A forma de uma curva de distribuição F é inclinada para à direita, mas a assimetria diminui à medida que o número de graus de liberdade aumenta, conforme observado na Figura 13.3. Figura13.3: Distribuições F. 13.2.2.1 Funções do R para trabalhar com a distribuição F No R, existem quatro funções principais para trabalhar com a distribuição F: df(x, gl1,gl2) calcula a densidade de probabilidade da distribuição F no ponto x; df1 e df2 são os graus de liberdade do numerador e denominador, respectivamente 44 ; pf(x, gl1, gl2) calcula a função de probabilidade acumulada da distribuição F no ponto x; qf(p, gl1, gl2) calcula o quantil da distribuição F correspondente a uma probabilidade p; rf(n, gl1, gl2) gera n valores aleatórios da distribuição F com os parâmetros gl1 e gl2. Essas funções são úteis para resolver problemas de probabilidade envolvendo a distribuição F. Por exemplo, se o objetivo é saber qual é a probabilidade de uma variável aleatória F com 10 e 20 graus de liberdade no numerador e no denominador, respectivamente, ser menor que 1, pode-se usar a função pf() da seguinte forma: x &lt;- 1 p &lt;- pf(x, 10, 20,lower.tail = TRUE) round(p, 3) ## [1] 0.524 Ou seja, ao se observar a curva da Figura 13.3 de cor verde (gl1 = 10 e gl2 = 20), a probabilidade abaixo de x = 1 é igual a 52,4%. Para calcular a densidade de probabilidade quando x = 1, pode-se usar a função df(): x &lt;- 1 d &lt;- df(x, 10, 20) round(d, 3) ## [1] 0.714 A saída da função df() corresponde à altura da curva da Figura 13.3 de cor verde (gl1 = 10 e gl2 = 20) quando x é igual a 1. Para encontrar o valor da distribuição F(10,20) que corresponde ao percentil 50%, ou seja, o valor que deixa 50% da área da curva à esquerda, usa-se a função qf() com lower.tail=TRUE. Assim: p &lt;- 0.50 x &lt;- qf(p, 10, 20, lower.tail = TRUE) round(x,3) ## [1] 0.966 Para representar, graficamente, esse resultado, foi construido o gráfico da Figura 13.4 com a shadeDist() do pacote fastGraph ((113)) , verifica-se que a área sob a curva abaixo de 0,97 é igual a 50%. Consulte a ajuda do RStudio para maiores detalhes dos argumentos da função. Figura13.4: Área da curva da distribuição F (10,20) abaixo de x = 0,97 é igual a 50% Para gerar 100.000 valores aleatórios da distribuição F com gl1=10 e gl2=20,será usada a função rf(). Em seguida, plota-se um histograma (Figura 13.5) com curva da distribuição F sobreposta (linha vermelha) e compara-se com a função de densidade de probabilidade da distribuição F (curva verde da Figura 13.3). x &lt;- rf(100000, df1 = 10, df2 = 20) hist(x, breaks = &#39;Scott&#39;, freq = FALSE, xlim = c(0,3), ylim = c(0,1), ylab = &quot;Densidade&quot;, xlab = &#39;&#39;, main = &#39;Histograma para uma distribuição F(10,20)&#39;, cex.main=0.9) curve(df(x, df1 = 10, df2 = 20), from = 0, to = 4, n = 5000, col= &#39;red&#39;, lwd=2, add = T) Figura13.5: Histograma com curva sobreposta de uma distribuição F (10,20) 13.3 ANOVA de um fator A análise de variância (ANOVA) de um fator, também conhecida como ANOVA de uma via, é uma extensão do teste t independente para comparar duas médias em uma situação em que há mais de dois grupos. Dito de outra forma, o teste t para uso com duas amostras independentes é um caso especial da análise de variância de uma via. A ANOVA de um fator compara o efeito de uma variável preditora (variável independente, fator) sobre uma variável contínua (desfecho). Por exemplo, verificar se a intensidade do tabagismo na gestação (não fumantes, fumantes leves, moderados ou pesados) afetam o peso dos recém-nascidos. Os boxplots da Figura 13.6parece mostrar que sim. Figura13.6: Impacto do tabagismo materno no peso ao nascer 13.3.1 Dados do exemplo Para testar a hipótese de que a intensidade do tabagismo materno tem efeito sobre o peso do recém-nascido, foram selecionados aleatoriamente 200 recém-nascidos classificados em quatro grupos de n = 50 cada grupo, conforme a quantidade de cigarros fumados por dia por suas mães. Estes dados estão no arquivo dadosFumo.xlsx. Grupo 1: recém-nascidos de mães não fumantes; Grupo 2: recém-nascidos de mães que fumavam até 10 cigarros/dia – categorizado como tabagismo leve; Grupo 3: recém-nascidos de mães que fumavam de 11 a 19 cigarros/dia – categorizado como tabagismo moderado; Grupo 4: recém-nascidos de mães que fumavam \\(\\ge\\) 20 cigarros por dia – categorizado como tabagismo pesado. Para baixar o banco de dados, clique aqui. Salve o mesmo no seu diretório de trabalho. 13.3.1.1 Leitura dos dados A leitura será feita com a função read_excel() do pacote readxl e serão atribuídos a um objeto de nome dados e verificada a sua estrutura com a função head(). dados &lt;- readxl::read_excel(&quot;Arquivos/dadosFumo.xlsx&quot;) str (dados) ## tibble [200 × 3] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:200] 1 2 3 4 5 6 7 8 9 10 ... ## $ pesoRN: num [1:200] 3458 2723 4125 2905 3608 ... ## $ fumo : num [1:200] 1 1 1 1 1 1 1 1 1 1 ... 13.3.1.2 Exploração e resumo dos dados Como a variável fumo encontra-se como uma variável numérica, será transformada em fator que é a sua verdadeira classe com 4 níveis. dados$fumo &lt;- factor (dados$fumo, ordered = TRUE, levels = c(1, 2, 3, 4), labels = c (&quot;não&quot;, &quot;leve&quot;, &quot;moderado&quot;, &quot;pesado&quot;)) As medidas resumidoras serão obtidas, usando as funções group_by () e summarise () do pacote dplyr. alpha = 0.05 resumo &lt;- dados %&gt;% dplyr::group_by(fumo) %&gt;% dplyr::summarise(n = n(), media = mean(pesoRN, na.rm = TRUE), dp = sd (pesoRN, na.rm = TRUE), ep = dp/sqrt(n), me = qt ((1-alpha/2), n-1)*ep, IC_Inf = media - me, IC_sup = media + me) resumo ## # A tibble: 4 × 8 ## fumo n media dp ep me IC_Inf IC_sup ## &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 não 50 3395. 405. 57.3 115. 3280. 3510. ## 2 leve 50 3102. 431. 60.9 122. 2980. 3225. ## 3 moderado 50 3151. 495. 70.0 141. 3011. 3292. ## 4 pesado 50 2954. 443. 62.6 126. 2828. 3080. 13.3.1.3 Visualização gráfica dos dados Os boxplots (Figura 13.7) são uma maneira interessante de visualizar os dados, principalmente com o pacote ggplot245 : Figura13.7: Boxplots do impacto do tabagismo materno no peso ao nascer Observa-se que há uma tendência de o peso ao nascer diminuir à medida que quantidade de cigarros fumados aumenta. Entretanto, esta diferença pode ser pelo acaso. 13.3.2 Definição das hipóteses estatísticas Para testar a igualdade entre as médias, \\(H_{0}: \\mu_{1} = \\mu_{2} = \\mu_{3} = \\mu_{4}\\), supondo homocedasticidade, isto é, as variâncias \\(σ_1^2=σ_2^2=σ_3^3=σ_4^2\\). A hipótese alternativa, \\(H_1\\), diz que, pelo menos, uma das médias é diferente das demais. Ela não é unilateral ou bilateral, é multifacetada porque permite qualquer relação que não seja todas as médias iguais. Por exemplo, a \\(H_1\\) inclui o caso em que \\(μ_1=μ_2=μ_3\\), mas \\(μ_4\\) tem um valor diferente. 13.3.3 Definição da regra de decisão O nível significância, \\(\\alpha\\), geralmente escolhido é igual a 0,05. A distribuição da estatística do teste, sob a \\(H_{0}\\), é a distribuição F. O número de graus de liberdade total \\((n – 1)\\) é dividido em dois componentes: Grau de liberdade do numerador (ENTRE) é dado por \\(gl_{E} = k - 1\\), onde k é o número de grupos. Grau de liberdade do denominador (DENTRO ou residual) é dado por \\(gl_{D} = n - k\\), onde, \\(n = \\sum n_{i}\\). No exemplo, para um \\(\\alpha = 0,05\\), tem-se: alpha &lt;- 0.05 k &lt;- length(resumo$media) n &lt;- nrow(dados) glE &lt;- k - 1 glE ## [1] 3 glD &lt;- n - k glD ## [1] 196 Com esses dados, usando a a função qf()calcula-se o valor crítico de F (Figura 13.8) que é igual: Fc &lt;- qf(1 - alpha, glE, glD) round(Fc, 2) ## [1] 2.65 Portanto, se \\[ |F_{calculado}| &lt; |F_{crítico}| \\to não \\quad se \\quad rejeita \\quad H_{0} \\\\F_{calculado}| \\ge F_{crítico}| \\to rejeita-se \\quad H_{0} \\] Figura13.8: Curva da Distribuição F 3,196 = 2,65 13.3.4 Teste Estatístico A estatística de teste é obtida calculando duas estimativas da variância populacional, \\(\\sigma^2\\): a variância entre os grupos (\\(s_{E}^2\\)) e a variância dentro dos grupos (\\(s_{D}^2\\)). A variância entre os grupos também é chamada de quadrado médio entre os grupos (\\(QM_{E}\\)) e é igual a soma dos quadrados entre (\\(SQ_{E}\\)) ou do fator dividida pelos graus de liberdade entre: \\[ QM_{E} = \\frac{SQ_{E}}{gl_{E}} \\] A variância dentro dos grupos é também denominada de quadrado médio dentro dos grupos ou residual (\\(QM_{D}\\)) e é igual a soma dos quadrados dentro dividida pelos graus de liberdade dentro: \\[ QM_{D} = \\frac {SQ_{D}}{gl_{D}} \\] A variância entre os grupos, \\(QM_{E}\\), dá uma estimativa de \\(\\sigma^2\\) com base na variação entre as médias das amostras extraídas de diferentes populações. Para o exemplo das quatro categorias de tabagismo durante a gestação, o \\(QM_{E}\\) será baseado nos valores das médias dos pesos dos recém-nascidos nos quatro grupos diferentes. Se as médias de todas as populações em consideração forem iguais, as médias das respectivas amostras ainda serão diferentes, mas a variação entre elas deverá ser pequena e, consequentemente, espera-se que o valor do \\(QM_{E}\\) seja pequeno. No entanto, se as médias das populações consideradas não são todas iguais, espera-se que a variação entre as médias das respectivas amostras seja grande e, consequentemente, o valor de \\(QM_{E}\\) seja grande. A variância dentro das amostras, \\(QM_{D}\\), dá uma estimativa de \\(\\sigma^2\\) com base na variação dos dados de diferentes amostras. Para o exemplo das quatro categorias de tabagismo durante a gestação, o \\(QM_{D}\\) será baseado nas médias individuais dos pesos dos recém-nascidos incluídos nas quatro amostras retiradas de quatro populações. O conceito de \\(QM_{D}\\) é semelhante ao conceito de desvio padrão conjugado ou agrupado, \\(s_{o}\\), para duas amostras. A estatística de teste é, como visto, a razão das variâncias entre e dentro do grupo. Dessa maneira, \\[ F = \\frac {s_{E}^2}{s_{D}^2} = \\frac {\\frac {SQ_{E}}{gl_{E}}}{\\frac {SQ_{D}}{gl_{D}}} = \\frac {QM_{E}}{QM_{D}} \\] 13.3.4.1 Avaliação dos pressupostos do teste Ao realizar um teste de ANOVA de um fator deve-se assumir que: As populações das quais as amostras são retiradas são normalmente distribuídas; As populações das quais as amostras são retiradas têm a mesma variância (homocedasticidade); Amostras aleatórias e independentes; Todos os grupos devem ter tamanho amostral adequado. Grupos com menos de 10 participantes são problemáticos por reduzirem a precisão da média. Na prática, deve-se evitar menos de 30 participantes. A relação entre os grupos não deve ser maior do que 1:4 (114); Não devem existir valores atípicos (outliers); A mensuração dos dados deve ser em nível intervalar ou de razão. Portanto, antes iniciar com o teste de hipótese, verifica-se se as suposições mencionadas para o teste de hipótese ANOVA unidirecional foram atendidas. As amostras são amostras aleatórias e independentes. Isto já é um bom começo! Avaliação da normalidade Verifica-se a premissa de normalidade, usando o teste de Shapiro-Wilk para os múltiplos grupos e desenhando um gráfico de probabilidade normal (gráficos Q-Q) para cada grupo. dados %&gt;% dplyr::group_by(fumo) %&gt;% shapiro_test(pesoRN) ## # A tibble: 4 × 4 ## fumo variable statistic p ## &lt;ord&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 não pesoRN 0.976 0.385 ## 2 leve pesoRN 0.979 0.499 ## 3 moderado pesoRN 0.985 0.776 ## 4 pesado pesoRN 0.971 0.257 Para o gráfico Q-Q (Figura 13.9), pode ser usado a função ggqqplot () do pacote ggpubr que produz um gráfico QQ normal com uma linha de referência, acompanhada de area sombreada, correspondente ao IC95%. ggpubr::ggqqplot(dados, x=&quot;pesoRN&quot;, facet.by = &quot;fumo&quot;) + labs(y = &quot;Peso ao nascer (g) (m)&quot;, x = &quot;Quantis teóricos&quot;) Figura13.9: Gráficos Q-Q O resultado do teste de Shapiro-Wilk entregou todos os resultados com valor P acima de 0.05 e os gráficos Q-Q, não são perfeitos, mas pode-se assumir que os dados para cada grupo caem aproximadamente em uma linha reta. Avaliação da homogeneidade das variâncias Em seguida, testa-se a suposição de que as variâncias são iguais, usando o Teste de Levene através da função leveneTest () do pacote `car. car::leveneTest(pesoRN~fumo, center = mean, data = dados) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 3 0.6306 0.5961 ## 196 O teste de Levene exibe como resultado um valor P &gt; 0,05, mostrando que não é possível rejeitar a \\(H_0\\) de igualdade das variâncias. Verificação da presença de outliers Pode-se aqui, além de verificar nos boxplots, usar a função by_group() do pacote dplyr junto com a função identify_outliers() do pacote rstatix : dados %&gt;% dplyr::group_by(fumo) %&gt;% rstatix::identify_outliers(pesoRN) ## # A tibble: 1 × 5 ## fumo id pesoRN is.outlier is.extreme ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 pesado 168 1611. TRUE FALSE Como mostrado nos boxplots, existe um valor atípico, ou seja, está abaixo de 1,5 IIQ. Entretanto, ele não é extremo (&gt; 3 IIQ). Da mesma maneira que no teste t, os pressupostos têm mais importância em grupos pequenos e desiguais. Para o exemplo em análise, os pressupostos foram verificados e pode-se assumir que os grupos são independentes e as médias têm distribuição normal e existe homocedasticidade, além disso, os grupos têm o mesmo tamanho (n = 50). Portanto, a análise pode ser continuada. O que fazer se os pressupostos são violados? Se a homogeneidade da variância é o problema, um teste possível de ser implementado no R é o F de Welch, aplicando a funçãowelch.test(), incluída no pacote onewaytests (115). Existem também testes não paramétricos, como o Teste de Kruskal-Wallis, que será visto mais adiante (Seção 17.6. 13.3.4.2 Execução do teste estatístico Para realizar um teste de hipótese ANOVA unidirecional no R, aplica-se a função aov() do R base. Esta função espera a chamada notação de fórmula, portanto, os dados são incluídos separando as duas variáveis de interesse separadas por ~ (til) e os dados, no qual as variáveis especificadas na fórmula, são encontradas. Além da fórmula e dos dados, a função aov() pode necessitar outros argumentos: effect.size \\(\\to\\) tamanho do efeito a ser calculado e mostrado nos resultados da ANOVA. Os valores permitidos podem ser “ges” (eta ao quadrado) ou “pes” (eta parcial ao quadrado) ou ambos. O padrão é “ges”; contrasts \\(\\to\\) uma lista de contrastes a ser usada para alguns dos fatores da fórmula modelo.aov &lt;- aov(pesoRN ~ fumo, dados) summary(modelo.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## fumo 3 5030606 1676869 8.482 2.52e-05 *** ## Residuals 196 38748837 197698 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A saída, liberada pela função summary(), é bem reduzida, relatando as informações específicas da Tabela da ANOVA, a estatística F junto com o valor P e os graus de liberdade, soma dos quadrados (Sum Sq) e quadrados médios (Mean Sq), que com frequência se necessita para o relatório do modelo. A variância entre os grupos também é chamada de quadrado médio entre os grupos e é igual à soma dos quadrados entre ou do fator dividida pelos graus de liberdade entre. A variância dentro dos grupos é também denominada de quadrado médio dentro dos grupos ou residual e é igual à soma dos quadrados dentro dividida pelos graus de liberdade dentro. A ANOVA detectou um efeito significativo do fator, que neste caso é o fumo, o valor \\(F_{calculado} = 8,48 &gt; F_{crítico} = 2,65\\) e o valor P &lt; 0,0001. Pode-se simplesmente relatar isso e encerrar, mas é provável que se queira saber quais grupos diferem uns dos outros. Lembre-se de que não se pode apenas inferir isso a partir de uma visão dos dados, existem testes estatísticos para ajudar a entender as diferenças dos grupos. 13.3.5 Testes post-hoc Os testes de comparações múltiplas constituem-se em uma análise após a realização da ANOVA. Se houve uma diferença, indicada pela ANOVA, os testes de comparações múltiplas ou também conhecidos como teste post hoc, ajudam a quantificar as diferenças entre os grupos para determinar quais grupos diferem significativamente uns dos outros. Aqui será usado o HSD de Tukey, que é conservador. HSD vem da expressão em inglês - Honest Significant Difference. Este teste requer um objeto aov no qual executa seu procedimento, que chamaremos de pwc. O procedimento de Tukey HSD executará uma comparação de pares de todas as combinações possíveis dos grupos e testará esses pares para diferenças significativas entre suas médias, tudo enquanto ajusta o valor P a um limite superior de significância para compensar o fato de que muitos testes estatísticos estão sendo realizados e a probabilidade de um falso positivo aumenta com o aumento do número de testes. A função a ser usada é a tukey_hsd(), do pacote rstatix. pwc &lt;- rstatix::tukey_hsd (modelo.aov) pwc ## # A tibble: 6 × 9 ## term group1 group2 null.value estimate conf.low conf.high p.adj ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 fumo não leve 0 -292. -523. -61.8 0.00654 ## 2 fumo não moderado 0 -243. -474. -12.8 0.0341 ## 3 fumo não pesado 0 -441. -671. -210. 0.00000915 ## 4 fumo leve moderado 0 49.0 -181. 279. 0.946 ## 5 fumo leve pesado 0 -149. -379. 81.9 0.342 ## 6 fumo moderado pesado 0 -198. -428. 32.8 0.121 ## # ℹ 1 more variable: p.adj.signif &lt;chr&gt; Com base nos valores P &lt; 0,05 tem-se três combinações de grupos que diferem: leve-não, moderado-não e pesado-não. Isto mostra que o grupo que difere é o das mães não fumantes. Pode-se visualizar isso na Figura 13.10 obtida com a função plot(), usando os resultados da função TukeyHSD() disponível no R base. Esta função gera o teste de Tukey com as diferença entre os pares e os intervalos de confiança que permitem a construção do gráfico. A função par()é empregada para adaptar as margens da figura ao tamanho da mesma e depois é usada novamente para retornar ao padrão par(mar=c(5.1, 4.1, 4.1, 2.1)). O argumento mar é um vetor numérico que define os tamanhos das margens na seguinte ordem: inferior, esquerda, superior e direita. par(mar=c(3,8,3,3)) # Adaptar o tamanho das margens plot(TukeyHSD(modelo.aov, conf.level = 0.95), las = 1) Figura13.10: Gráficos do Teste de Tukey par(mar=c(5.1, 4.1, 4.1, 2.1)) # Retorna as margens ao padrão 13.3.6 Tamanho do efeito Uma das medidas de tamanho de efeito mais comumente relatadas para a ANOVA é o eta ao quadrado (\\(\\eta^2\\)), que é um índice da força da associação entre um fator e uma variável dependente. Eta ao quadrado é a proporção da variação total atribuível ao fator. É calculado como a razão da variância do fator para a variância total e os valores variam de 0 a 1. Esta medida pode ser obtida com o pacote effectsize (116), usando a função eta_squared()com um objeto da classe tipo modelo.aov. effectsize::eta_squared (modelo.aov, partial = FALSE) ## # Effect Size for ANOVA (Type I) ## ## Parameter | Eta2 | 95% CI ## ------------------------------- ## fumo | 0.11 | [0.05, 1.00] ## ## - One-sided CIs: upper bound fixed at [1.00]. O eta quadrado é uma estimativa tendenciosa da força da associação, na medida em que superestima os efeitos, especialmente para amostras pequenas. Uma outra medida do tamanho do efeito menos tendenciosa é o ômega ao quadrado (\\(\\omega^2\\)). O ômega ao quadrado é uma medida corrigida, menos enviesada e menos inflacionada. Ela pode ser calculada com a função omega_squared(), também do pacote effectsize: effectsize::omega_squared (modelo.aov, partial = FALSE) ## # Effect Size for ANOVA (Type I) ## ## Parameter | Omega2 | 95% CI ## --------------------------------- ## fumo | 0.10 | [0.04, 1.00] ## ## - One-sided CIs: upper bound fixed at [1.00]. Apesar de ser controverso, pode-se seguir a orientação da Tabela 13.1, para a interpretação (117): Tabela13.1: Interpretação do tamanho do efeito Resultado Effectsize 0.01 pequeno 0,06 médio 0,14 grande 13.3.7 Conclusão O peso dos recém-nascidos foi estatisticamente diferente entre os diferentes grupos, F(3, 196) = 8,48, P = 0.0000252, \\(\\eta^2\\) = 0,11. As análises post-hoc de Tukey revelaram que o peso dos recém-nascidos a termo no grupo das gestantes não fumantes apresentou uma diferença estatisticamente significativa do grupo de tabagismo leve (-292 g, IC95%: -523 a -62 g; P = 0,0065); do grupo de tabagismo moderado (-243 g, IC95%: -474 a -13 g; P = 0,0341) e do grupo de tabagismo pesado (-441 g, IC95%: -671 a -210 g; P &lt; 0,0001), mas entre os grupos de fumantes não houve diferença estatisticamente significativa. 13.3.7.1 Apresentação dos resultados Serão apresentados boxplots (Figura 13.11), com ggboxplot(), do pacote ggpubr, utilizando, para cores, a pallete = \"jama\", do pacote ggsci. Para adicionar teste estatístico, usou-se a função get_test_label() e para o teste post hoc, a função get_pwc_label(), ambas do pacote rstatix. tab.aov &lt;- anova_test(dados, pesoRN ~ fumo, type = 2) pwc &lt;- tukey_hsd(dados,pesoRN~fumo) pwc &lt;- pwc %&gt;% add_xy_position (x = &quot;fumo&quot;) p &lt;- ggplot2::ggplot(dados, aes(x=fumo, y=pesoRN)) + stat_boxplot(geom = &quot;errorbar&quot;, width = 0.1) + geom_boxplot(aes(color = fumo), size = 0.8) + scale_color_nejm() + labs(x = &quot;Tabagismo&quot;, y = &quot;Peso ao nascer (g)&quot;, subtitle = get_test_label (tab.aov, detailed = TRUE), caption = get_pwc_label(pwc)) + stat_pvalue_manual (pwc, label = &quot;p.adj.signif&quot;, label.size = 3.5, hide.ns = TRUE) + theme (text = element_text (size = 12)) + theme_classic() p + theme(legend.position = &quot;none&quot;) Figura13.11: Efeito do tabagismo na gestação sobre o peso do recém-nascido.([*]: P entre 0,01 e 0,05; [**]: P entre 0,001 e 0,01; [****]: P &lt; 0,0001). 13.4 ANOVA de dois fatores A ANOVA de dois fatores é uma extensão da ANOVA de um fator. Neste tipo de ANOVA, ao invés de observar o efeito de um fator sobre a variável desfecho contínua, é analisado simultaneamente o efeito de duas variáveis de agrupamento. Outros sinônimos para a ANOVA de dois fatores são: ANOVA fatorial ou ANOVA de duas vias. Quando se tem dois ou mais fatores, além de observar o efeito desses fatores sobre a variável desfecho, há necessidade de verificar se eles não interagem entre si. Portanto, é um objetivo importante da ANOVA fatorial avaliar se há um efeito de interação estatisticamente significativo entre os fatores. 13.4.1 Dados usados nesta seção O conjunto de dados dadosMemoria.xlsx que contém informações de um teste de memória realizado em homens e mulheres, após o consumo de álcool, categorizado em três grupos (nenhum, 3 latas e 6 latas de cerveja tipo pilsen com 4,5% de álcool). O grupo sem consumo de álcool (cerveja sem álcool) serve como controle. Após o consumo de álcool, foi avaliada a memória para a realização de uma tarefa cognitiva. Neste exemplo, modificado de Andy Field (118), o efeito do álcool sobre a memória do indivíduo é a variável focal, a principal preocupação. Acredita-se que o efeito de álcool depende de outro fator, sexo, que são chamados de variáveis moderadoras. Para baixar o banco de dados, clique aqui. Salve o mesmo no seu diretório de trabalho. 13.4.1.1 Leitura dos dados A leitura será feita com a função read_excel() do pacote readxl e serão atribuídos a um objeto de nome dados e verificada a sua estrutura com a função head(). dados &lt;- readxl::read_excel(&quot;Arquivos/dadosMemoria.xlsx&quot;) str(dados) ## tibble [48 × 3] (S3: tbl_df/tbl/data.frame) ## $ sexo : chr [1:48] &quot;Feminino&quot; &quot;Feminino&quot; &quot;Feminino&quot; &quot;Feminino&quot; ... ## $ alcool: chr [1:48] &quot;nenhum&quot; &quot;nenhum&quot; &quot;nenhum&quot; &quot;nenhum&quot; ... ## $ escore: num [1:48] 65 70 60 60 60 55 60 55 70 65 ... 13.4.1.2 Exploração e sumarização dos dados Na saída da função str(), verifica-se que as variáveis alcool e sexo estão como &lt;chr&gt;e o ideal é que estejam como fatores. Portanto, vamos colocar as categorias do consumo de álcool como fator e em uma ordem lógica (nenhum consumo, três latas e 6 latas). A variável sexo será apenas colocada como fator porque não tem uma ordem lógica. As demais variáveis, id (identificação) e escore(escore de memória) podem permanecer com dbl (numérica). dados$alcool &lt;- factor(dados$alcool, levels = c(&quot;nenhum&quot;, &quot;3 latas&quot;, &quot;6 latas&quot;)) dados$sexo &lt;- as.factor(dados$sexo) A sumarização dos dados será feita com as funções group_by() e summarise() do pacote dplyr para a variável escore por grupos, sexo e alcool. alpha &lt;- 0.05 resumo &lt;- dados %&gt;% dplyr::group_by(sexo, alcool) %&gt;% dplyr::summarise(n = n(), media = mean(escore, na.rm=TRUE), dp = sd(escore, na.rm=TRUE), ep = dp/sqrt(n), me = qt((1 - alpha/2),n-1)*ep, linf = media - me, lsup = media + me) resumo ## # A tibble: 6 × 9 ## # Groups: sexo [2] ## sexo alcool n media dp ep me linf lsup ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Feminino nenhum 8 60.6 4.96 1.75 4.14 56.5 64.8 ## 2 Feminino 3 latas 8 62.5 6.55 2.31 5.47 57.0 68.0 ## 3 Feminino 6 latas 8 57.5 7.07 2.5 5.91 51.6 63.4 ## 4 Masculino nenhum 8 66.9 10.3 3.65 8.64 58.2 75.5 ## 5 Masculino 3 latas 8 66.9 12.5 4.43 10.5 56.4 77.3 ## 6 Masculino 6 latas 8 35.6 10.8 3.83 9.06 26.6 44.7 Os dados estão estruturados com um desenho onde as células tem um formato 2 x 3 com os fatores sexo e alcool e 8 indivíduos em cada célula. O fator sexo tem dois níveis (feminino e masculino) e o fator alcool tem três níveis (nenhum, 3 latas e 6 latas). Observe que o desenho é balanceado, pois todas as células têm o mesmo número de indivíduos. Esta estrutura é o caso mais simples; desenhos não balanceados são mais complexos. 13.4.1.3 Visualização gráfica dos dados Para visualizar os dados, será construido um gráfico com boxplots (Figura 13.12), usando o pacote ggpubr(119), com a função ggboxplot(), que fornece algumas funções fáceis de usar para criar e personalizar gráficos prontos para publicação baseados em ‘ggplot2’. O boxplot irá plotar os dados agrupados pelas combinações dos níveis dos dois fatores. ggpubr::ggboxplot (dados, bxp.errorbar = TRUE, bxp.errorbar.width = 0.2, x = &quot;alcool&quot;, y = &quot;escore&quot;, color = &quot;black&quot;, fill = &quot;sexo&quot;, palette = &quot;bmj&quot;, ylab = &quot;Escore da Memória&quot;, xlab = &quot;&quot;, legend.title = &quot;Sexo&quot;, legend = &quot;top&quot;) + theme (text = element_text (size = 12)) Figura13.12: Efeito do álcool na memória de acordo com o sexo. Além dos boxplot, é interessante desenhar um gráfico de linhas (Figura 13.13) que plota a média (ou outro resumo) da variável escore (resposta) para combinações bidirecionais de fatores, ilustrando assim possíveis interações. Aqui, pode-se usar a função ggline(), também pertencente ao interessante pacote ggpubr. ggpubr::ggline(dados, x = &quot;alcool&quot;, y = &quot;escore&quot;, color = &quot;sexo&quot;, size = 0.7, linetype = &quot;dashed&quot;, position = position_dodge(width = 0.2), add = c(&quot;mean_ci&quot;), palette = c(&quot;red&quot;, &quot;dodgerblue4&quot;)) Figura13.13: Efeito do álcool na memória de acordo com o sexo. O gráfico sugere um possível efeito do álcool sobre a memória, bem como uma interação entre os sexos. 13.4.2 Hipóteses estatísticas Como mencionado acima, uma ANOVA de duas vias é usada para avaliar simultaneamente o efeito de duas variáveis categóricas em uma variável quantitativa contínua. Ela é chamada de ANOVA de duas vias porque compara grupos formados por duas variáveis categóricas independentes. No exemplo, o objetivo é saber se a memória depende da álcool e/ou do sexo. Em particular, estamos interessados em: medir e testar a relação entre a alcool e a memória, medir e testar a relação entre sexo e memória, e possivelmente verificar se a relação entre álcool e memória é diferente para mulheres e homens (o que é equivalente a verificar se a relação entre sexo e memória depende da álcool) As duas primeiras relações são chamadas de efeitos principais, enquanto o item 3 é conhecido como efeito de interação. Os efeitos principais testam se pelo menos um grupo é diferente de outro (durante o controle da outra variável independente). Por outro lado, o efeito de interação tem como objetivo testar se a relação entre duas variáveis difere dependendo do nível de uma terceira variável. Em outras palavras, se a variação entre a resposta e a primeira variável categórica não depender das modalidades da segunda variável categórica, então não há interação entre as duas variáveis. Se, ao contrário, houver uma modificação dessa variação, seja por um aumento no efeito da primeira variável, seja por uma diminuição, então há uma interação. Voltando ao exemplo, tem-se os seguintes testes de hipótese: Efeito principal do sexo no escore de memória: \\(H_{0}\\): o escore de memória médio é igual entre mulheres e homens. \\(H_{1}\\): o escore de memória médio é diferente entre mulheres e homens. Efeito principal do álcool no escore de memória: \\(H_{0}\\): o escore de memória médio é igual entre as categorias de ingesta de álcool. \\(H_{1}\\): o escore de memória médio é diferente entre as categorias de ingesta de. álcool Interação entre sexo e álcool: \\(H_{0}\\): não há interação entre sexo e álcool, o que significa que a relação entre álcool e memória é a mesma para mulheres e homens (da mesma forma, a relação entre sexo e memória é a mesma para todas as três categorias de ingesta de álcool). \\(H_{1}\\): há interação entre sexo e álcool, o que significa que a relação entre álcool e memória é diferente para mulheres e homens (da mesma forma, a relação entre sexo e memória depende da ingesta de álcool). 13.4.3 Pressupostos do modelo Para usar uma ANOVA de duas vias, os dados devem atender a certos pressupostos. A ANOVA de duas vias faz todas as suposições usuais de um teste paramétrico de diferença: Independência de observações As variáveis respostas não devem ser dependentes umas das outras (ou seja, uma não deve causar a outra). Isso é impossível de testar com variáveis categóricas - só pode ser garantido por um bom projeto experimental. Além disso, a variável dependente deve representar observações únicas - não devem ser agrupadas em locais ou indivíduos. Se esta premissa for violada, você pode incluir uma variável de bloqueio e/ou usar uma ANOVA de medidas repetidas. Normalidade Variável desfecho normalmente distribuída em todos os grupos. Ausência de valores atípicos (outliers) Um valor aberrante ou valor atípico, é uma observação que apresenta um grande afastamento das demais da série, \\(\\pm 1,5\\) o intervalo interquartil (IIQ) e extremo se estiver \\(\\pm 3\\) IIQ. A existência de outliers implica, tipicamente, em prejuízos à interpretação dos resultados. Homogeneidade de variância (homocedasticidade) A variação em torno da média para cada grupo sendo comparado deve ser semelhante entre todos os grupos. Se os dados não atenderem a essa suposição, é possível usar uma alternativa não paramétrica, como o teste de Kruskal-Wallis. 13.4.4 Verificação dos pressupostos nos dados brutos Existe uma discussão se os pressupostos devem ser avaliados nos dados brutos ou apenas nos resíduos. Aqui serão realizadas as duas abordagens que frequentemente resultam no mesmo resultado. 13.4.4.1 Normalidade A variável dependente (escore) deve apresentar distribuição aproximadamente normal dentro de cada grupo. Os grupos aqui serão formados pela combinação das duas variáveis independentes (sexo e alcool). A normalidade será avaliada pelo teste de Shapiro-Wilk, com a função shapiro_test() do pacote rstatix (108), separando os grupos com a função group_by() do pacote dplyr, encadeadas com o operador pipe (%&gt;%): dados %&gt;% dplyr::group_by (sexo, alcool) %&gt;% rstatix::shapiro_test (escore) ## # A tibble: 6 × 5 ## sexo alcool variable statistic p ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Feminino nenhum escore 0.872 0.156 ## 2 Feminino 3 latas escore 0.899 0.283 ## 3 Feminino 6 latas escore 0.897 0.273 ## 4 Masculino nenhum escore 0.941 0.622 ## 5 Masculino 3 latas escore 0.967 0.870 ## 6 Masculino 6 latas escore 0.951 0.720 Os resultados suportam a conclusão de não rejeição da hipótese nula de que os dados se ajustam a distribuição normal. 13.4.4.2 Pesquisa de valores atípicos A forma mais simples de verificar a presença de um valor atípico é observar o boxplot, mostrado anteriormente. Se observa a presença de valores atípicos entre as mulheres que não ingeriram álcool e nas que ingeriram 3 latas de cerveja. Agora, para confirmar esse achado, será usado a função identify_outliers (), do pacote rstatix: dados %&gt;% dplyr::group_by (sexo, alcool) %&gt;% rstatix::identify_outliers(escore) ## # A tibble: 2 × 5 ## sexo alcool escore is.outlier is.extreme ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 Feminino nenhum 70 TRUE TRUE ## 2 Feminino 3 latas 50 TRUE FALSE A saída do teste confirma a existência dos dois valores atípicos, sendo um deles extremo, entretanto como estes valores são possíveis e, relativamente, próximos da média do sexo feminino, portanto, causam pouca preocupação, principalmente porque o teste de ANOVA é bastante robusto. 13.4.4.3 Verificação da homogeneidade das variâncias Para verificar a homocedasticidade, como os dados têm distribuição normal, é possível usar o teste de Levene, o leveneTest() do pacote car (107). car::leveneTest (escore ~ sexo*alcool, data = dados, center = mean) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 5 1.5268 0.2021 ## 42 13.4.5 Verificação dos pressupostos nos resíduos O modelo da ANOVA pode ser considerado como um modelo de regressão. Desta forma, este modelo de regressão vai usar os dados brutos para criar um modelo de previsão para esses dados. Este modelo de regressão não é perfeito, existe uma diferença entre os valores previstos e os valores observados, são os resíduos. Faz sentido, então, preocupar-se com os resíduos quando se analisa fatores tentando explicar uma variável dependente contínua, como na ANOVA, pensando em uma regressão linear simples. A ANOVA prevê que todos os valores do grupo sejam iguais a média do grupo. Ou seja, um homem que ingere 3 latas de cerveja tem um valor de seu escore de memória igual ao deste grupo. Por este motivo, fazer a análise dos resíduos é praticamente o mesmo que a análise dos valores brutos. Para analisar os resíduos (diferença entre os valores observados e o previsto pelo modelo), em primeiro lugar se constrói o modelo da ANOVA com efeito da interação, usando a função lm() do pacote stats, incluído no R base: mod.int.lm &lt;- lm(formula = escore ~ alcool * sexo, data = dados) Ao se executar o comando, tem-se a impressão que nada ocorreu, entretanto foi criado o modelo da ANOVA com uma série de variáveis, entre elas os resíduos (residuals). Para observar os resíduos, basta digitar: mod.int.lm$residuals ## 1 2 3 4 5 6 7 8 9 10 ## 4.375 9.375 -0.625 -0.625 -0.625 -5.625 -0.625 -5.625 7.500 2.500 ## 11 12 13 14 15 16 17 18 19 20 ## -2.500 7.500 2.500 -2.500 -2.500 -12.500 -2.500 7.500 12.500 -2.500 ## 21 22 23 24 25 26 27 28 29 30 ## -2.500 2.500 -7.500 -7.500 -16.875 -11.875 13.125 -1.875 3.125 8.125 ## 31 32 33 34 35 36 37 38 39 40 ## 8.125 -1.875 -21.875 -6.875 18.125 -1.875 3.125 3.125 13.125 -6.875 ## 41 42 43 44 45 46 47 48 ## -5.625 -5.625 -5.625 19.375 -0.625 -15.625 9.375 4.375 Função summary() fornece um resumo estatístico dos resíduos: summary(mod.int.lm$residuals) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -21.875 -5.625 -0.625 0.000 5.156 19.375 13.4.5.1 Avaliação da normalidade dos resíduos Uma das suposições de uma ANOVA é que os resíduos são normalmente distribuídos. A normalidade dos resíduos, inicialmente, será verificada, usando o teste de Shapiro-Wilk com a função shapiro.test(), também pertencente ao pacote stats. shapiro_test (mod.int.lm$residuals) ## # A tibble: 1 × 3 ## variable statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mod.int.lm$residuals 0.982 0.664 O teste entrega um valor P &gt; 0.05, indicando que não é possível rejeitar \\(H_{0}\\) de normalidade dos resíduos. Uma outra maneira comum de verificar essa suposição é criando um gráfico Q-Q. Se os resíduos forem normalmente distribuídos, os pontos em um gráfico Q-Q ficarão em uma linha diagonal reta. Este gráfico (Figura 13.14) pode ser contruído com a função ggqqplot() do pacote ggpubr. ggpubr::ggqqplot(mod.int.lm$residuals, conf.int = TRUE, shape = 19, xlab = &quot;Quantis teóricos&quot;, ylab = &quot;Resíduos&quot;, color = &quot;dodgerblue4&quot;) Figura13.14: Normalidade dos resíduos - QQ plot. O gráfico QQ de normalidade, mostra que os resíduos seguem aproximadamente uma linha reta, permitindo assumir a normalidade dos mesmos. 13.4.5.2 Pesquisa de valores atípicos nos resíduos Para a verificação da presença de valores atípicos entre os resíduos, cria-se uma variável que será denominada de residuos (observe o banco de dados com a função str() para ver o acréscimo dessa variável): dados$residuos &lt;- mod.int.lm$residuals str (dados) ## tibble [48 × 4] (S3: tbl_df/tbl/data.frame) ## $ sexo : Factor w/ 2 levels &quot;Feminino&quot;,&quot;Masculino&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ alcool : Factor w/ 3 levels &quot;nenhum&quot;,&quot;3 latas&quot;,..: 1 1 1 1 1 1 1 1 2 2 ... ## $ escore : num [1:48] 65 70 60 60 60 55 60 55 70 65 ... ## $ residuos: Named num [1:48] 4.375 9.375 -0.625 -0.625 -0.625 ... ## ..- attr(*, &quot;names&quot;)= chr [1:48] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... Para identificar os outliers, usa-se função identify_outliers() do pacote rstatix: dados %&gt;% dplyr::group_by(sexo, alcool) %&gt;% rstatix::identify_outliers(residuos) ## # A tibble: 2 × 6 ## sexo alcool escore residuos is.outlier is.extreme ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 Feminino nenhum 70 9.38 TRUE TRUE ## 2 Feminino 3 latas 50 -12.5 TRUE FALSE Observando os resultados com os dados brutos, verifica-se que eles são iguais aos atuais, confirmando, que neste caso, tanto faz avaliar os dados brutos como os resíduos. 13.4.5.3 Verificação da homogeneidade da variância nos resíduos A verificação da homogeneidade da variância entre os resíduos pode ser feita com o teste de Levene, como feito com os dados brutos. car::leveneTest (residuos ~ sexo*alcool, data = dados, center = mean) ## Levene&#39;s Test for Homogeneity of Variance (center = mean) ## Df F value Pr(&gt;F) ## group 5 1.5268 0.2021 ## 42 Uma outra maneira de avaliar a homogeneidade da variância, é construir um gráfico diagnóstico46 (Figura 13.15) do modelo com a função plot(), tipo 1, resíduos versus ajustes (Residuals vs Fitted). plot(mod.int.lm, 1) Figura13.15: Resíduos versus ajuste Não há correlações óbvias entre resíduos e valores ajustados (a média de cada grupo) no gráfico abaixo,onde a linha vermelha tracejada (Figura 13.15) segue praticamente uma linha horizontal em torno de 0, o que é bom. Como resultado, pode-se, assim como no teste de Levene, assumir que as variâncias são homogêneas. Verica-se o mesmo ocorrido com a normalidade, os resultados nos resíduos não diferem daqueles realizados com os dados brutos. 13.4.6 Realização do teste de ANOVA de dois fatores Mostrou-se que praticamente todos os pressupostos foram atendidos, portanto, agora pode-se prosseguir com a implementação da ANOVA de duas vias. A inclusão de um efeito de interação em uma ANOVA de duas vias não é obrigatória. Entretanto, para evitar conclusões errôneas, recomenda-se verificar primeiro se a interação é significativa ou não e, dependendo dos resultados, incluí-la ou não. Se a interação não for significativa, é seguro removê-la do modelo final. Por outro lado, se a interação for significativa, ela deverá ser incluída no modelo final que será usado para interpretar os resultados. Portanto, deve-se começar com um modelo que inclui os dois efeitos principais (ou seja, sexo e alcool) e a interação: mod.aov &lt;- aov(formula = escore ~ alcool * sexo, data = dados) summary (mod.aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## alcool 2 3332 1666.1 20.065 7.65e-07 *** ## sexo 1 169 168.7 2.032 0.161 ## alcool:sexo 2 1978 989.1 11.911 7.99e-05 *** ## Residuals 42 3488 83.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Semelhante a uma ANOVA de uma via, o princípio de uma ANOVA de duas vias baseia-se na dispersão total dos dados e em sua decomposição em quatro componentes: a parcela atribuível ao primeiro fator a parcela atribuível ao segundo fator a parcela atribuível à interação dos dois fatores a parte não explicada ou residual. A soma dos quadrados (coluna Sum Sq) mostra esses quatro componentes. A ANOVA de duas vias consiste em usar um teste estatístico para determinar se cada componente de dispersão (atribuível aos dois fatores estudados e à interação deles) é significativamente maior do que o componente residual. Se esse for o caso, concluímos que o efeito considerado (fator A, fator B ou a interação) é significativo. Vê-se que a variável alcool explica uma grande parte da variabilidade da memória. Ela é o fator mais importante para explicar essa variabilidade. Os valore P são exibidos na última coluna do resultado acima (Pr(&gt;F)). A partir desses valores conclui-se que, no nível de significância de 5%: controlando para o alcool, o escore de memória não é significativamente diferente entre os dois sexos (P = 0,161), controlando para o sexo, o escore memória é significativamente diferente (P &lt; 0,0001) para pelo menos uma categoria de ingesta de álcool, e a interação entre sexo e álcool (exibida na linha alcool:sexo no resultado) é significativa (P &lt; 0,0001). Portanto, com base no efeito de interação significativo, se observa que relação entre os escores de memória e álcool é diferente entre os sexos. Como ela é significativa, deve-se mantê-la no modelo e interpretar os resultados desse modelo. Se, ao contrário, a interação não for significativa (ou seja, se o valor P &gt; 0,05), esse efeito de interação do modelo seria removido. Abaixo, segue o código de uma ANOVA de dois fatores sem interação, chamada de modelo aditivo: mod.aov2 &lt;- aov(formula = escore ~ alcool * sexo, data = dados) Na Seção 13.4.5, foi construído um modelo para analisar os resíduos. Este modelo está baseado na semelhança dos modelos de regressão linear (veja Seção 15.3) com o modelo da ANOVA. Observa-se que o código é bem semelhante, usando a fórmula variável dependente ~ variável independentes, o sinal + é usado para incluir variáveis independentes sem interação e o sinal * quando há interação. Ou seja, a ANOVA, como todas as ANOVAs, é na verdade um modelo linear. Observe que o código a seguir, usando a função lm() e após Anova() do pacote car, também funciona e retorna os mesmos resultados 47 : mod.int.lm &lt;- lm(formula = escore ~ sexo * alcool, data = dados) Anova(mod.int.lm) ## Anova Table (Type II tests) ## ## Response: escore ## Sum Sq Df F value Pr(&gt;F) ## sexo 168.7 1 2.0323 0.1614 ## alcool 3332.3 2 20.0654 7.649e-07 *** ## sexo:alcool 1978.1 2 11.9113 7.987e-05 *** ## Residuals 3487.5 42 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Observe que a função aov() pressupõe um projeto balanceado, o que significa tamanhos de amostra iguais dentro dos níveis das variáveis de agrupamento independentes. Para verificar se os dados estão balanceados, pode-se usar a seguinte função: with(dados, table(sexo, alcool)) ## alcool ## sexo nenhum 3 latas 6 latas ## Feminino 8 8 8 ## Masculino 8 8 8 Os resultados mostram o mesmo número de indivíduos em todas as células, portanto, não importa qual o tipo de ANOVA a ser usado. Os resultados serão iguais. Além disso, aov() usa as somas de quadrados do tipo I. Para delineamentos não balanceados, ou seja, números desiguais de indivíduos em cada subgrupo, os métodos recomendados são: a ANOVA do tipo II, quando não há interação significativa, que pode ser feita no R com Anova(mod, type = “II”) ou Anova(mod, type = 2), em que mod é o nome do seu modelo salvo, e a ANOVA do tipo III, quando há uma interação significativa, que pode ser feita no R com Anova(mod, type = “III”) ou Anova(mod, type = 3). Fundamentalmente, a diferença entre um método e outro é como o R calcula a soma dos quadrados ao calcular a ANOVA. Quando os dados são balanceados, os três tipos dão o mesmo resultado 48. 13.4.7 Testes post hoc Neste estágio, chegou-se ao ponto em que se constatou que o efeito principal do sexo não é significativo e que o efeito principal do álcool é significativo. Além disso, mais importante, existe uma interação entre o álcool e o sexo, o efeito do álcool depende do sexo. Não é possível saber exatamente qual categoria da variável alcool é diferente da outra em termos de escore de memória. Para saber isso, há que comparar cada categoria duas a duas graças aos testes post-hoc, também conhecidos como comparações entre pares (121) (122) (123). Há vários testes post-hoc, sendo os mais comuns o Tukey HSD, que testa todos os pares possíveis de grupos. Será utilizada a função TukeyHSD(), usando como argumento o modelo com interação, mod.aov: TukeyHSD(mod.aov) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = escore ~ alcool * sexo, data = dados) ## ## $alcool ## diff lwr upr p adj ## 3 latas-nenhum 0.9375 -6.889643 8.764643 0.9544456 ## 6 latas-nenhum -17.1875 -25.014643 -9.360357 0.0000105 ## 6 latas-3 latas -18.1250 -25.952143 -10.297857 0.0000040 ## ## $sexo ## diff lwr upr p adj ## Masculino-Feminino -3.75 -9.058607 1.558607 0.1613818 ## ## $`alcool:sexo` ## diff lwr upr p adj ## 3 latas:Feminino-nenhum:Feminino 1.875 -11.726381 15.476381 0.9983764 ## 6 latas:Feminino-nenhum:Feminino -3.125 -16.726381 10.476381 0.9825753 ## nenhum:Masculino-nenhum:Feminino 6.250 -7.351381 19.851381 0.7432243 ## 3 latas:Masculino-nenhum:Feminino 6.250 -7.351381 19.851381 0.7432243 ## 6 latas:Masculino-nenhum:Feminino -25.000 -38.601381 -11.398619 0.0000306 ## 6 latas:Feminino-3 latas:Feminino -5.000 -18.601381 8.601381 0.8796489 ## nenhum:Masculino-3 latas:Feminino 4.375 -9.226381 17.976381 0.9277939 ## 3 latas:Masculino-3 latas:Feminino 4.375 -9.226381 17.976381 0.9277939 ## 6 latas:Masculino-3 latas:Feminino -26.875 -40.476381 -13.273619 0.0000080 ## nenhum:Masculino-6 latas:Feminino 9.375 -4.226381 22.976381 0.3286654 ## 3 latas:Masculino-6 latas:Feminino 9.375 -4.226381 22.976381 0.3286654 ## 6 latas:Masculino-6 latas:Feminino -21.875 -35.476381 -8.273619 0.0002776 ## 3 latas:Masculino-nenhum:Masculino 0.000 -13.601381 13.601381 1.0000000 ## 6 latas:Masculino-nenhum:Masculino -31.250 -44.851381 -17.648619 0.0000003 ## 6 latas:Masculino-3 latas:Masculino -31.250 -44.851381 -17.648619 0.0000003 Quando se tem muitos grupos para fazer a comparação, fica mais fácil interpretar, usando gráficos (Figura 13.16): # Definir as margens do eixo para que os rótulos não sejam cortados par(mar = c(4.1, 13.5, 4.1, 2.1)) # Criar intervalo de confiança para cada comparação plot(TukeyHSD(mod.aov, which = &quot;alcool:sexo&quot;), las = 1) Figura13.16: Resíduos versus ajuste # Retorna as margens ao padrão par(mar=c(5.1, 4.1, 4.1, 2.1)) As Saída exibe resultados onde aparece que o consumo de álcool não afetou a memória das mulheres, mas o consumo de 6 latas de cerveja diminuiu o escore de memória dos homens quando comparados com homens que consumiram cerveja sem álcool ou que ingeriram apenas 3 latas de cerveja. Em outras palavras, os termos de interação dizem como o efeito dos álcool muda quando quem o ingere é do sexo masculino ou feminino. O efeito do álcool na memória das mulheres é pequeno, ficando praticamente estável nas três condições. Por outro lado, os homens permanecem estáveis no seu escore de memória quando quantidades pequenas de álcool são ingeridas, declina rapidamente quando ingerem 6 latas de cerveja. 13.4.8 Relatando os resultados de uma ANOVA de dois fatores Pode-se relatar os resultados da ANOVA de dois fatores da seguinte maneira: Uma ANOVA de dois fatores foi realizada para avaliar se a memória de homens e mulheres era afetada pelo consumo do álcool avliado em três níveis: Não consumiram álcool Consumiram 3 latas de cerveja (~ 1L) Consumiram 6 latas de cerveja (~ 2L) Os dados são apresentados como média e desvio padrão, na Tabela 13.2. Tabela13.2: Efeito do Álcool sobre a Memória - Escore médio (desvio padrão) Sexo Sem_alcool Um_litro Dois_litros Feminino 60,6 (5,0) 62,5 (6,6) 57,5 (7,1) Masculino 66,9 (10,3) 66,9 (12,5) 35,6 (10,8) Valor P 0,145 0,396 0,0003 * Um litro de cerveja (4,5%) = 5 unidades de alcool O efeito principal do sexo na memória foi não significativo (F(1,42) = 2,03, P = 0,1614). Houve um efeito principal significativo de acordo com a quantidade de álcool consumida na memória dos participantes (F(2,42) = 20,07, P &lt;0,0001). As análises posteriores (teste de Tukey, Figura 13.16) revelaram que a memória não foi afetada nas mulheres pelo consumo de álcool, mas o consumo de 6 latas de cerveja afetou a memória dos homens quando comparados os homens que não consumiram álcool ou que consumiram até 3 latas de cerveja. Visualização dos resultados: Serão apresentados gráficos de barra de erro (Figura 13.17), com ggbarplot(), do pacote ggpubr, utilizando, para cores tonalidades de cinza. Para adicionar teste estatístico, usou-se a função get_test_label() e para o teste post hoc, a função get_pwc_label(), ambas do pacote rstatix. # Construção de um gráfico de barra de erro be &lt;- ggpubr::ggbarplot(dados, x = &quot;alcool&quot;, y = &quot;escore&quot;, add = &quot;mean_ci&quot;, error.plot = &quot;upper_errorbar&quot;, fill = &quot;sexo&quot;, palette = c(&quot;gray60&quot;, &quot;gray40&quot;), position = position_dodge(0.8)) + theme(legend.key.size = unit(0.3, &#39;cm&#39;)) + theme(legend.position = &quot;right&quot;) # Comparações por pares (pairwise comparisons) pwc &lt;- dados %&gt;% dplyr::group_by(alcool) %&gt;% rstatix::tukey_hsd(formula = escore ~ sexo) # Calcular e adicionar as posições x e y. pwc &lt;- pwc %&gt;% add_xy_position(fun = &quot;mean_ci&quot;, x = &quot;alcool&quot;, dodge = 0.8) # Cálculo do teste estatístico com pacote rstatix anova &lt;- anova_test(mod.aov) # Acrescentar o teste e o valor P ajustado ao gráfico be + stat_pvalue_manual(pwc, label = &quot;p.adj&quot;, tip.length = 0.01, y.position = 85) + labs (x = &quot;Ingestão de álcool&quot;, y = &quot;Média escore de memória&quot;, subtitle = rstatix::get_test_label (anova, detailed = TRUE), caption = rstatix::get_pwc_label(pwc)) Figura13.17: Efeito do álcool na memória de acordo com o sexo. Uma opção, é apresentar os resultados como um gráfico de linhas (Figura 13.18), já mostrado anteriormente , usando a função ggline() do pacote ggpubr: # Construção de um gráfico linha gl &lt;- ggpubr::ggline(dados, x = &quot;alcool&quot;, y = &quot;escore&quot;, add = &quot;mean_ci&quot;, color = &quot;sexo&quot;, size = 0.7, linetype = &quot;dashed&quot;, palette = &quot;lancet&quot;, position = position_dodge(0.2)) + theme(legend.key.size = unit(0.3, &#39;cm&#39;)) + theme(legend.position = &quot;right&quot;) # Comparações por pares (pairwise comparisons) pwc &lt;- dados %&gt;% dplyr::group_by(alcool) %&gt;% rstatix::tukey_hsd(formula = escore ~ sexo) # Calcular e adicionar as posições x e y. pwc &lt;- pwc %&gt;% add_xy_position(fun = &quot;mean_ci&quot;, x = &quot;alcool&quot;, dodge = 0.8) # Cálculo do teste estatístico com pacote rstatix anova &lt;- anova_test(mod.aov) # Acrescentar o teste e o valor P ajustado ao gráfico gl + stat_pvalue_manual(pwc, label = &quot;p.adj&quot;, tip.length = 0.01, y.position = 85) + labs (x = &quot;Ingestão de álcool&quot;, y = &quot;Média escore de memória&quot;, subtitle = rstatix::get_test_label (anova, detailed = TRUE), caption = rstatix::get_pwc_label(pwc)) Figura13.18: Efeito do álcool na memória de acordo com o sexo. Pode ser usada também para comparar a média de duas populações e o resultado será o mesmo de um teste t para amostras independentes.↩︎ No texto, df1 e df2 (em inglês, df = degree of freedom) foram traduzidos para o português como gl1 e gl2 (gl = graus de liberdade).↩︎ Volte à Seção 6.6 para mais informações sobre o como fazer gráficos no ggplot2.↩︎ Outros gráficos diagnósticos podem ser obtidos para analisar resíduos em um modelo de regressão (120)↩︎ A função Anova() do pacote car, usada para testar efeitos principais e de interação em modelos lineares gerais, não deve ser confundida com a função anova(), da base do R, porque esta fornece resultados sequenciais que dependem da ordem em que as variáveis aparecem no modelo.↩︎ Para os interessados, pode-se obter maiores informações sobre os diferentes tipos de ANOVA em https://www.r-bloggers.com/2011/03/anova-%E2%80%93-type-iiiiii-ss-explained/↩︎ "],["anova-de-medidas-repetidas.html", "Capítulo 14 ANOVA de medidas repetidas 14.1 Pacotes necessários neste capítulo 14.2 ANOVA de medidas repetidas de um fator 14.3 ANOVA de medidas repetidas de dois fatores", " Capítulo 14 ANOVA de medidas repetidas 14.1 Pacotes necessários neste capítulo Instalar e carregar os seguintes pacotes: pacman::p_load(dplyr, ggplot2, ggpubr, ggsci, readxl, rstatix, tidyr) 14.2 ANOVA de medidas repetidas de um fator 14.2.1 Dados usados nesta seção Os dados usados como exemplo contém os escores de autoestima de 10 indivíduos em três pontos de tempo durante uma dieta específica para determinar se esta interfere na sua autoestima. A autoestima foi determinada por uma escala (124), (125), cujos resultados variam de 0 a 30 pontos. Valores entre 15 e 25 caracterizam uma autoestima muito boa; abaixo de 15 é considerada baixa autoestima. Os dados podem ser obtidos aqui. Baixe no seu diretório de trabalho e carregue com a função read_excel() do pacote readxl: dados &lt;- read_excel(&quot;Arquivos/dadosAutoestima.xlsx&quot;) 14.2.1.1 Exploração e transformação dos dados Incialmente, observa-se como os dados foram registrados, usando a função head(): head(dados) ## # A tibble: 6 × 4 ## id t1 t2 t3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 12.0 15.5 21.3 ## 2 2 7.67 20.7 18.9 ## 3 3 9.73 13.3 29.3 ## 4 4 10.3 14.1 25.0 ## 5 5 8.61 11.7 19.4 ## 6 6 6.14 16.0 20.0 Os dados se encontram no formato amplo e para realizar a ANOVA de medidas repetidas, o R necessita que os dados estejam no formato longo. Para fazer esta transformação será usada a função pivot_longer()49 do pacote tidyr (62). Nesta função, no argumento data, coloca-se o nome do conjunto de dados; em cols, há necessidade de nomear a coluna a ser criada que receberá as colunas do formato amplo que serão reunidas. No argumento names_to, nomear a coluna que receberá os valores e em values_to, especificar o nome da variável no formato longo que conterá os valores. A variável id e a nova variável tempo devem ser convertida para fatores e o novo conjunto de dados será atribuído a um objeto nomeado dadosL: dadosL &lt;- dados %&gt;% tidyr::pivot_longer(cols = c(t1, t2, t3), names_to = &quot;tempo&quot;, values_to = &quot;escores&quot;) %&gt;% convert_as_factor(id, tempo) head(dadosL) ## # A tibble: 6 × 3 ## id tempo escores ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 t1 12.0 ## 2 1 t2 15.5 ## 3 1 t3 21.3 ## 4 2 t1 7.67 ## 5 2 t2 20.7 ## 6 2 t3 18.9 14.2.1.2 Sumarização dos dados Calcular algumas estatísticas resumidas dos escores de autoestima por grupos (tempo): média e desvio padrão, usando a funções group_by() e summarise() do dplyr: dadosL %&gt;% dplyr::group_by(tempo) %&gt;% dplyr::summarise(n = n(), média = mean(escores, na.rm =TRUE), dp = sd(escores, na.rm = TRUE)) ## # A tibble: 3 × 4 ## tempo n média dp ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 t1 10 9.42 1.66 ## 2 t2 10 14.8 2.59 ## 3 t3 10 22.9 3.43 14.2.2 Visualização dos dados A visualização pode ser obtida com um conjunto de boxplots (Figura 14.1), acrescido de barras de erro. Estes gráficos permitem visualizar a variação dos escores com o tempo. ggpubr::ggboxplot (dadosL, bxp.errorbar = TRUE, bxp.errorbar.width = 0.1, x = &quot;tempo&quot;, y = &quot;escores&quot;, color = &quot;black&quot;, fill = &quot;lightblue&quot;, ylab = &quot;Escore de Autoestima&quot;, xlab = &quot;Tempo&quot;, ggtheme = theme_bw(), legend = &quot;none&quot;) + scale_fill_grey(start=0.95, end=0.6) + theme (text = element_text (size = 12)) Figura14.1: Boxplots mostrando o impacto de uma dieta nos escores de autoestima. Outra maneira de visualizar os dados é através de um gráfico de linha (Figura 14.2) que mostra bem o comportamento dos escores com o tempo: ggpubr::ggline(dadosL, x = &quot;tempo&quot;, y = &quot;escores&quot;, color = &quot;gray60&quot;, add.params = list(color = &quot;red&quot;), size = 1, linetype = &quot;dashed&quot;, add = c(&quot;mean_ci&quot;), point.size = 2, point.color = &quot;red&quot;, ggtheme = theme_bw()) + ylab(&quot;Escore de Autoestima&quot;) + xlab(&quot;Tempo&quot;) Figura14.2: Gráfico de linha mostrando o impacto de uma dieta nos escores de autoestima. 14.2.3 Avaliação dos pressupostos A ANOVA de medidas repetidas faz as seguintes suposições sobre os dados: A amostra foi selecionada aleatoriamente da população; A variável dependente é normalmente distribuída na população para cada nível do fator dentro dos sujeitos; Não deve e existir outliers extremos; Existência de esfericidade 14.2.3.1 Identificação de valores atípicos Não deve haver valores atípicos em nenhuma célula do delineamento Isso pode ser verificado visualizando os dados nos boxplots, mostrados anteriormente, onde se observa a presença de dois outliers, um no t1 e outro em t2. Além disso, pode-se verificar a presença de valores atípicos, usando a função identify_outliers() do pacote rstatix. dadosL %&gt;% dplyr::group_by(tempo) %&gt;% rstatix::identify_outliers(escores) ## # A tibble: 2 × 5 ## tempo id escores is.outlier is.extreme ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 t1 6 6.14 TRUE FALSE ## 2 t2 2 20.7 TRUE FALSE A saída confirma a presença de dois valores atípicos, em t1 e em t2. Entretanto, eles não são extremos, não estão afastados acima de 3 intervalos interquatis e , provavelmente, não trarão problemas, apesar da amostra ser pequena. 14.2.3.2 Avaliação da normalidade Para testar a hipótese de normalidade dos dados, será utilizado o teste de Shapiro-Wilk através da função shapiro_test (), do pacote rstatix e a função group_by (), incluída no pacote dplyr ou rstatix, junto com o operador pipe (%&gt;%): dadosL %&gt;% dplyr::group_by (tempo) %&gt;% rstatix::shapiro_test(escores) ## # A tibble: 3 × 4 ## tempo variable statistic p ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 t1 escores 0.967 0.859 ## 2 t2 escores 0.876 0.117 ## 3 t3 escores 0.923 0.380 A saída exibe valores P acima de 0,05, significando que os escores de autoestima estão normalmente distribuídos em cada momento do tempo. A normalidade pode também ser avaliada com um gráfico QQ (Figura 14.3) para cada um dos momentos, usando a função ggqqplot () do pacote ggpubr, consulte a vinheta do pacote para maiores detalhes. Foi utilizado também o argumento faced.by, que divide em painéis, organizando-os como uma grade, de acordo com o momento ( t1, t2 e t3). ggpubr::ggqqplot(dadosL, x = &quot;escores&quot;, facet.by = &quot;tempo&quot;, color = &quot;tempo&quot;, palette = get_palette(&quot;Dark2&quot;, 3), legend = &quot;none&quot;) Figura14.3: Gráfico QQ para verificar a normalidade Observando o gráfico, como quase todos os pontos caem aproximadamente ao longo da linha de referência, pode-se assumir a normalidade dos escores em todos os momentos do tempo. 14.2.3.3 Esfericidade A violação da suposição de esfericidade pode distorcer os cálculos de variância resultantes de um teste ANOVA de medidas repetidas mais liberal (ou seja, um aumento na taxa de erro Tipo I). Nesse caso, a ANOVA de medidas repetidas deve ser corrigida apropriadamente dependendo do grau em que a esfericidade foi violada. Na relação entre os escores, há necessidade de pressupor que exista esfericidade (\\(\\epsilon\\) - épsilon), também chamada de circularidade, grosseiramente semelhante à homocedasticidade da ANOVA de uma via. A ANOVA de medidas repetidas pressupõe que as variâncias das diferenças entre todas as combinações de condições relacionadas (ou níveis de grupo) são iguais. A melhor maneira de verificá-la é calcular as diferenças entre os pares de escores em todas as combinações dos níveis de tratamento. O teste de esfericidade de Mauchly é usado para avaliar se a suposição de esfericidade é atendida ou não. Isso é relatado automaticamente ao usar a função anova_test () do pacote rstatix. Se o teste resulta em um valor P menor do que 0,05, pode-se concluir de que há uma diferença significativa entre as variâncias das diferenças. O principal problema da violação da condição de esfericidade é a ocorrência de testes F não exatos e liberais, com consequente perda do poder do teste. Existem várias correções que podem ser aplicadas para produzir uma razão F válida, através do ajuste dos graus de liberdade. As correções mais frequentemente preconizadas são o \\(\\epsilon\\) de Greenhouse-Geisser (GGe) e o \\(\\epsilon\\) de Huynh-Feldt (HFe). Huynh e Feldt (126) relataram que quando a correção \\(\\epsilon\\) de Greenhouse-Geisser é &gt; 0,75 muitas hipóteses nulas falsas deixam de ser rejeitadas, isto é, o teste é muito conservador, propondo outra correção dos graus de liberdade. É recomendado o uso da correção de Greenhouse-Geisser para o ajuste dos graus de liberdade quando \\(\\epsilon\\) &lt; 0,75 ou nada se sabe a respeito da esfericidade (127). Avaliando o poder destes testes, Muller (128) verificou que a correção de Greenhouse-Geisser fornece um controle adicional do erro Tipo I, enquanto o poder é maximizado. 14.2.4 Cálculo da estatística do teste Como mencionado, será usada a função anova_test(), do pacote rstatix, para o cálculo da ANOVA de medidas repetidas, criando um modelo que será atribuído ao objeto mod.anova: mod.anova &lt;- rstatix::anova_test(data = dadosL, dv = escores, wid = id, within = tempo) mod.anova ## ANOVA Table (type III tests) ## ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 1 tempo 2 18 55.463 2.02e-08 * 0.829 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 1 tempo 0.551 0.092 ## ## $`Sphericity Corrections` ## Effect GGe DF[GG] p[GG] p[GG]&lt;.05 HFe DF[HF] p[HF] ## 1 tempo 0.69 1.38, 12.42 2.16e-06 * 0.774 1.55, 13.94 6.04e-07 ## p[HF]&lt;.05 ## 1 * Em primeiro lugar, observar o Teste de Mauchly para a esfericidade. Verifica-se que o efeito do tempo tem um valor P = 0,092, ou seja, &gt; 0,05 e, portanto, não houve violação da esfericidade e não há necessidade de observar as correções do \\(\\epsilon\\) de Greenhouse-Geisser (GGe) ou o \\(\\epsilon\\) de Huynh-Feldt (HFe). Desta forma, pode-se dizer que houve uma modificação significativa no escore de de autoestima, à medida que o tempo passou (F (2,18) = 55,5, P &lt; 0.0001, \\(\\eta^2 = 0,83\\)). Usando a função get_anova_table() do pacote rstatix para extrair a tabela ANOVA, a correção de esfericidade Greenhouse-Geisser é aplicada automaticamente aos fatores que violam a suposição de esfericidade. rstatix::get_anova_table(mod.anova) ## ANOVA Table (type III tests) ## ## Effect DFn DFd F p p&lt;.05 ges ## 1 tempo 2 18 55.463 2.02e-08 * 0.829 Onde, F indica que se está comparando com uma distribuição F (teste F); (2, 18) indica os graus de liberdade no numerador (DFn) e no denominador (DFd), respectivamente; 55,5 indica o valor da estatística F. p especifica o valor P. ges é o tamanho do efeito generalizado (quantidade de variabilidade devido ao fator dentro dos assuntos), eta ao quadrado \\(\\eta^2\\). 14.2.5 Testes post hoc É possível fazer comparações por pares. realizando vários testes t pareados entre os níveis do dentro do fator (tempo), usando a função pairwise_t_test(), incluída no R base. Os valores P são ajustados usando o método de correção de testes múltiplos de Bonferroni. pwc &lt;- dadosL %&gt;% pairwise_t_test( escores ~ tempo, paired = TRUE, p.adjust.method = &quot;bonferroni&quot; ) pwc ## # A tibble: 3 × 10 ## .y. group1 group2 n1 n2 statistic df p p.adj p.adj.signif ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 escores t1 t2 10 10 -4.97 9 7.73e-4 2e-3 ** ## 2 escores t1 t3 10 10 -13.2 9 3.34e-7 1e-6 **** ## 3 escores t2 t3 10 10 -4.87 9 8.87e-4 3e-3 ** Todas as diferenças pareadas são estatisticamente significativas. 14.2.6 Relatando os resultados da ANOVA de medidas repetidas unifatorial Pode-se relatar de forma simples: Os escores de autoestima se modificaram de forma significativa de acordo com a passagem do tempo, F(2, 18) = 55,5, p &lt; 0,0001, \\(\\eta^2\\) = 0,82. Análises post hoc, com um ajuste de Bonferroni, revelaram que todas as diferenças pareadas, entre os pontos de tempo, foram estatisticamente diferentes (P &lt; 0,05). Uma opção de apresentação gráfica, é o gráfico de linhas (Figura 14.4), usando a função ggline() do pacote ggpubr, junto com os teste estatísticos: gl &lt;- ggpubr::ggline(dadosL, x = &quot;tempo&quot;, y = &quot;escores&quot;, color = &quot;gray60&quot;, add.params = list(color = &quot;black&quot;), size = 0.7, linetype = &quot;dashed&quot;, add = &quot;mean_ci&quot;, point.size = 1, point.color = &quot;black&quot;) gl + ggpubr::stat_pvalue_manual(pwc, label = &quot;p.adj&quot;, tip.length = 0.00, y.position = c(23, 29, 26)) + labs (x = &quot;Tempo&quot;, y = &quot;Escore de autoestima&quot;, subtitle = get_test_label (mod.anova, detailed = TRUE), caption = get_pwc_label(pwc)) Figura14.4: Impacto de uma dieta específica na autoestima. 14.3 ANOVA de medidas repetidas de dois fatores 14.3.1 Dados usados nesta seção O conjunto de dados de dadosAutoestima2.xlsx contém as medidas dos escores de autoestima de 12 indivíduos inscritos em 2 ensaios clínicos sucessivos de curto prazo (4 semanas): placebo e dieta especial. Os dados podem ser obtidos aqui. Baixe no seu diretório de trabalho. Cada participante participou dos dois ensaios. A ordem das tentativas foi equilibrada e foi permitido tempo suficiente entre os ensaios para permitir que quaisquer efeitos dos ensaios anteriores se dissipassem (washout).O escore de autoestima foi registrado em três momentos: no início (t1), no meio (t2) e no final (t3) dos ensaios. A questão é investigar se esse tratamento dietético de curto prazo pode induzir um aumento significativo do escore de autoestima ao longo do tempo. Em outras palavras, se quer saber se há interação significativa entre dieta e tempo no escore de autoestima. A ANOVA de medidas repetidas pode ser realizada para determinar se existe uma interação significativa entre dieta e tempo no escore de autoestima. 14.3.1.1 Leitura dos dados A leitura dos dados será feita com a função read_excel() do pacote readxl. Após a leitura, será exibida 3 linhas aleatórias por grupo de tratamento, usando a função sample_n_by() do pacote rstatix:: autoestima &lt;- readxl::read_excel(&quot;Arquivos/dadosAutoestima2.xlsx&quot;) set.seed(123) autoestima %&gt;% sample_n_by(tratamento, size = 3) ## # A tibble: 6 × 5 ## id tratamento t1 t2 t3 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3 dieta 27.3 27.3 27.6 ## 2 12 dieta 24 24 23.4 ## 3 10 dieta 27 27.9 28.5 ## 4 2 placebo 29.1 28.5 26.4 ## 5 6 placebo 21.6 19.5 18.9 ## 6 5 placebo 23.1 21.9 20.4 14.3.1.2 Transformação dos dados Os dados autoestima estão no formato amplo e as colunas t1, t2 e t3 devem ser reunidas, em uma única variável denominada tempo, transformando o formato amplo em longo, usando a função pivot_longer() do pacote tidyr. A seguir, converter em fator esta nova variável tempo e a variável identificadora id: autoestimaL &lt;- autoestima %&gt;% tidyr::pivot_longer(cols = c(t1, t2, t3), names_to = &quot;tempo&quot;, values_to = &quot;escores&quot;) %&gt;% convert_as_factor(id, tratamento, tempo) Explorar o novo conjunto de dados no formato longo: autoestimaL %&gt;% sample_n_by(tratamento, tempo, size = 1) ## # A tibble: 6 × 4 ## id tratamento tempo escores ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 4 dieta t1 27.3 ## 2 6 dieta t2 22.5 ## 3 9 dieta t3 27.9 ## 4 10 placebo t1 27.6 ## 5 11 placebo t2 27.6 ## 6 5 placebo t3 20.4 Nesse exemplo, o efeito do “tempo” no escore de autoestima é nossa variável focal, nossa principal preocupação. No entanto, pensa-se que o efeito “tempo” será diferente se o tratamento for realizado ou não. Nesse cenário, a variável “tratamento” é considerada como variável moderadora. 14.3.1.3 Sumarização dos dados Os dados serão por tratamento e tempo e, em seguida, serão calculadas algumas estatísticas resumidas da variável de escore: média e sd (desvio padrão). autoestimaL %&gt;% dplyr::group_by(tratamento, tempo) %&gt;% rstatix::get_summary_stats(escores, type = &quot;mean_sd&quot;) ## # A tibble: 6 × 6 ## tratamento tempo variable n mean sd ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dieta t1 escores 12 26.3 2.29 ## 2 dieta t2 escores 12 26.4 2.23 ## 3 dieta t3 escores 12 26.3 2.44 ## 4 placebo t1 escores 12 26.4 2.42 ## 5 placebo t2 escores 12 25.2 3.07 ## 6 placebo t3 escores 12 23.6 3.16 14.3.1.4 Visualização dos dados Serão criados boxplots (Figura 14.5) do escore coloridos pelos grupos de tratamento, com cores da paleta do NEJM, usando o pacote ggsci e a função scale_fill_nejm(): ggpubr::ggboxplot (autoestimaL, bxp.errorbar = TRUE, bxp.errorbar.width = 0.1, x = &quot;tempo&quot;, y = &quot;escores&quot;, color = &quot;black&quot;, fill = &quot;tratamento&quot;, ylab = &quot;Escore de Autoestima&quot;, xlab = &quot;Tempo&quot;, ggtheme = theme_bw())+ scale_fill_nejm() + theme (text = element_text (size = 12)) Figura14.5: Impacto de uma dieta específica na autoestima. 14.3.2 Avaliação dos pressupostos Os pressupostos da ANOVA de medidas repetidas de dois fatores são os mesmos da de um fator (Seção 14.2.3). 14.3.2.1 Identificação dos outliers A observação dos boxplots mostra que não existem valores atípicos. Estes outliers serão analisados, usando função identify_outliers() do pacote rstatix. autoestimaL %&gt;% dplyr::group_by(tratamento, tempo) %&gt;% rstatix::identify_outliers(escores) ## [1] tratamento tempo id escores is.outlier is.extreme ## &lt;0 linhas&gt; (ou row.names de comprimento 0) A saída mostra a ausência de valores atípicos, com já se havia observado nos boxplots (Figura 14.5). 14.3.2.2 Avaliação da normalidade Para testar a hipótese de normalidade dos dados, será utilizado o teste de Shapiro-Wilk através da função shapiro_test (), do pacote rstatix e a função group_by (), incluída no pacote dplyr ou rstatix, junto com o operador pipe (%&gt;%): autoestimaL %&gt;% dplyr::group_by (tratamento, tempo) %&gt;% rstatix::shapiro_test(escores) ## # A tibble: 6 × 5 ## tratamento tempo variable statistic p ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dieta t1 escores 0.919 0.279 ## 2 dieta t2 escores 0.923 0.316 ## 3 dieta t3 escores 0.886 0.104 ## 4 placebo t1 escores 0.828 0.0200 ## 5 placebo t2 escores 0.868 0.0618 ## 6 placebo t3 escores 0.887 0.107 A saída do teste mostra que os escores de autoestima estão normalmente distribuídos em cada momento do tempo, havendo uma exceção: o grupo placebo, no momento t1. Fato que pode ser constatado pela assimetria do boxplot nesse momento do tempo (Figura 14.5. É possível construir um gráfico QQ (Figura 14.6) para cada um dos momentos, usando a função ggqqplot () do pacote ggpubr, consulte a vinheta do pacote para maiores detalhes. Foi utilizado também a função facet_grid(), do ggplot2, que divide em painéis, organizando-os como uma grade, de acordo com o momento ( t1, t2 e t3). ggpubr::ggqqplot(data = autoestimaL, x = &quot;escores&quot;, color = &quot;tempo&quot;, palette = get_palette(&quot;Dark2&quot;, 3), legend = &quot;none&quot;, ggtheme = theme_bw() ) + facet_grid(tempo~tratamento) Figura14.6: Gráfico QQ para verificar a normalidade Observando o gráfico, como quase todos os pontos caem aproximadamente ao longo da linha de referência, pode-se seguir a análise, pois não há muito problema. 14.3.2.3 Esferecidade A esferecidade será avaliada junto com a construção do modelo. 14.3.3 Cálculo da estatística do teste É realizado da mesma maneira do que a ANOVA de medidas repetidas de uma via com a função anova_test() do pacote rstatix: mod.anova2 &lt;- rstatix::anova_test(data = autoestimaL, dv = escores, wid = id, within = tempo, between = tratamento, type = 3) ## Warning: The &#39;wid&#39; column contains duplicate ids across between-subjects ## variables. Automatic unique id will be created mod.anova2 ## ANOVA Table (type III tests) ## ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 1 tratamento 1 22 1.432 2.44e-01 0.059 ## 2 tempo 2 44 28.417 1.19e-08 * 0.049 ## 3 tratamento:tempo 2 44 29.259 8.29e-09 * 0.050 ## ## $`Mauchly&#39;s Test for Sphericity` ## Effect W p p&lt;.05 ## 1 tempo 0.582 0.003 * ## 2 tratamento:tempo 0.582 0.003 * ## ## $`Sphericity Corrections` ## Effect GGe DF[GG] p[GG] p[GG]&lt;.05 HFe DF[HF] ## 1 tempo 0.705 1.41, 31.03 1.04e-06 * 0.739 1.48, 32.52 ## 2 tratamento:tempo 0.705 1.41, 31.03 8.00e-07 * 0.739 1.48, 32.52 ## p[HF] p[HF]&lt;.05 ## 1 6.19e-07 * ## 2 4.72e-07 * Analisando o teste de Mauchly para a esfericidade, verifica-se que houve violação da esfericidade, tanto para o efeito do tempo (W = 0,582, P = 0,003) como para a interação tratamento:tempo (W = 0,582, P = 0,003). Esses resultados indicam a necessidade de correção para ajustar os graus de liberdade. A correção de Greenhouse-Geisser é utilizada, pois é o recomendado quando W &lt; 0,75 (veja Seção 14.2.2.3). O modelo ANOVA, gerado pela função anova_test(), entregou a tabela da ANOVA sem correção, o teste de Mauchly e a correção dos graus de liberdade da esfericidade (Greenhouse-Geisser (GGe) e Huynh-Feldt (HFe)). A tabela da ANOVA corrigida pode, então, ser obtida, usando a função get_anova_table (), do pacote rstatix, com o mod.anova2 e correction = “GG” como argumentos: rstatix::get_anova_table(mod.anova2, correction = &quot;GG&quot;) ## ANOVA Table (type III tests) ## ## Effect DFn DFd F p p&lt;.05 ges ## 1 tratamento 1.00 22.00 1.432 2.44e-01 0.059 ## 2 tempo 1.41 31.03 28.417 1.04e-06 * 0.049 ## 3 tratamento:tempo 1.41 31.03 29.259 8.00e-07 * 0.050 Como existe uma interação estatisticamente significativa (F(1,41, 31,26) = 29,6, P &lt; 0,0001) entre tratamento e o tempo, deve-se prestar atenção a este fato. Os efeitos principais isolados perdem a importância. Para esta análise são necessários testes posteriores. 14.3.4 Teste post hoc Uma interação significativa entre os dois fatores indica que o impacto que um fator (tratamento) tem na variável desfecho (escore de autoestima) depende do nível do outro fator (tempo), e vice-versa. Assim, é possível decompor a interação entre os dois fatores significativos em: Efeito principal simples: executar o modelo unifatorial da primeira variável (tratamento) em cada nível da segunda variável (tempo), Comparações simples pareadas: se o efeito principal simples for significativo, executar várias comparações pareadas para determinar quais grupos são diferentes. Para uma interação não significativa entre os dois fatores, há necessidade de determinar se tem algum efeito principal estatisticamente significativo da saída ANOVA. 14.3.4.1 Procedimento para uma interação significativa entre os dois fatores Efeito do tratamento No exemplo, será analisado o efeito do tratamento no escore de auto-estima em cada momento no tempo. Note que como o tratamento tem apenas dois níveis (dieta e placebo), o teste de ANOVA e teste t pareado fornecem os mesmos resultados. Efeito do tratamento em cada ponto de tempo tratamento &lt;- autoestimaL %&gt;% dplyr::group_by(tempo) %&gt;% rstatix::anova_test(dv = escores, wid = id, within = tratamento) %&gt;% rstatix::get_anova_table() %&gt;% rstatix::adjust_pvalue(method = &quot;bonferroni&quot;) tratamento ## # A tibble: 3 × 9 ## tempo Effect DFn DFd F p `p&lt;.05` ges p.adj ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 t1 tratamento 1 11 0.376 0.552 &quot;&quot; 0.000767 1 ## 2 t2 tratamento 1 11 9.03 0.012 &quot;*&quot; 0.052 0.036 ## 3 t3 tratamento 1 11 30.9 0.00017 &quot;*&quot; 0.199 0.00051 Comparações pareadas entre os grupos de tratamentos pwc &lt;- autoestimaL %&gt;% dplyr::group_by(tempo) %&gt;% pairwise_t_test(escores ~ tratamento, paired = TRUE, p.adjust.method = &quot;bonferroni&quot;) pwc ## # A tibble: 3 × 11 ## tempo .y. group1 group2 n1 n2 statistic df p p.adj ## * &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 t1 escores dieta placebo 12 12 -0.613 11 0.552 0.552 ## 2 t2 escores dieta placebo 12 12 3.00 11 0.012 0.012 ## 3 t3 escores dieta placebo 12 12 5.56 11 0.00017 0.00017 ## # ℹ 1 more variable: p.adj.signif &lt;chr&gt; Considerando o valor P ajustado de Bonferroni (p.adj), pode-se observar que o efeito principal simples do tratamento não foi significativo no ponto de tempo t1 (P = 1). Torna-se significativo em t2 (p = 0,036) e t3 (p = 0,00051). Comparações pareadas mostram que o escore médio de autoestima foi significativamente diferente entre o grupo placebo e dieta em t2 (P = 0,12) e t3 (P = 0,00017), mas não em t1 (P = 0,55). Efeito do tempo Observe que também é possível realizar a mesma análise para a variável tempo em cada nível de tratamento. Esta análise, necessariamente, não precisa ser feita! Efeito do tempo em cada nível de tratamento tempo &lt;- autoestimaL %&gt;% dplyr::group_by(tratamento) %&gt;% rstatix::anova_test(dv = escores, wid = id, within = tempo) %&gt;% rstatix::get_anova_table() %&gt;% adjust_pvalue(method = &quot;bonferroni&quot;) tempo ## # A tibble: 2 × 9 ## tratamento Effect DFn DFd F p `p&lt;.05` ges p.adj ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dieta tempo 2 22 0.078 0.925 &quot;&quot; 0.000197 1 ## 2 placebo tempo 2 22 39.7 0.00000005 &quot;*&quot; 0.145 0.0000001 Comparações pareadas entre pontos no tempo pwc2 &lt;- autoestimaL %&gt;% dplyr::group_by(tratamento) %&gt;% pairwise_t_test(escores ~ tempo, paired = TRUE, p.adjust.method = &quot;bonferroni&quot;) pwc2 ## # A tibble: 6 × 11 ## tratamento .y. group1 group2 n1 n2 statistic df p p.adj ## * &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dieta escores t1 t2 12 12 -0.522 11 0.612 1 e+0 ## 2 dieta escores t1 t3 12 12 -0.102 11 0.921 1 e+0 ## 3 dieta escores t2 t3 12 12 0.283 11 0.782 1 e+0 ## 4 placebo escores t1 t2 12 12 4.53 11 0.000858 3 e-3 ## 5 placebo escores t1 t3 12 12 6.91 11 0.0000255 7.65e-5 ## 6 placebo escores t2 t3 12 12 6.49 11 0.0000449 1.35e-4 ## # ℹ 1 more variable: p.adj.signif &lt;chr&gt; Após a execução do código, verifica-se que o efeito do tempo é significativo apenas para o placebo, F(2, 22) = 39,7, p &lt; 0,0001. As comparações pareadas mostram que todas as comparações entre os pontos de tempo foram estatisticamente significativas para o placebo. 14.3.4.2 Procedimento para uma interação não significativa entre os dois fatores Se a interação não for significativa, é preciso interpretar os efeitos principais para cada uma das duas variáveis: tratamento” e tempo. Um efeito principal significativo pode ser acompanhado com comparações pareadas No exemplo, (consulte a tabela ANOVA em mod.anova2), não houve efeitos principal estatisticamente significativo para o tratamento (F(1, 22) = 1,43, P = 0,244) no escore de autoestima. Entretanto, os escores foram afetados significativamente pelo tempo (F(1,41, 31,03) = 28,41, P &lt; 0,0001). 14.3.5 Relatando os resultados da ANOVA de medidas repetidas de dois fatores O resultado pode ser relatado da seguinte forma: Uma ANOVA de medidas repetidas de dois fatores foi realizada para avaliar o efeito de diferentes tratamentos dietéticos ao longo do tempo no escore de autoestima. Houve uma interação estatisticamente significativa entre tratamento e tempo no escore de autoestima, F(1,41, 31,03) = 29,26, P &lt; 0,0001. Por isso, o efeito da variável tratamento foi analisado em cada ponto de tempo. Os valores de P foram ajustados usando o método de correção de testes múltiplos de Bonferroni. O efeito da variável tratamento foi significativo em t2 (P = 0,036) e t3 (P = 0,00051), mas não no ponto de tempo t1 (P = 1). Comparações pareadas, usando o teste t pareado, mostram que o escore médio de autoestima foi significativamente diferente entre os ensaios placebo e dieta nos pontos de tempo t2 (P = 0,012) e t3 (P = 0,00017), mas não em t1 (P = 0,55). 14.3.5.1 Visualização dos resultados Os resultados podem ser visualizados através de gráficos tipo boxplots (Figura 14.7) ou por um gráfico de linha (Figura 14.8). Os gráficos foram construídos , usando as funções ggboxplot() e ggline() do pacote ggpubr. Os boxplots receberam um preenchimento em tons de cinza, bem interessante em publicações científicas; o gráfico de linha, usando o pacote ggsci, recebeu as cores da paleta do periódico Lancet. Boxplots bxp &lt;- ggpubr::ggboxplot (autoestimaL, bxp.errorbar = TRUE, bxp.errorbar.width = 0.1, x = &quot;tempo&quot;, y = &quot;escores&quot;, color = &quot;black&quot;, fill = &quot;tratamento&quot;, ylab = &quot;Escore de Autoestima&quot;, xlab = &quot;Tempo&quot;)+ scale_fill_grey(start=0.95, end=0.6) + theme(legend.position=&quot;right&quot;) + theme (text = element_text (size = 12)) pwc &lt;- pwc %&gt;% add_xy_position(x = &quot;tempo&quot;) bxp + stat_pvalue_manual(pwc, label = &quot;p.adj&quot;, tip.length = 0.01, hide.ns = FALSE, y.position = c(32, 32, 32)) + labs(subtitle = get_test_label(mod.anova2, detailed = TRUE), caption = get_pwc_label(pwc)) Figura14.7: Avaliação da autoestima no decorrer do tempo Gráfico de linha ggl &lt;- ggline( autoestimaL, x = &quot;tempo&quot;, y = &quot;escores&quot;, color = &quot;tratamento&quot;, palette = &quot;lancet&quot;, linetype = &quot;dashed&quot;, size = 0.7, shape = 19, add = &quot;mean_ci&quot;, error.plot = &quot;errorbar&quot;, position = position_dodge(width = 0.2), ggtheme = theme_pubr()) + theme(legend.position=&quot;right&quot;) ggl + ggpubr::stat_pvalue_manual(pwc, label = &quot;p.adj&quot;, tip.length = 0.00, y.position = c(28.5, 28.5, 28.5)) + labs (x = &quot;Tempo&quot;, y = &quot;Escore de autoestima&quot;, subtitle = get_test_label (mod.anova2, detailed = TRUE), caption = get_pwc_label(pwc)) Figura14.8: Avaliação da autoestima no decorrer do tempo Veja a Seção 12.3.1.1 para mais detalhes da função.↩︎ "],["correlação-e-regressão.html", "Capítulo 15 Correlação e Regressão 15.1 Pacotes necessários 15.2 Correlação 15.3 Regressão Linear Simples", " Capítulo 15 Correlação e Regressão 15.1 Pacotes necessários pacman::p_load(car, dplyr, ggplot2, ggpubr, ggsci, kableExtra, lmtest, readxl, rstatix) 15.2 Correlação A correlação é usada para avaliar a força e a direção da relação entre duas variáveis numéricas contínuas, normalmente distribuídas. A maneira mais comum de mostrar a relação entre duas variáveis quantitativas é através de um diagrama ou gráfico de dispersão (scatterplot). A Figura 15.1 exibe um exemplo de um gráfico de dispersão, onde se observa um padrão geral que sugere uma relação entre o estriol urinário (mg/24h) e o peso fetal em uma gravidez normal (129). Figura15.1: Correlação da excreção de estriol urinário e peso fetal O gráfico de dispersão mostra que os valores de uma variável aparecem no eixo horizontal x e os valores da outra variável aparecem no eixo vertical y. Cada indivíduo nos dados aparece como o ponto no gráfico fixado pelos valores de ambas as variáveis para aquele indivíduo. Normalmente, eixo x é a variável explicativa (ou variável explanatória ou independente) e y a variável desfecho (variável resposta ou dependente). Em um diagrama de dispersão deve-se procurar o padrão geral e desvios marcantes desse padrão. Verifica-se o padrão geral, observando a direção, a forma e força do relacionamento. Um tipo importante de desvio é um valor atípico, um valor individual que está fora do padrão geral do relacionamento. A Figura 15.1 mostra uma clara direção do padrão geral que se move da esquerda inferior para a direita superior. Este comportamento é denominado de correlação positiva entre as variáveis. A forma do relacionamento é aproximadamente uma linha reta com uma ligeira curva para a direita à medida que se move para cima. A força de uma correlação em um gráfico de dispersão é determinada pela proximidade dos pontos em uma forma clara. No caso, quanto mais se aproxima de uma reta, mais forte é a associação, no caso de uma correlação linear. Duas variáveis estão negativamente associadas quando se comportam de forma oposta ao da Figura 15.1. Obviamente, nem todos os diagramas de dispersão mostram uma direção clara que permita descrever como correlação positiva ou negativa e não tem uma forma linear, sugerindo que não há correlação, como a Figura 15.2. Figura15.2: Gráfico de dispersão sugerindo ausência de correlação. 15.2.1 Coeficiente de correlação de Pearson A correlação é quantificada pelo Coeficiente de Correlação Linear de Pearson. Este coeficiente paramétrico, denotado por r, é um número adimensional, independente das unidades usadas para medir as variáveis x e y. Suponha que se tenha dados sobre as variáveis x e y para n indivíduos. Os valores para o primeiro indivíduo são \\({x}_{1}\\) e \\({y}_{1}\\), os valores para o segundo indivíduo são \\({x}_{2}\\) e \\({y}_{2}\\) e assim por diante. As médias e desvios padrão das duas variáveis são \\(\\bar{x}\\) e \\({s}_{x}\\) para os valores de x e \\(\\bar{y}\\) e \\({s}_{y}\\) para os valores de y. A correlação r entre x e y é dada pela equação: \\[ r = \\frac{\\sum{(x_{1} - \\bar{x})(y_{1} - \\bar{y})}}{{\\sqrt{\\sum (x_{1} - \\bar{x})^2\\times\\sum (y_{1} - \\bar{y})^2}}} \\] O Coeficiente de Correlação, r, apresenta as seguintes características: É um valor numérico que varia de -1 a +1 (Figura 15.3: Quando r = -1, há uma correlação linear negativa ou inversa perfeita; Quando r = +1, há uma correlação linear positiva ou direta perfeita; Quando r = 0, não há correlação entre as variáveis. Figura15.3: Coeficiente de Correlação. Quanto mais os pontos se aproximam de uma linha reta, maior a magnitude de r. O coeficiente de correlação r é calculado para uma amostra e é uma estimativa do coeficiente de correlação da população \\(\\rho\\) (leia-se rô). A correlação não faz distinção entre variáveis explicativas e variáveis resposta. Apesar de haver uma recomendação para que x seja a variável explanatória e y a variável desfecho. Não faz diferença qual variável será chamada chama de x e qual de y no cálculo da correlação. Como r usa os valores padronizados das observações, r não muda se as unidades de medida de x, y ou ambos são modificados. A correlação r em si não tem unidade de medida; é apenas um número. 15.2.2 Dados usados nesta seção Está claro que existe uma relação entre a idade de crianças e a sua altura (comprimento). Vamos usar os dados coletados em um ambulatório pediátrico de 40 crianças entre 18 e 36 meses (20 meninos e 20 meninas). Os dados estão no banco de dadosdadosReg.xlsx. Para baixar o banco de dados, clique aqui. Salve o arquivo no seu diretório de trabalho. 15.2.2.1 Leitura e exploração dos dados Usar a função read_excel do pacote readxl para carregar o arquivo. Observar os dados com a função str(). dados &lt;- read_excel(&quot;Arquivos/dadosReg.xlsx&quot;) str(dados) ## tibble [40 × 5] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:40] 1 2 3 4 5 6 7 8 9 10 ... ## $ idade : num [1:40] 18 18 19 19 20 20 21 21 22 22 ... ## $ comp : num [1:40] 80 80 83 82 84 81 84.5 84 85 82.5 ... ## $ irmaos: num [1:40] 0 0 2 0 0 1 1 1 0 1 ... ## $ sexo : chr [1:40] &quot;masc&quot; &quot;fem&quot; &quot;masc&quot; &quot;fem&quot; ... De acordo com uma das exigências da correlação, as variáveis idade e comp pertencem a classe das variáveis numéricas. A variável sexo foi lida como um variável numérica e será transformada em fator: dados$sexo &lt;- as.factor(dados$sexo) 15.2.2.2 Medidas resumidoras Ass medidas resumidoras serão calculadas, usando a função get_summary_stats () do pacote rstatix que necessita dos seguintes argumentos: dados %&gt;% rstatix::get_summary_stats(idade, comp, type = &quot;mean_sd&quot;) ## # A tibble: 2 × 4 ## variable n mean sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 idade 40 27.0 5.41 ## 2 comp 40 90.2 6.00 15.2.2.3 Visualização dos dados Aqui, será usado o gráfico de dispersão (Figura 15.4), usando a função geom_point() do pacote ggplot2: ggplot2::ggplot(dados, aes(x=idade, y=comp, color = sexo)) + geom_point(size = 3, shape = 19) + xlab(&quot;Idade (meses)&quot;) + ylab (&quot;Comprimento(cm)&quot;) + theme_bw() + theme(text = element_text(size = 12, color = NULL, face = &quot;bold&quot;)) Figura15.4: Correlação entre a idade e a altura de uma criança. A separação dos pontos por sexo, usando cores diferentes, não muda a análise e foi realizada apenas para treinamento (e curiosidade!). 15.2.3 Pressupostos da Correlação A primeira e mais importante etapa antes de analisar os dados, usando a correlação de Pearson é verificar se é apropriado usar este teste estatístico. Serão discutidos sete pressupostos, três estão relacionados com o projeto do estudo e como as variáveis foram medidas (pressupostos 1, 2 e 3) e quatro que se relacionam com as características dos dados (pressupostos 4, 5, 6 e 7) (130). Variáveis numéricas contínuas As duas variáveis devem ser medidas em uma escala contínua (são medidas no nível intervalar ou de razão). No exemplo, tanto a variável idade como o comprimento (comp) são variáveis contínuas, como verificado acima. Variáveis devem estar como pares As duas variáveis contínuas devem ser emparelhadas, o que significa que cada caso (por exemplo, cada participante) tem dois valores: um para cada variável. Independência das observações Deve haver independência de casos, o que significa que as duas observações para um caso (por exemplo, a idade e o comprimento) devem ser independentes das duas observações para qualquer outro caso. Se estes pressupostos forem atendidos, avalia-se os outros pressupostos: Relação linear entre as variáveis O coeficiente de correlação de Pearson é uma medida da força de uma associação linear entre duas variáveis. Dito de outra forma, ele determina se há um componente linear de associação entre duas variáveis contínuas. Por esse motivo, verifica-se a relação entre duas variáveis, em um gráfico de dispersão, para ver se a execução de uma correlação de Pearson é a melhor escolha como medida de associação. A variável idade é colocada como variável preditora (eixo x) e comp como desfecho (eixo y). O gráfico de dispersão anterior, mostra uma nítida correlação linear. Normalidade das variáveis Para verificar se as variáveis têm distribuição normal, é possível usar o teste de Shapiro-Wilk, usando a função shapiro_test(), incluída no pacote rstatix: dados %&gt;% shapiro_test(idade, comp) ## # A tibble: 2 × 3 ## variable statistic p ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 comp 0.958 0.141 ## 2 idade 0.958 0.145 O teste de Shapiro-Wilk de ambas as variáveis retorna um valor P &gt; 0,05, indicando que não é possível rejeitar a \\(H_{0}\\); os dados seguem a distribuição normal, portanto o pressuposto foi atendido. Pesquisa de valores atípicos A identificação dos valores atípicos pode ser feita usando a função identify_outliers() do pacote rstatix. dados %&gt;% identify_outliers(idade) ## [1] id idade comp irmaos sexo is.outlier is.extreme ## &lt;0 linhas&gt; (ou row.names de comprimento 0) dados %&gt;% identify_outliers(comp) ## [1] id idade comp irmaos sexo is.outlier is.extreme ## &lt;0 linhas&gt; (ou row.names de comprimento 0) Homoscedasticidade A homocedasticidade assume que os dados são igualmente distribuídos sobre a linha de regressão. Descreve uma situação na qual o resíduo é o mesmo em todos os valores das variáveis independentes. A heterocedasticidade (a violação da homocedasticidade) está presente quando o tamanho dos resíduos difere entre os valores de uma variável independente. O impacto de violar o pressuposto da homocedasticidade é uma questão de grau, aumentando à medida que a heterocedasticidade aumenta. Dessa forma, avalia-se a homocedasticidade, observando os resíduos. Uma correlação linear pode ser descrita por uma reta. Em uma correlação linear perfeita, a reta passa por todos os pontos. Normalmente, não é possível traçar uma reta que passe por todos os pontos. A melhor reta é aquela que promove o melhor ajuste,ou seja, é aquela cuja distância dos pontos até a reta é a menor possível. Os resíduos são a diferença entre o valor observado e o valor previsto pelo melhor ajuste, estabelecido pelo modelo de regressao linear. Construção do modelo: Para ajustar a um modelo linear, usa-se a função lm() que deve conter um objeto da classe formula como argumento. Demais características da função podem ser obtidas com ?lm ou direto na ajuda do RStudio. O modelo será atribuído a um objeto denominado mod_reg. mod_reg &lt;- lm(comp ~ idade, dados) Análise gráfica da homoscedasticidade: Pode ser feita através dos gráficos diagnósticos para a regressão linear, utilizados para verificar se o modelo funciona bem para representar os dados. Uma forma de avaliar é verificar como as variâncias se comportam. Os gráficos diagnósticos são apresentados de quatro maneiras diferentes.50 Neste estágio, serão avaliados o primeiro e o terceiro tipo (Figura 15.5). par(mfrow=c(1,2)) plot(mod_reg, which=c(1,3)) Figura15.5: Gráficos diagnósticos 1 e 3 par(mfrow=c(1,1)) No gráfico 1, tem-se os resíduos em função dos valores estimados. Pode-se utilizar este gráfico para observar a independência e a homocedasticidade, se os resíduos se distribuem de maneira razoavelmente aleatória e com mesma amplitude em torno do zero. No gráfico 3 (valores ajustados x resíduos), tem-se os resíduos em função dos valores estimados. Pode-se utilizar este gráfico para observar a independência e a homocedasticidade, se os resíduos se distribuem de maneira razoavelmente aleatória e com mesma amplitude em torno do zero. Permite verificar se há outliers - valores de resíduos padronizados acima de 3 ou abaixo de -3. Embora o gráfico possa dar uma ideia sobre homocedasticidade, às vezes, um teste mais formal é preferido. Existem vários testes para isso, mas aqui será utilizado o Teste de Breusch-Pagan. A \\(H_{0}\\) e a \\(H_{1}\\) podem ser consideradas como: \\(H_{0}\\): Homocedasticidade. Os resíduos têm variância constante sobre o modelo verdadeiro. \\(H_{1}\\): Heterocedasticidade. Os resíduos não têm variância constante sobre o modelo verdadeiro. Se o valor P &gt; 0,05 não se rejeita a \\(H_{0}\\) de homocedasticidade. O teste de Breusch-Pagan é encontrado na função bptest(), incluída no pacote lmtest: lmtest::bptest(mod_reg) ## ## studentized Breusch-Pagan test ## ## data: mod_reg ## BP = 0.049988, df = 1, p-value = 0.8231 Os resultados não indicam heteroscedasticidade e isso é bom. Desta forma, pode-se aplicar a equação final de predição. 15.2.4 Execução do teste de correlação 15.2.4.1 Coeficiente de correlação de Pearson (r) O coeficiente de correlação, r, é calculado para uma amostra e é uma estimativa do coeficiente de correlação da população \\(\\rho\\) (rô). Como visto, no início deste capítulo, a correlação não faz distinção entre variáveis explicativas e variáveis resposta. Apesar de haver uma recomendação para que x seja a variável explanatória e y a variável desfecho. Não faz diferença qual variável será chamada chama de x e qual de y no cálculo da correlação. Como o r usa os valores padronizados das observações, não muda nada se as unidades de medida de x, y ou ambas são modificadas. A correlação r em si não tem unidade de medida; é apenas um número. O cálculo pode ser realizado com a função cor_test() do pacote rstatix que usa os seguintes argumentos: data \\(\\to\\) dataframe contendo as variáveis; … \\(\\to\\) Uma ou mais expressões (ou nomes de variáveis) sem aspas separadas por vírgulas. Usado para selecionar uma variável de interesse. Alternativa ao argumento vars. Ignorado quando vars é especificado; vars2 \\(\\to\\) • vetor de caracteres opcional. Se especificado, cada elemento em vars será testado em relação a todos os elementos em vars2. Aceita nomes de variáveis sem aspas: c(var1, var2); alternative \\(\\to\\) hipótese alternativa “two.sided” (bilateral) ou “greater” ou “less” (unilateral a direita ou a esquerda, respectivamente); method \\(\\to\\) ⟶ qual coeficiente de correlação deve ser usado para o teste. Um dos termos “pearson”, “kendall” ou “spearman” pode ser abreviado; conf.level \\(\\to\\) nivel de confiança. Padrão 0.95. r &lt;- dados %&gt;% cor_test(idade, comp, method = &quot;pearson&quot;) r ## # A tibble: 1 × 8 ## var1 var2 cor statistic p conf.low conf.high method ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 idade comp 0.96 21.4 7.87e-23 0.927 0.979 Pearson A saída do Coeficiente de Correlação de Pearson (r) é igual 0.96 (IC95%: 0.93, 0.98) o que corresponde a uma correlação linear muito forte (Tabela 15.1) entre a idade e o comprimento de crianças (131). O coeficiente refere a existência de correlação linear, mas não especifica se a relação é de causa e efeito. O valor P especifica se a correlação é igual a zero (\\(H_{0}\\)) ou diferente de zero (\\(H_{1}\\)). No caso, ela é diferente de zero. O importante é a magnitude do r, entretanto, o coeficiente r e o valor P devem ser interpretados em conjunto. Se o valor P &gt; 0,05, mesmo que r seja diferente de zero, a correlação não deveria ser interpretada. Tabela15.1: Interpretação do Coeficiente de Correlação Coeficiente de Correlação Interpretação 0,0 &lt; 0,3 desprezível 0,3 &lt; 0,5 fraca 0,5 &lt; 0,7 moderada 0,7 &lt; 0,9 forte 0,9 &lt; 1,0 muito forte 1,0 perfeita Talvez a melhor maneira de interpretar a correlação linear é elevar o valor do r ao quadrado para obter o Coeficiente de Determinação (\\(R^{2}\\)). No exemplo usado, tem-se que o \\(R^{2}\\) é igual a \\(0,96^{2} = 0,922\\), então, 92,2% da variação do comprimento da criança (y) podem ser explicados pela variação da sua idade (x), fato mais ou menos óbvio!. 15.2.4.2 Coeficiente de correlação de Spearman (\\(\\rho\\)) Se os pressupostos são violados é recomendado o uso de correlação não paramétrica, incluindo testes de correlação baseados em postos de Spearman e Kendall (132). Usar a mesma função, usada para a correlação de Pearson, mudando o argumento method: rho &lt;- dados %&gt;% cor_test(idade, comp, method = &quot;spearman&quot;) rho ## # A tibble: 1 × 6 ## var1 var2 cor statistic p method ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 idade comp 0.96 448. 3.30e-22 Spearman 15.2.4.3 Coeficiente de correlação de Kendall (\\(\\tau\\)) O coeficiente de correlação de postos de Kendall ou estatística tau de Kendall é usado para estimar uma medida de associação baseada em postos. Pode ser usado com variáveis ordinais ou quando não existe relação linear entre as variáveis. Uma vantagem sobre o coeficiente de Spearman é a possibilidade de ser generalizado para um coeficiente de correlação parcial. Deve ser usada ao invés do coeficiente de Spearman quando temos um conjunto pequeno de dados com um grande número de postos empatados. Para o cálculo desse coeficiente, continua-se com a mesma função anterior, mudando o method = “kendall”. tau &lt;- dados %&gt;% cor_test(idade, comp, method = &quot;kendall&quot;) tau ## # A tibble: 1 × 6 ## var1 var2 cor statistic p method ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 idade comp 0.85 7.57 3.64e-14 Kendall No caso normal, a correlação de Kendall é mais robusta e eficiente que a correlação de Spearman. Isso significa que a correlação de Kendall é preferida quando há amostras pequenas ou alguns valores atípicos. O rho de Spearman geralmente é maior que o tau de Kendall. 15.3 Regressão Linear Simples A regressão linear simples, assim como a correlação, é uma técnica usada para explorar a natureza da relação entre duas variáveis aleatórias contínuas. A principal diferença entre esses dois métodos analíticos é que a regressão permite investigar a alteração em uma variável, chamada resposta, correspondente a uma determinada alteração em outra, conhecida como variável explicativa. A regressão é um modelo matemático que permite a predição de uma variável resposta a partir de uma outra variável explicativa. A análise de correlação quantifica a força da relação entre as variáveis, tratando-as simetricamente (133). A regressão linear simples é chamada assim, porque se tem apenas uma variável independente. Se houver mais de uma variável independente, é chamada de regressão múltipla. A representação matemática do modelo de regressão linear populacional é descrita pela equação da reta de melhor ajuste em um conjunto de pares de dados (x, y) em um gráfico de dispersão de pontos. \\[ y = \\beta_{0} + \\beta_{1}x \\] Figura15.6: Reta de regressão, coeficiente angular e coeficiente linear. A inclinação da reta de regressão (\\(\\beta_{1}\\)) determina a variação de y para cada unidade de variação de x e recebe o nome de coeficiente angular ou de regressão. O ponto de interceptação da reta com y quando x é igual a zero é \\(\\beta_{0}\\) e é denominado de coeficiente linear (Figura 15.6). A equação da reta de regressão amostral que estima a reta de regressão populacional é igual a: \\[ \\hat {y} = b_{0} + b_{1}x \\] A reta do diagrama de dispersão da Figura 15.6 é a melhor reta de ajuste aos dados. 15.3.1 Resíduos No exemplo usado no início desta seção, verificou-se que existe uma correlação linear entre o a idade e o comprimento de crianças, usando uma amostra de 40 crianças entre 18 e 36 meses. A correlação de Pearson foi muito forte (r = 0,96, P &lt; 0,00001). Esta relação linear pode ser descrita pela reta, mostrada na Figura 15.7. ggplot2::ggplot(dados, aes(x = idade, y = comp, color = &quot;tomato&quot;)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;steelblue&quot;) + geom_point() + theme_classic() + xlab(&quot;Idade (meses&quot;) + ylab(&quot;Comprimento (cm)&quot;) + theme(text = element_text(size = 12)) + theme(legend.position = &quot;none&quot;) Figura15.7: Reta de regressão Não é possível traçar uma reta que passe por todos os pontos. Esta reta ideal descreveria uma correlação perfeita, que não é o caso. Pode haver várias retas, a reta calculada pela regressão linear é aquela que promove o melhor ajuste, ou seja, é aquela cuja distância dos pontos até a reta é a menor possível. Os resíduos são a diferença entre o valor observado e o valor previsto pelo modelo de regressão linear, construído anteriormente (mod_reg). A técnica estatística para achar a melhor reta que ajusta um conjunto de dados é denominada de método dos mínimos quadrados (Ordinary Least Square). A melhor reta ajustada é aquela em que a soma dos quadrados da distância de cada ponto (soma dos quadrados residual) em relação à reta é minimizada. Para se obter os resíduos, graficamente, pode ser usar os seguintes comandos que resultam na Figura 15.8. # Obter e salvar os valores preditos e residuais dados$previsto &lt;- predict(mod_reg) dados$residuos &lt;- residuals(mod_reg) # Construção do gráfico com os resíduos ggplot2::ggplot(dados, aes(x = idade, y = comp)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;steelblue&quot;) + geom_segment(aes(xend = idade, yend = previsto), linewidth = 0.7, linetype = &quot;dotted&quot;) + geom_point(aes(y = previsto), shape = 19, colour = &quot;red&quot;) + geom_point() + theme_classic() + xlab(&quot;Idade (meses&quot;) + ylab(&quot;Comprimento (cm)&quot;) + theme(text = element_text(size = 12)) Figura15.8: Resíduos Uma boa maneira de testar a qualidade do ajuste do modelo é observar os resíduos (134) ou as diferenças entre os valores reais (pontos pretos) e os valores previstos (pontos vermelhos). A reta de regressão, em azul no gráfico, representa os valores previstos. A linha vertical pontilhada da linha reta até o valor dos dados observados é o resíduo. A ideia aqui é que a soma dos resíduos seja aproximadamente zero ou o mais baixo possível. Na vida real, a maioria dos casos não seguirá uma linha perfeitamente reta, portanto, resíduos são esperados. Na saída do resumo da função lm() em (mod_reg$residuals), você pode ver estatísticas descritivas sobre os resíduos do modelo (residuals), elas mostram como os resíduos são aproximadamente zero. Pode-se observar isso, usando a função summary () e sum(): summary(mod_reg$residuals) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -3.20326 -1.20326 0.08994 0.00000 1.25849 3.49221 sum(mod_reg$residuals) ## [1] -3.538836e-15 Como se observa, a soma dos residuos é praticamente iguais a zero (\\(-3,54 \\times 10^-15\\). 15.3.2 Análise dos pressupostos do modelo de regressão A análise exploratória do conjunto de dados foi feita quando do estudo da Correlação. Assim como a correlação, a regressão linear faz várias suposições sobre os dados. 15.3.2.1 Gráficos diagnósticos Os gráficos de diagnóstico da regressão (Figura 15.9) podem ser criados usando a função plot() do R base, como mostrado para a correlação. O modelo de regressão, anteriormente criado, mod_reg, entra como argumento da função. A função par(mfrow = 2, 2) foi utilizada, como de outras vezes, para colocar os gráficos em duas linhas e duas colunas: O modelo de regressão, anteriormente criado, mod_reg, entra como argumento da função: par(mfrow=c(2,2)) plot (mod_reg) Figura15.9: Gráficos diagnósticos par(mfrow=c(1,1)) Os gráficos de diagnóstico mostram resíduos de quatro maneiras diferentes: Resíduos vs. ajustados (Residuals vs Fitted). Usado para verificar os pressupostos de relação linear. Uma linha horizontal, sem padrões distintos é um indicativo de uma relação linear, o que é bom. Os dados do exemplo (linha azul) afastam-se muito pouco do zero, mas a acompanham e não se observa nenhum padrão distinto, como uma parábola por exemplo. Q-Q plot. Usado para examinar se os resíduos são normalmente distribuídos. É bom se os pontos residuais seguirem a linha reta tracejada. É possível dizer que os resíduos seguem a linha diagonal, com pequenos desvios toleráveis. Localização da dispersão (scale-location). Usado para verificar a homogeneidade de variância dos resíduos (homocedasticidade). Uma linha horizontal com pontos igualmente dispersos é uma boa indicação de homocedasticidade. No exemplo usado, os resíduos parecem estar dispersos e a linha azul não está próxima do zero, sugerindo um problema com a homocedasticidade, entretanto, não está acima de 3. Resíduos vs. alavancagem (leverage). Usado para identificar casos influentes, ou seja, valores extremos que podem influenciar os resultados da regressão quando incluídos ou excluídos da análise. Nem todos os outliers são influentes na análise de regressão linear. Mesmo que os dados tenham valores extremos, eles podem não ser influentes para determinar uma linha de regressão. Isso significa que os resultados não seriam muito diferentes, incluindo ou não esses valores. Por outro lado, alguns casos podem ser muito influentes, mesmo que pareçam estar dentro de uma faixa razoável de valores. Outra forma de colocar, é que eles não se entendem com a tendência na maioria dos casos. Ao contrário dos outros gráficos, desta vez os padrões não são relevantes. Deve-se estar atento aos valores distantes no canto superior direito ou no canto inferior direito. Esses pontos são os lugares onde os casos podem ter influência contra uma linha de regressão. Procurar casos fora de uma linha tracejada, distância de Cook. Quando os casos estão fora da distância de Cook (o que significa que têm pontuações altas de distância de Cook), os casos são influentes para os resultados da regressão. Os resultados da regressão serão alterados se excluirmos esses casos. A aparência dos gráficos do exemplo mostra que não há nenhum caso influente. Pouco se observa as linhas de distância de Cook (uma linha tracejada) porque todos os casos estão bem dentro das linhas de distância de Cook. 15.3.2.2 Avaliação da normalidade dos resíduos Ao analisar os pressupostos da correlação, foi realizado a avaliação da normalidade nos dados brutos que indicaram não ser possível rejeitar a hipótese nula de que os dados têm distribuição normal. Agora, isto será repetido para avaliar a normalidade dos resíduos, usando o mesmo teste, teste de Shapiro-Wilk. Ao ser criado o modelo de regressão (mod_reg), ele fornece uma série de variáveis que pode ser listada da seguinte maneira: ls(mod_reg) ## [1] &quot;assign&quot; &quot;call&quot; &quot;coefficients&quot; &quot;df.residual&quot; ## [5] &quot;effects&quot; &quot;fitted.values&quot; &quot;model&quot; &quot;qr&quot; ## [9] &quot;rank&quot; &quot;residuals&quot; &quot;terms&quot; &quot;xlevels&quot; Usando a variável residuals, confirma-se o observado no QQPlot de que os resíduos apresentam distribuição normal, pois o valor de P &gt; 0,05. shapiro_test(mod_reg$residuals) ## # A tibble: 1 × 3 ## variable statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mod_reg$residuals 0.979 0.655 A saída retorna a estatística do teste de Shapiro-Wilk com um valor P = 0,655, mostrando que os dados se ajustam à distribuição normal. Pode-se também construir um gráfico QQ (Figura 15.10), usando a função ggqqplot() do pacote ggpubr que exibe o mesmo resultado. ggpubr::ggqqplot(mod_reg$residuals, color = &quot;steelblue4&quot;, xlab = &quot;Quantis normais&quot;, ylab = &quot;Residuos&quot;, ggtheme = theme_bw()) Figura15.10: Gráfico QQ mostrando a normalidade dos resíduos do modelo de regressão linear 15.3.2.3 Pesquisa de valores atípicos nos resíduos Existe uma função pode ser usada para verificar valores atípicos nos resíduos da regressão para modelos lineares como rstandard() do pacote stats, que analisa os resíduos padronizados. A função padroniza todos os resíduos e inclui no objeto residuos_p. Para analisá-los, faz-se um sumário, usando a função summary(). Esta função exibirá os a estatística dos 5 números mais a média para os resíduos padronizados: residuos_p &lt;- rstandard(mod_reg) summary(residuos_p) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.9327846 -0.7271178 0.0548028 0.0006059 0.7779208 2.1154118 Em uma amostra normalmente distribuída, ao redor de 95% dos valores estão entre –1,96 e +1,96, 99% deve estar entre –2,58 e +2,58 e quase todos (99,9%) deve se situar entre –3,09 e +3,09. Portanto, resíduos padronizados com um valor absoluto maior que 3 são motivo de preocupação porque em uma amostra média é improvável que aconteça um valor tão alto por acaso (135). Se a saída da função rstandard() for comparada com o eixo y do gráfico Residuals vs Leverage, dos gráficos diagnósticos, verifica-se valores semelhantes que variam abaixo de 3 e acima de -3, indicando que não há outliers influenciando e a mediana está próxima de zero. 15.3.2.4 Homocedasticidade dos resíduos Na Seção 15.2.3, foi analisada a homocedasticidade , onde se viu que o teste de Breusch-Pagan, retornou um resultado de P = 0,8231, indicando que a variância permanece praticamente constante, havendo homocedasticidade nos resíduos. O problema mais sério associado à heterocedasticidade é o fato de que os erros padrão são tendenciosos. Como o erro padrão é fundamental para a realização de testes de significância e cálculo de intervalos de confiança, os erros padrão tendenciosos levam a conclusões incorretas sobre a significância dos coeficientes de regressão. No geral, no entanto, a violação da suposição de homocedasticidade deve ser bastante grave para apresentar um grande problema, dada a natureza robusta da regressão pelo método ordinary least-squares. No entanto, é importante que a equação final de predição seja aplicada apenas a populações com as mesmas características da amostra do estudo. 15.3.2.5 Independência dos resíduos Os resíduos no modelo devem ser independentes, ou seja, não devem ser correlacionados entre si. Para verificar isso, pode-se executar o teste Durbin-Watson (teste dw), utilizando a função durbinWatsonTest() do pacote ´car`. O teste retorna um valor entre 0 e 4. Um valor maior que 2 indica uma correlação negativa entre resíduos adjacentes, enquanto um valor menor que 2 indica uma correlação positiva. Se o valor for dois, é provável que exista independência. Existe uma sugestão de que valores abaixo de 1 ou mais de 3 são um motivo definitivo de preocupação (135). É importante mencionar que o teste tem como pressuposto a normalidade dos dados. durbinWatsonTest(mod_reg) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.1044054 2.204843 0.628 ## Alternative hypothesis: rho != 0 Como na saída do teste o valor P &gt; 0,05 e a estatística DW é igual a 2,2, não se rejeita a hipótese nula de independência (rho = 0). 15.3.3 Tamanho amostral na regressão O tamanho da amostra deve ser suficiente para suportar o modelo de regressão. É importante coletar dados suficientes para obter um modelo de regressão confiável. O tamanho da amostra necessário para suportar um modelo depende do valor do coeficiente de correlação do modelo (no caso da correlação linear simples é o r de Pearson) e do número de variáveis incluídas. A Tabela 15.2 (136) mostra o número de participantes necessários em modelos com 1 a 4 preditores independentes. Como se observa, o requisito de tamanho da amostra aumenta com o número de variáveis preditoras. Tabela15.2: Tamanho amostral para regressão Valor r 1 variável preditora 2 variáveis preditoras 3 variáveis preditoras 4 variáveis preditoras 0.2 190 230 265 290 0.3 80 100 115 125 0.4 45 55 65 70 Existem muitas regras práticas, sugerindo o tamanho da amostra. Uma delas, diz que se deve ter 10 a 15 casos por variável preditora no modelo. Entretanto, essas regras podem ser duvidosas e o melhor é calcular o tamanho amostral baseado no tamanho do efeito, usando, por exemplo o site StatToDo 15.3.4 Realização da regressão linear Após analisar os pressupostos do modelo de regressão do exemplo, verificou-se que as variáveis idade e comprimento da criança têm relação linear, que os resíduos do modelo têm distribuição normal, que existe homoscedasticidade e que não há pontos influentes. E, portanto, o modelo permite que se realize uma análise de regressão linear para avaliar a relação entre as variáveis independentes e dependentes. Para realizar uma análise de regressão linear simples e verificar os resultados, há necessidade de executar dois comandos. O primeiro, que cria o modelo linear já foi realizado na análise dos gráficos e será repetido aqui. O segundo, imprime o resumo do modelo com a função summary(): mod_reg &lt;- lm (comp ~ idade, dados) summary (mod_reg) ## ## Call: ## lm(formula = comp ~ idade, data = dados) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2033 -1.2033 0.0899 1.2585 3.4922 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 61.44408 1.36466 45.02 &lt;2e-16 *** ## idade 1.06515 0.04967 21.45 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.678 on 38 degrees of freedom ## Multiple R-squared: 0.9237, Adjusted R-squared: 0.9217 ## F-statistic: 459.9 on 1 and 38 DF, p-value: &lt; 2.2e-16 A saída da função summary() primeiro apresenta como o modelo foi obtido e, em seguida, resume os resíduos do modelo. Por último, tem-se os Coeficientes: As estimativas (Estimate) para os parâmetros do modelo - o valor do intercepto y (neste caso, 61,44) e o efeito estimado da idade sobre o comprimento (1,1). Isto significa que para cada unidade de aumento na idade se espera um aumento de 1,1 cm no comprimento. O erro padrão dos valores estimados (Std. Error). A estatística de teste (t value) O valor P (Pr (&gt;| t |)), também conhecido como a probabilidade de encontrar a estatística t fornecida se a hipótese nula de nenhuma correlação for verdadeira. As três linhas finais são os diagnósticos do modelo - o mais importante a observar é o valor P (\\(2,2\\times 10^{-16}\\)), que indica se o modelo se ajusta bem aos dados. A partir desses resultados, pode-se dizer que existe uma correlação positiva significativa entre idade e comprimento (valor P &lt; 0,001), com um aumento de 1,1 cm no comprimento para cada aumento de 1 mês no na idade , possibilitando a previsão comprimento da criança pela idade. Estes dados são empregados para formular a equação do modelo de regressão da seguinte maneira: \\[ \\hat {y} = 61,44 + 1,1 x \\] O erro padrão das estimativas são fornecidos. Esses dados permitem calcular o IC95%. Ou pode-se usar a função confint() do pacote stats, que será colocada dentro da função round() para arredondar os valores até um digito. round (confint (mod_reg, level = 0.95), 1) ## 2.5 % 97.5 % ## (Intercept) 58.7 64.2 ## idade 1.0 1.2 Dessa forma, é possível prever que uma criança de 30 meses, de acordo com o modelo, terá o seguinte comprimento: comp_30m &lt;- 61.4 + 1.1 *30 comp_30m ## [1] 94.4 lim.sup &lt;- 64.2 + 1.2*30 lim.inf &lt;- 58.7 + 1.0*30 print (c(lim.inf, lim.sup)) ## [1] 88.7 100.2 Ou seja, espera-se que uma criança tenha, aos 30 meses de idade, um comprimento médio de 94,4 cm (IC95%: 88,7-100,2) 15.3.5 Visualização dos resultados Será obtido um gráfico de dispersão com a reta de regressão e seu intervalo de confiança de 95% (Figura 15.11). Além disso, adicionou-se a equação do modelo de regressão (o R arredondou os valores), juntamente com o coeficiente de determinação \\(R^{2}\\). ggplot2:: ggplot (dados, aes (x = idade, y = comp)) + geom_point (size = 2) + geom_smooth (method = &quot;lm&quot;, se = TRUE, color = &quot;steelblue&quot;) + stat_regline_equation (label.y = 100, aes (label = (..eq.label..))) + stat_regline_equation (label.y = 99, aes (label = (..rr.label..))) + theme_classic () + xlab (&quot;Idade (meses)&quot;) + ylab (&quot;Comprimento(cm)&quot;) + theme (text = element_text (size = 12)) Figura15.11: Resultado da regressão linear No gráfico, o intervalo de previsão médio de 95% em torno da reta de regressão é um intervalo de confiança de 95%, ou seja, a área na qual há 95% de certeza de que a reta de regressão verdadeira se encontra (137). Esta banda de intervalo é levemente curvada porque os erros na estimativa do intercepto e da inclinação são incluídos em adição ao erro na previsão da variável desfecho. Maiores detalhes sobre os testes diagnósticos podem ser encontrados em: https://data.library.virginia.edu/diagnostic-plots/↩︎ "],["análise-de-dados-categóricos.html", "Capítulo 16 Análise de Dados Categóricos 16.1 Pacotes necessários 16.2 Qui-Quadrado 16.3 Teste exato de Fisher 16.4 Teste de Macnemar", " Capítulo 16 Análise de Dados Categóricos 16.1 Pacotes necessários pacman::p_load(coin, DescTools, dplyr, expss, ggplot2, gmodels, kableExtra, nhstplot, readxl, rstatix, summarytools) 16.2 Qui-Quadrado Dois testes de hipótese são proeminentes na pesquisa na área da saúde. Um é o teste t de duas amostras, que é usado para testar a igualdade de duas médias populacionais independentes. O segundo é o teste qui-quadrado (denotado por \\(\\chi^{2}\\)). O teste é denominado teste qui-quadrado porque usa a distribuição qui-quadrado ou \\(\\chi^{2}\\). 16.2.1 Distribuição qui-quadrado Se uma variável X é normalmente distribuída, então a variável \\(X^{2}\\) tem uma distribuição qui-quadrado (138). A distribuição qui-quadrado com k categorias é a distribuição de uma soma dos quadrados de k variáveis aleatórias independentes com distribuição normal. O número de categorias determina o número de graus de liberdade. O formato da distribuição qui-quadrado depende desses graus de liberdade. Em geral, ela é assimétrica com apenas valores positivos, iniciando em zero. A assimetria diminui à medida que aumentam os graus de liberdade. Para cada grau de liberdade tem-se curvas de distribuição diferentes. Figura16.1: Distribuição do qui-quadrado. A distribuição \\(\\chi^{2}\\) converge para a distribuição normal à medida que os graus de liberdade aumentam, de acordo com o teorema do limite central, entretanto esta convergência é lenta (Figura 16.1). A distribuição qui-quadrado tem duas aplicações comuns: primeiro, como um teste para saber se duas variáveis categóricas são independentes ou não (Teste de independência ou associação); segundo, o teste de qualidade do ajuste do qui-quadrado (Teste de aderência ou ajuste) que é usado para comparar uma determinada distribuição com uma distribuição conhecida. 16.2.2 Estatística do qui-quadrado O cálculo da estatística \\(\\chi^{2}\\) é baseado nas frequências existentes nas células da tabela de contingência. Em primeiro lugar, calcula-se as frequências que se espera em cada célula caso a hipótese nula seja verdadeira (frequências esperadas). Em segundo lugar, usando a equação geral, o teste mede o grau de discrepância entre o conjunto de frequências observadas (O) e o conjunto de frequências esperadas (E). \\[ \\chi^{2}= \\sum \\left [\\frac{\\left (O_{i} - E_{i} \\right )^2}{E_{i}} \\right] \\] Se \\(O_{i}\\) é muito semelhante ao \\(E_{i}\\), então o \\(\\chi^{2}\\) é baixo; se \\(O_{i}\\) é muito diferente em relação ao \\(E_{i}\\), então o \\(\\chi^{2}\\) é alto. As frequências observadas são o número de sujeitos ou objetos na amostra que se enquadram nas várias categorias da variável de interesse. As frequências esperadas são o número de sujeitos ou objetos na amostra que seria esperado observar se hipótese nula fosse verdadeira. \\[ E = \\frac{total\\ coluna\\ \\times total\\ linha }{total\\ geral} \\] Por exemplo, suponha a Tabela 16.1: Tabela16.1: Acidentes automobilísticos Sexo Acidentes Sem acidentes Total Homens 16 44 60 Mulheres 4 36 40 Total 20 80 100 Usando os dados da Tabela 16.1, o número de acidentes esperados para os homens será: total_c &lt;- 20 total_l &lt;- 60 total_geral &lt;- 100 esperado &lt;- (total_c*total_l)/total_geral esperado ## [1] 12 O número de acidentes esperado para os homens é igual a 12, entretanto ocorreram 16. Houve uma diferença. Esta diferença é calculada para todas as células e será o importante no cálculo do qui-quadrado. Após o cálculo, acrescentando os valores esperados à Tabela 16.1, tem-se a Tabela 16.2 que será usada no cálculo do qui-quadrado. Tabela16.2: Acidentes automobilisticos - Valores Esperados Sexo Acidentes Sem acidentes Total Homens 12 48 60 Mulheres 8 32 40 Total 20 80 100 Substituindo osdados na fórmula do qui-quadrado, tem-se: \\[ \\chi^{2}= \\left [\\frac{\\left (16 - 12 \\right )^2}{12} \\right]+ \\left [\\frac{\\left (44 - 48 \\right )^2}{48} \\right]+\\left [\\frac{\\left (4 - 8 \\right )^2}{8} \\right]+\\left [\\frac{\\left (36 - 32 \\right )^2}{32} \\right] \\\\ \\chi^{2}=1,333 + 0,333 + 2,0 +0,50 =4,17 \\] 16.2.2.1 Restrições ao qui-quadrado Regra Geral O teste pode ser usado, se a frequência observada em cada célula for maior ou igual a 5 e a frequência esperada for maior ou igual a 5. Tabela 2 \\(\\times\\) 2 (gl = 1) Neste caso, é recomendada a Correção de Continuidade de Yates, mesmo quando o n for grande. Tabela l \\(\\times\\) c O teste pode ser usado se o número de células com frequência esperada inferior a 5 for menor do que 20% do total das células e nenhuma frequência esperada é igual a zero. n pequeno Neste caso, é preconizado o Teste Exato de Fisher. 16.2.2.2 Valor crítico do qui-quadrado A estatística de teste (que em certo sentido é a diferença entre as frequências observadas e esperadas) deve ser comparada a um valor crítico para determinar se a diferença é grande ou pequena. Não se pode dizer se uma estatística de teste é grande ou pequena sem colocá-la em perspectiva com o valor crítico. Se a estatística de teste estiver acima do valor crítico, significa que a probabilidade de observar tal diferença entre as frequências observadas e esperadas é improvável. O valor crítico pode ser encontrado na tabela estatística da distribuição Qui-quadrado e depende do nível de significância, denotado \\(\\alpha\\), e dos graus de liberdade, denotado \\(gl\\). O nível de significância geralmente é igual a 5%. Os graus de liberdade para um teste de Qui-quadrado de independência são encontrados da seguinte forma: \\[ gl = (numero \\ de \\ linhas - 1) \\ \\times \\ (numero \\ de \\ colunas - 1) \\] Em uma tabela de contingência 2 \\(\\times\\) 2, como a Tabela 16.1, tem \\(gl = (2 - 1) \\times (2 - 1) = 1\\). Basta agora obter o valor crítico com a função qchisq(): alpha &lt;- 0.05 gl = 1 qchisq (1-alpha, gl) ## [1] 3.841459 Este valor é comparado com o \\(\\chi^{2}_{calculado}\\) para um nível de significância de 5%. Se ele é maior, rejeita-se se a \\(H_{0}\\); caso contrário, não se rejeita. Para obter o valor P, pode-se usar a função pchisq(), onde, como argumento, coloca-se o valor do \\(\\chi^{2}_{calculado}\\), os graus de liberdade e acrescenta-se lower.tail = FALSE para obter a probabilidade da cauda superior, uma vez que a distribuição do quiquadrado é positiva. Na Tabela 16.1, o valor crítico é igual a 3,84 e, no exemplo, o \\(\\chi^{2}_{calculado}\\) é igual a 4,17, logo a valor P é igual a: pchisq (4.17, 1, lower.tail = FALSE) ## [1] 0.0411458 Dessa forma, conclui-se ,com uma confiança de 95%, que os homens têm uma proporção maior de acidentes comparados às mulheres (\\(\\chi^{2} (1) = 4,17;P=0,041\\)). Observe na Figura 16.2 que o \\(\\chi^{2}_{calculado}\\) localiza-se a direita da linha vertical, na área vermelha de rejeição da \\(H_{0}\\). plotchisqtest(chisq = 3.84, df = 1, colorleft = &quot;aliceblue&quot;, colorright = &quot;red&quot;, ylabel = &quot;Densidade de probabilidade sob a hipótese nula&quot;) Figura16.2: Distribuição do qui-quadrado, gl = 1, alpha = 0,05. O gráfico foi criado com a função plotchisqteste() do pacote nhstplot, pacote simples e conveniente para representar graficamente os testes de significância de hipótese nula mais comuns, como testes F, testes t e testes z (139). 16.2.3 Qui-quadrado de independência ou associação 16.2.3.1 Dados usados nesta seção O exemplo da Tabela 16.1, usado para mostrar a lógica do qui-quadrado, é um teste de independência ou associação. Ali, foi mostrado que existe uma associação estatisticamente significativa (P &lt; 0,05) entre acidentes automobilísticos e o sexo masculino. Como exercício, serão usados outros dados que necessitam mais manipulação como treinamento do qui-quadrado de associação e do próprio R. Estes dados servirão para testar a hipótese de associação entre tabagismo e baixo peso ao nascer (&lt; 2500g). Leitura e transformação dos dados Os dados serão provenientes do banco de dados dadosMater.xlsx. bastante usado neste livro 51 . Como ele já deve estar no seu diretório, fazer a leitura da seguinte maneira: dados &lt;- read_excel (&quot;Arquivos/dadosMater.xlsx&quot;) Adicione a este arquivo uma variável denominada baixoPeso, usando a função mutate() do pacvote dplyr a função ifelse (), da base do R: dados &lt;- dados %&gt;% mutate(baixoPeso = ifelse(pesoRN &lt; 2500, &quot;1&quot;, &quot;2&quot;)) Onde 1 = sim e 2 = não, ou seja, com peso de nascimento &lt; 2500g ou \\(\\ge\\) 2500g. O próximo passo é selecionar, deste arquivo, apenas esta variável criada e a variável fumo, porque o objetivo da análise será verificar se existe associação entre tabagismo na gestação e baixo peso ao nascer (&lt; 2500g). Para isso, usa-se a função select() do pacote dplyr: dados &lt;- dados %&gt;% select (fumo, baixoPeso) A seguir, será extraída uma amostra deste banco de dados com n = 300, usando a função sample_n() do pacote dplyr. A função set.seed() apenas garante que os dados selecionados aleatoriamente se mantenham os mesmos em outros sorteios (veja seção 7.7.2): set.seed(123) dados &lt;- sample_n(dados, 300) str(dados) ## tibble [300 × 2] (S3: tbl_df/tbl/data.frame) ## $ fumo : num [1:300] 2 2 1 2 2 2 1 2 2 2 ... ## $ baixoPeso: chr [1:300] &quot;2&quot; &quot;2&quot; &quot;1&quot; &quot;2&quot; ... Tem-se, agora, um conjunto de dados com duas colunas: fumo, como uma variável numérica e baixoPeso, como caractere. Ambas devem ser transformadas em fator e, onde os rótulos são 1 e 2, passam para “sim” e “não” e mantendo a ordem “sim” e “não”. dados$fumo &lt;- factor(dados$fumo, levels = c(1, 2), labels = c(&quot;sim&quot;, &quot;não&quot;)) dados$baixoPeso &lt;- factor(dados$baixoPeso, levels = c(1, 2), labels = c(&quot;sim&quot;, &quot;não&quot;)) str (dados) ## tibble [300 × 2] (S3: tbl_df/tbl/data.frame) ## $ fumo : Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 1 2 2 2 1 2 2 2 ... ## $ baixoPeso: Factor w/ 2 levels &quot;sim&quot;,&quot;não&quot;: 2 2 1 2 1 2 2 2 2 2 ... Tabelas Com os dados, será construída uma tabela com a função table() tab &lt;- with(data = dados, table(fumo, baixoPeso)) addmargins(tab) ## baixoPeso ## fumo sim não Sum ## sim 18 51 69 ## não 22 209 231 ## Sum 40 260 300 OBSERVAÇÃO: Uma tabela é constituída por linhas e colunas. Para se extrair valores da tabela, usa-se os colchetes, após o nome da tabela. O primeiro valor dentro dos colchetes é referente ao número da linha; o segundo, separado pela virgula, é referente ao número da coluna. Então, tab[1,1] se refere ao valor que está na primeira linha e primeira coluna: tab[1,1] ## [1] 18 A proporção de baixo peso por categoria de tabagismo (nº de casos/total da linha): fumantes &lt;- tab[1,1]/(tab[1,1]+ tab[1,2]) fumantes ## [1] 0.2608696 não.fumantes &lt;- tab[2,1]/(tab[2,1]+ tab[2,2]) não.fumantes ## [1] 0.0952381 Visualização gráfica Será construído um gráfico de barras empilhadas com o ggplot() do pacote ggplot2 (ver Seção 6.6): ggplot(dados) + aes (x = fumo, fill = baixoPeso) + geom_bar (color = &quot;black&quot;) + scale_fill_manual(values = c(&quot;gray&quot;, &quot;aliceblue&quot;)) + labs (title = NULL, x = &quot;Tabagismo&quot;, y = &quot;Frequência&quot;) + annotate(&quot;text&quot;, x=&quot;sim&quot;, y=62, label= &quot;26,1%&quot;) + annotate(&quot;text&quot;, x = &quot;não&quot;, y=223, label = &quot;9,5%&quot;) + theme_bw () + theme (text = element_text (size = 12)) + labs(fill = &quot;Peso ao nascer &lt; 2500g&quot;) Figura16.3: Gráfico de barras empilhadas: tabagismo vs baixo peso ao nascer. Observa-se, Figura 16.3, que 24,6% das gestantes fumantes geram bebês com baixo peso, enquanto que entre as não fumantes este percentual cai três vezes, indo para 9,5%. É uma diferença grande! Aqui, quase se tem certeza que ela é significativa, mesmo sem cálculos! 16.2.3.2 Hipóteses estatísticas \\(H_{0}\\): a proporção de baixo peso é igual nos dois grupo (fumantes e não fumantes); não há associação entre as variáveis. \\(H_{1}\\): a proporção de baixo peso é diferente nos dois grupo (fumantes e não fumantes); existe associação entre as variáveis. 16.2.3.3 Cálculo do Qui-quadrado de Pearson no R Para este exemplo, o \\(\\chi^{2}\\) irá verificar se existe uma associação entre as variáveis fumo e baixoPeso, assumindo um \\(\\alpha = 0,05\\) que equivale a um valor crítico de 3,84 com um grau de liberdade, em uma tabela \\(2 \\times 2\\). A função chisq_test() do pacote rstatix libera o qui-quadrado, usando os seguintes argumentos: x \\(\\to\\) vetor numérico ou matriz. Tanto x como y podem ser fatores; y \\(\\to\\) vetor numérico. Ignorado quando se x é uma matriz; correct \\(\\to\\) TRUE é o padrão. Indica se deve ser aplicada a correção de continuidade ao calcular a estatística de teste para tabelas 2 por 2 52; … \\(\\to\\) para outros argumentos consulte a ajuda do RStudio. Para executar a função chisq_test(), basta colocar como argumento as variáveis fumo e baixoPeso ou construir antes uma tabela de contingência com a função table() e depois colocá-la como argumento. Como tabela tab já existe: teste &lt;- rstatix::chisq_test(tab) teste ## # A tibble: 1 × 6 ## n statistic p df method p.signif ## * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 300 11.2 0.000809 1 Chi-square test *** A saída do teste exibe tudo que é necessário. O \\(\\chi^{2}\\) = 11.2 com um valor muito maior que o valor crítico de 3,84, mostrando que a diferença é estatisticamente significativa (P = 8.09^{-4}). Essas e outras estatísticas podem ser obtidas, usando o objeto teste seguido do sinal $. Por exemplo, para o valor P: teste$p ## [1] 0.000809 NOTA: Se um aviso como Chi-squared approximation may be incorrect (Aproximação qui-quadrado pode estar incorreta) aparecer, significa que as menores frequências esperadas são inferiores a 5. Para evitar esse problema, é possível usar uma das seguintes opções: reunir alguns níveis (especialmente aqueles com um pequeno número de observações) para aumentar o número de observações nos subgrupos, ou usar o teste exato de Fisher. Outras maneiras de calcular o qui-quadrado no R No pacote gmodels (140), existe uma função muito interessante, a função CrossTable() que que imprime, além de uma tabela de frequência com as proporções, exibe vários testes, como o teste \\(\\chi^2\\), o teste exato de Fisher e o teste de McNemar com e sem correção de continuidade. Consulte a ajuda para melhor estudar esta elegante função! Neste momento, será explorado apenas o qui-quadrado e os valores esperados com três dígitos: CrossTable (tab, digits = 3, prop.chisq = FALSE, prop.t = FALSE, chisq = TRUE, expected = TRUE) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Expected N | ## | N / Row Total | ## | N / Col Total | ## |-------------------------| ## ## ## Total Observations in Table: 300 ## ## ## | baixoPeso ## fumo | sim | não | Row Total | ## -------------|-----------|-----------|-----------| ## sim | 18 | 51 | 69 | ## | 9.200 | 59.800 | | ## | 0.261 | 0.739 | 0.230 | ## | 0.450 | 0.196 | | ## -------------|-----------|-----------|-----------| ## não | 22 | 209 | 231 | ## | 30.800 | 200.200 | | ## | 0.095 | 0.905 | 0.770 | ## | 0.550 | 0.804 | | ## -------------|-----------|-----------|-----------| ## Column Total | 40 | 260 | 300 | ## | 0.133 | 0.867 | | ## -------------|-----------|-----------|-----------| ## ## ## Statistics for All Table Factors ## ## ## Pearson&#39;s Chi-squared test ## ------------------------------------------------------------ ## Chi^2 = 12.61347 d.f. = 1 p = 0.0003829762 ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ------------------------------------------------------------ ## Chi^2 = 11.22084 d.f. = 1 p = 0.0008088369 ## ## Observe que a saída mostra em cada célula da tabela, o número de casos, o número esperado, a percentagem por linha (nº de casos/total da linha) e a percentagem por coluna (nº de casos/total da coluna). Por último, exibe o qui-quadrado de Pearson com e sem coreção de continuidade de Yates. 16.2.3.4 Conclusão e relato dos resultados Usando a correção de continuidade de Yates, pois é uma tabela \\(2\\times2\\), vê-se que o valor P é menor que o nível de significância de 5% e, consequentemente, rejeita-se a hipótese nula e conclui-se que existe uma associação significativa entre tabagismo na gestação e o baixo peso ao nascimento (\\(\\chi^{2}_{com \\ correção \\ de \\ Yates} (1)\\) = 11.2; P = 0,00081). Além disso, no relato dos resultados pode-se apresentar uma tabela ou em um gráfico. Tabela Para a construção da tabela, pode-se usar a função ctable() do pacote summarytools(141) para obter uma tabela com todos os dados a serem exibidos. O argumento prop = \"r\" exibe os percentuais das linhas (“c”, nas colunas). Na realidade, são maneiras diferente de se obter o mesmo resultado. ctable(dados$fumo, dados$baixoPeso, prop = &quot;r&quot;, chisq = TRUE, headings = FALSE, OR = TRUE) ## ## ------- ----------- ------------ ------------- -------------- ## baixoPeso sim não Total ## fumo ## sim 18 (26.1%) 51 (73.9%) 69 (100.0%) ## não 22 ( 9.5%) 209 (90.5%) 231 (100.0%) ## Total 40 (13.3%) 260 (86.7%) 300 (100.0%) ## ------- ----------- ------------ ------------- -------------- ## ## ---------------------------- ## Chi.squared df p.value ## ------------- ---- --------- ## 11.2208 1 8e-04 ## ---------------------------- ## ## ---------------------------------- ## Odds Ratio Lo - 95% Hi - 95% ## ------------ ---------- ---------- ## 3.35 1.67 6.71 ## ---------------------------------- Ou seja, os bebês que se expuseram ao fator de risco (fumo) têm uma chance 3,35 (IC95% : 1,67-6,71) vezes maior de apresentar peso &lt; 2500g ao nascer, nessa amostra. Além desses resultados, a tabela final pode conter (e isto é recomendado!) os intervalos de confiança para cada uma das proporções de fumantes e não fumantes. Para isso a função BinomCI() cumpre um papel suficiente (veja Seção 10.7.3.2). Então, a proporção de baixo peso entre as fumantes é: BinomCI(18, 69, conf.level = 0.95, method = &quot;clopper-pearson&quot;) ## est lwr.ci upr.ci ## [1,] 0.2608696 0.1625161 0.3805962 E, entre as não fumantes é: BinomCI(22, 231, conf.level = 0.95, method = &quot;clopper-pearson&quot;) ## est lwr.ci upr.ci ## [1,] 0.0952381 0.06065157 0.1406387 Finalmente, esses dados podem ser colocados em uma tabela, como a Tabela 16.3: Tabela16.3: Efeito do tabagismo materno no peso ao nascer Fumantes Não fumantes Valor P * Baixo Peso 18/69 22/231 0.00081 IC95% 16,3-38,1 6,1-14,1 Note: * Qui-quadrado de Pearson com correção. Gráfico Uma boa apresentação gráfica complementa o relatório dos resultados. Pode-se fazer isso com gráfico de barras empilhadas (), acompanhado dos percentuais e do tipo de teste realizado, usando a função get_test_label() que necessita do teste calculado com a função chisq_test() do pacote rstatix, apresentado antes. O gráfico assume o aspecto da Figura 16.4: ggplot(dados) + aes (x = fumo, fill = baixoPeso) + geom_bar (color = &quot;black&quot;) + scale_fill_manual(values = c(&quot;gray&quot;, &quot;gray95&quot;)) + labs (title = NULL, subtitle = get_test_label (teste, detailed = TRUE), x = &quot;Tabagismo&quot;, y = &quot;Frequência&quot;) + annotate(&quot;text&quot;, x=&quot;sim&quot;, y=62, label= &quot;24,6% (15,0-36,5)&quot;) + annotate(&quot;text&quot;, x = &quot;não&quot;, y=223, label = &quot;8,2% (5,0-12,5)&quot;) + theme_bw () + theme (text = element_text (size = 12)) + labs(fill = &quot;Peso ao nascer &lt; 2500g&quot;) Figura16.4: Gráfico de barras empilhadas: tabagismo vs baixo peso ao nascer. 16.2.4 Teste Aderência ou do Melhor Ajuste O teste de qualidade de ajuste do qui-quadrado (chi-square goodness of fit) é usado para comparar a distribuição observada com uma distribuição esperada, em uma situação em que se tem duas ou mais categorias em dados discretos. Em outras palavras, ele compara várias proporções observadas com as probabilidades esperadas (142). 16.2.4.1 Dados usados nesta seção Há uma dúvida se o número de pacientes que procura uma determinada Unidade de Pronto Atendimento (UPA) é aproximadamente o mesmo em todos os dias da semana. Esta é uma informação importante sob o ponto de vista administrativo. Para se atingir este objetivo registrou-se o número de pacientes que procurou a UPA por dia da semana. O número de atendimentos nos sete dias (de segunda-feira à domingo) da semana está representada pela frequência observada, freq_obs: freq_obs &lt;- c(20, 17, 22, 21, 26, 33, 36) freq_obs ## [1] 20 17 22 21 26 33 36 O total de atendimentos durante uma semana é igual a: soma &lt;- sum(freq_obs) soma ## [1] 175 Assim, a frequência esperada diária é igual a soma total dos atendimentos dividido pelo número observações (no caso, dias da semana), representada por k: k = 7 freq_esp &lt;- soma/k freq_esp ## [1] 25 Com estes valores , pode-se criar um vetor, p, com as proporçõess dos atendimentos diários esperados: p &lt;- rep(freq_esp/soma, 7) p ## [1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 16.2.4.2 Hipóteses estatísticas \\(H_{0}\\): a distribuição das frequências observadas (O) é igual a distribuição de frequências esperadas (E). \\(H_{1}\\): a distribuição das frequências observadas (O) não é igual a distribuição de frequências esperadas (E) 16.2.4.3 Cálculo da estatística do teste Assumindo um \\(\\alpha = 0,05\\). Os graus de liberdade são calculados como o número de células (k) menos 1: \\(gl = (k - 1)\\). O \\(\\chi^{2}_{crítico}\\) pode ser encontrado usando: alpha = 0.05 k = 7 gl = k - 1 qchisq (1 - alpha, gl) ## [1] 12.59159 Em outras palavras, se o \\(\\chi^{2}_{calculado}\\) &gt; \\(\\chi^{2}_{crítico}\\), rejeita-se a \\(H_{0}\\). Na Figura 16.5, o resultado tem que ficar à direita da linha vertical vermelha para que a hipótese nula seja rejeitada. Se cair fora da área de rejeição, à esquerda da linha vertical vermelha, aceita-se a hipóteses nula. plotchisqtest(chisq = 12.6, df = 6, colorleft = &quot;aliceblue&quot;, colorright = &quot;red&quot;, ylabel = &quot;Densidade de probabilidade&quot;, colorcut = &quot;red&quot;,) Figura16.5: Distribuição do qui-quadrado, gl = 6, alpha = 0,05. A estatística do teste pode ser encontrada, usando a função chisq.test(): teste1 &lt;- chisq.test (x = freq_obs, p = p) teste1 ## ## Chi-squared test for given probabilities ## ## data: freq_obs ## X-squared = 12, df = 6, p-value = 0.06197 16.2.4.4 Conclusão Observando-se a saída do teste do qui-quadrado, verifica-se que o \\(\\chi^{2}_{calculado}\\) &lt; \\(\\chi^{2}_{crítico}\\), portanto, não se rejeita a \\(H_{0}\\) e conclui-se que, nesta amostra, com uma confiança de 95%, que a frequência observada de pacientes à UPA é igual a esperada (P = ). Lembrando que, neste caso, como se tem um valor P limitrofe, marginal, existe a possibilidade de se estar aceitando uma \\(H_{0}\\) falsa e cometendo um erro tipo II. Seria recomendado, aumentar o tamanho amostral em uma nova coleta, usando estes dados como um piloto para o cálculo amostral. 16.2.5 Qui-quadrado de Pearson para tabelas extensas Este teste é utilizado quando o número de grupos, k, é superior a 2. 16.2.5.1 Dados usados nesta seção Como exemplo, será verificado se existe uma tendência de maior taxa de infecção nos neonatos que permanecem mais tempo hospitalizados. O banco de dados dadosCirurgia.xlsx que pode ser encontrado aqui. Salve o mesmo no seu diretório de trabalho. Esse bancode dados contém 144 recém-nascidos submetidos a diferentes procedimentos cirúrgicos. As variávreis disponíveis são: id \\(\\to\\) identificação do neonato; sexo \\(\\to\\) sexo do recém-nascido, fem e masc; peso \\(\\to\\) peso do neonato em gramas; tempohosp \\(\\to\\) tempo de hospitalização em dias; infec \\(\\to\\) presença de infecção secundária: sim e não; cirurgia \\(\\to\\) tipo de cirurgia: abdominal, cardíaca, outra. A variável tempo de hospitalização (tempohosp) é contínua e assimétrica. Para ser usada aqui, será categorizada por quartis. A variável infec (presença de infecção) é uma variável dicotômica (sim, não). Leitura e transformação dos dados A leitura dos dados é feita com: cirurgia &lt;- read_excel (&quot;Arquivos/dadosCirurgia.xlsx&quot;) str(cirurgia) ## tibble [144 × 7] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:144] 1 2 3 4 5 6 7 8 9 10 ... ## $ sexo : chr [1:144] &quot;masc&quot; &quot;masc&quot; &quot;masc&quot; &quot;masc&quot; ... ## $ peso : num [1:144] 2020 1850 2540 1150 2900 ... ## $ ig : num [1:144] 36 30 38 31 36 37 38 39 38 39 ... ## $ tempohosp: num [1:144] 37 37 37 46 37 36 30 18 25 14 ... ## $ infec : chr [1:144] &quot;não&quot; &quot;não&quot; &quot;sim&quot; &quot;sim&quot; ... ## $ cirurgia : chr [1:144] &quot;abdominal&quot; &quot;abdominal&quot; &quot;abdominal&quot; &quot;outra&quot; ... Característicamente, a variável tempohosp (tempo de hospitalização) é, quase sempre, assimétrica: cirurgia %&gt;% ggplot() + geom_histogram(aes(x = tempohosp, y = after_stat(density)), fill = &quot;tomato&quot;, bins = 20, col=alpha(&quot;gray40&quot;,0.5)) + geom_function(fun=dnorm, args=list(mean=mean(cirurgia$tempohosp, na.rm = T), sd= sd(cirurgia$tempohosp, na.rm = T)), col=&#39;dodgerblue4&#39;, lwd=1, lty=2) + labs(x=&#39;Tempo de hospitalização (dias)&#39;, y=&#39;Densidade de probabilidade&#39;) + theme_bw() A variável tempohosp será categorizada em quartis. O resumo da mesma, usando a função summary(), fornece orientação para esse procedimento: summary (cirurgia$tempohosp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 20.75 27.50 37.42 42.00 245.00 O sumário da variável fornece orientação para a categorização, que será realizada, usando a função cut(), consulte a Seção 6.4.2 para detalhes de construção de uma tabela de frequência: cirurgia$tempo &lt;- cut(cirurgia$tempohosp, breaks = c(1, 20.75, 27.50, 42.00, 245), labels = c(&quot;&lt;= 21&quot;,&quot;22-28&quot;, &quot;29-42&quot;,&quot;&gt;42&quot;), right = FALSE, include.lowest = TRUE) tab1 &lt;- table (cirurgia$tempo) tab1 ## ## &lt;= 21 22-28 29-42 &gt;42 ## 36 36 35 37 Por exemplo, 36 recém-nascidos premaneceram 21 dias ou menos no pós-operatório e 37 recém-nascidos ficarm internados mais do que 42 dias. Agora, a variável cirurgia$infec será colocada como um fator: cirurgia$infec &lt;- factor(cirurgia$infec, levels = c(&quot;sim&quot;, &quot;não&quot;)) table(cirurgia$infec) ## ## sim não ## 56 88 A nova variável tempo será cruzada com a variável infec em uma tabela: tab2 &lt;- table (cirurgia$tempo, cirurgia$infec) addmargins(tab2) ## ## sim não Sum ## &lt;= 21 9 27 36 ## 22-28 11 25 36 ## 29-42 14 21 35 ## &gt;42 22 15 37 ## Sum 56 88 144 Ou seja, a proporção de recém-nascidos que se infectaram no pós-operatório foi 56/144 = 0,39 ou 39%. Se for calculada a proporção para cada um dos quartis do tempo de hospitalização, através da função ctable() do pacote summarytools, observa-se que a proporção de neonatos infectados no pós-operatório aumenta com o tempo de hospitalização (Tabela ??): ## ## ------- ------- ------------ ------------ -------------- ## infec sim não Total ## tempo ## &lt;= 21 9 (25.0%) 27 (75.0%) 36 (100.0%) ## 22-28 11 (30.6%) 25 (69.4%) 36 (100.0%) ## 29-42 14 (40.0%) 21 (60.0%) 35 (100.0%) ## &gt;42 22 (59.5%) 15 (40.5%) 37 (100.0%) ## Total 56 (38.9%) 88 (61.1%) 144 (100.0%) ## ------- ------- ------------ ------------ -------------- 16.2.5.2 Hipóteses estatísticas \\(H_{0}\\): A presença de infecção não altera o tempo de hospitalização. \\(H_{1}\\): A presença de infecção altera o tempo de hospitalização. 16.2.5.3 Cálculo da estatística do teste O teste estatístico pode ser calculado, usando o argumento chisq = TRUE na função ctable(): summarytools::ctable(cirurgia$tempo, cirurgia$infec, prop = &quot;r&quot;, chisq = TRUE, headings = FALSE) ## ## ------- ------- ------------ ------------ -------------- ## infec sim não Total ## tempo ## &lt;= 21 9 (25.0%) 27 (75.0%) 36 (100.0%) ## 22-28 11 (30.6%) 25 (69.4%) 36 (100.0%) ## 29-42 14 (40.0%) 21 (60.0%) 35 (100.0%) ## &gt;42 22 (59.5%) 15 (40.5%) 37 (100.0%) ## Total 56 (38.9%) 88 (61.1%) 144 (100.0%) ## ------- ------- ------------ ------------ -------------- ## ## ---------------------------- ## Chi.squared df p.value ## ------------- ---- --------- ## 10.5801 3 0.0142 ## ---------------------------- 16.2.5.4 Conclusão A partir desses resultados, pode-se inferir que a menor taxa de infecção no grupo do primeiro quartil e é significativamente diferente em relação a taxa de infecção do maior quartil, mas sem indicação para os grupos intermediários. É útil fazer o teste de tendência linear (Linear-by-linear Association). Para isso, pode-se usar a função lbl_test () do pacote coin. coin::lbl_test (cirurgia$tempo ~ cirurgia$infec) ## ## Asymptotic Linear-by-Linear Association Test ## ## data: cirurgia$tempo (ordered) by cirurgia$infec (sim, não) ## Z = 3.1231, p-value = 0.001789 ## alternative hypothesis: two.sided Este teste indica uma tendência significativa para a presença de infecção à medida que aumenta o tempo de hospitalização (P = 0,0018). 16.3 Teste exato de Fisher O teste do qui-quadrado não é um método apropriado de análise se a amostra é pequena. Por exemplo, se n for menor que 20 ou se n estiver entre 20 e 40 e uma das frequências esperadas for menor que 5, o teste do qui-quadrado deve ser evitado. Nesta situação, é recomendado o teste exato de Fisher. 16.3.1 Dados usados nesta seção Um estudo estabeleceu como objetivo verificar se a asma não controlada é um fator de risco para a procura da emergência. Foram acompanhados 16 escolares asmáticos durante um ano com relação ao número de visitas à emergência de acordo com o controle da sua asma. 16.3.1.1 Entrando com os dados Em primeiro lugar, serão criadas duas variáveis: emerg &lt;- c (1,1,2,2,2,2,2,1,1,1,1,1,1,1,1,2) controle &lt;- c (1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2) A seguir, usando essas variáveis, será construído um dataframe que será atribuído ao objeto dadosControle: dadosControle &lt;- data.frame(emerg, controle) 16.3.1.2 Transformação dos dados Ambas as variáveis são numéricas e serão transformadas em fatores, considerando 1 = sim e 2 = não e considerando essa ordem: dadosControle$emerg &lt;- factor (dadosControle$emerg, ordered=TRUE, levels = c (1,2), labels = c (&#39;sim&#39;, &#39;não&#39;)) dadosControle$controle &lt;- factor (dadosControle$controle, ordered=TRUE, levels = c (1,2), labels = c (&#39;sim&#39;, &#39;não&#39;)) str(dadosControle) ## &#39;data.frame&#39;: 16 obs. of 2 variables: ## $ emerg : Ord.factor w/ 2 levels &quot;sim&quot;&lt;&quot;não&quot;: 1 1 2 2 2 2 2 1 1 1 ... ## $ controle: Ord.factor w/ 2 levels &quot;sim&quot;&lt;&quot;não&quot;: 1 1 1 1 1 1 1 2 2 2 ... 16.3.1.3 Construção da tabela Para o cálculo da estatística do teste, é necessário uma tabela \\(2\\times2\\): tab3 &lt;- table (dadosControle$controle, dadosControle$emerg) addmargins(tab3) ## ## sim não Sum ## sim 2 5 7 ## não 8 1 9 ## Sum 10 6 16 16.3.2 Hipóteses estatísticas \\(H_0\\): as variáveis são independentes, não há relação entre as duas variáveis categóricas. \\(H_1\\): as variáveis são dependentes, existe uma relação entre as duas variáveis categóricas. 16.3.3 Cálculo da estatística do teste O teste exato de Fisher é usado quando há pelo menos uma célula na tabela de contingência das frequências esperadas abaixo de 5. Para recuperar as frequências esperadas, use a função chisq.test (), do R base, junto com $expected: chisq.test (dadosControle$controle, dadosControle$emerg)$expected ## Warning in chisq.test(dadosControle$controle, dadosControle$emerg): Aproximação ## do qui-quadrado pode estar incorreta ## dadosControle$emerg ## dadosControle$controle sim não ## sim 4.375 2.625 ## não 5.625 3.375 A saída do teste imprime um aviso de que o qui-quadrado pode estar incorreto. Há necessidade, devido a presença de três células com valores abaixo de 5, de se usar o teste de Fisher. este pode ser obtido através da função fisher_test(), do pacote rstatix, colocando como argumento uma tabela de contingência \\(2\\times2\\), como a tab3: rstatix::fisher_test (tab3, detailed = TRUE) ## # A tibble: 1 × 8 ## n estimate p conf.low conf.high method alternative p.signif ## * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 16 0.0646 0.035 0.000953 0.991 Fisher&#39;s Exact t… two.sided * 16.3.4 Conclusão O valor \\(P=0,035\\) é menor que o nível de significância de 5%, previamente estabelecido, e, portanto, deve-se rejeitar a hipótese nula. No contexto, rejeitar a hipótese nula para o teste exato de independência de Fisher significa que há uma associação significativa entre as duas variáveis categóricas (controle da asma e visitas à emergência). 16.4 Teste de Macnemar É um teste estatístico não paramétrico aplicável nos estudos tipo “antes-e-depois” em que cada indivíduo é utilizado como seu próprio controle e a medida é efetuada em escala nominal. O teste de McNemar é usado para determinar se há uma diferença estatisticamente significativa nas proporções entre os dados emparelhados. As medidas coletadas nesses tipos de projetos de estudo não são independentes e, portanto, os testes do Qui-quadrado não podem ser usados porque os pressupostos serão violados. O teste de McNemar é usado para avaliar se há uma mudança significativa nas proporções ao longo do tempo para dados emparelhados ou se há uma diferença significativa nas proporções entre casos e controles. O resultado de interesse é a mudança dentro da pessoa (ou diferenças dentro do par) e não há variáveis explicativas. O teste é calculado examinando o número de respostas que são concordantes para positivo (sim em ambas as ocasiões) e negativo (não em ambas as ocasiões) e o número de pares disconcordantes (sim e não, ou não e sim). Os pares concordantes não fornecem informações sobre as diferenças e não são usados na avaliação. Em vez disso, deve-se concentrar nos pares discordantes, que podem ser divididos em dois tipos: um par discordante do tipo sim – não e um par discordante tipo não – sim (143). 16.4.1 Pressupostos do teste de McNemar Os pressupostos para o teste de McNemar são: A variável desfecho é binária, dicotômica; Cada participante é representado na tabela apenas uma vez; A diferença entre as proporções emparelhadas é o resultado de interesse; O teste de McNemar pode não ser confiável se houver contagens baixas nas células “discordantes”. Existe recomendação de que a soma dessas células seja \\(\\ge 20\\) (144). 16.4.2 Dados usados nesta seção Em uma universidade, um professor de bioestatística comparou as atitudes de 200 estudantes de Medicina em relação à confiança que eles depositam na análise estatística antes e depois da conclusão da disciplina. A pergunta feita foi: Confiam na análise estatística utilizada nos periódicos médicos? As respostas podem ser resumidas na Tabela 16.4: Tabela16.4: Confiança na análise estatística após término da disciplina Pré-teste Pós-teste (sim) Pós-teste (não) Total Sim 20 (a) 8 (b) 28 Não 22 (c) 150 (d) 172 Total 42 158 200 16.4.3 Hipóteses estatísticas Considerando as caselas a, b, c e da Tabela 16.4, a hipótese nula de homogeneidade marginal indica que as duas probabilidades marginais para cada resultado são as mesmas, isto é, \\[ p_{a} + p_{b} = p_{a} + p_{c} \\] e \\[ p_{c} + p_{d} = p_{b} + p_{d} \\] Assim, a hipótese nula e a hipótese alternativa são: \\(H_{0}\\): a proporção de alunos que respondem sim no pré-teste e no pós-teste é a mesma. \\(H_{1}\\): a proporção de alunos que respondem sim no pré-teste e no pós-teste não é a mesma. 16.4.4 Lógica do teste O teste estatístico de McNemar, com correção de continuidade, é obtido utilizando a equação: \\[ \\chi^{2} = \\frac {\\left (\\left |b - c \\right |- 1 \\right )^{2}}{b + c} \\] Sob a hipótese nula, com um número suficientemente grande de discordantes (células b e c), o \\(\\chi^{2}\\) tem uma distribuição qui-quadrado com um grau de liberdade. Se o resultado é significativo, isto é, fornece evidências suficientes para rejeitar a hipótese nula, significa que as proporções marginais são significativamente diferentes umas das outras. Substituindo os dados da Tabela na Equação, tem-se: a &lt;- 20 b &lt;- 8 c &lt;- 22 d &lt;- 150 chi &lt;- ((abs(b - c) - 1)^2)/(b + c) chi ## [1] 5.633333 Assumindo um \\(\\alpha = 0,05\\), pode-se obter valor crítico para o \\(\\chi^{2}\\) para gl = 1, usando a função qchisq(), do pacote stats: alpha = 0.05 qchisq(1 - alpha, 1) ## [1] 3.841459 Desta maneira, rejeita-se a \\(H_{0}\\), pois o \\(\\chi_{calculado}^{2} &gt; \\chi_{crítico}^{2}\\). O valor P pode ser conseguido com a função pchisq(): pchisq (5.633, 1, lower.tail = FALSE) ## [1] 0.01762544 16.4.5 Cálculo do teste de McNemar no R Carregar o arquivo dadosBioestatistica.xlsx, que pode ser encontrado aqui. Este conjunto de dados contem os dados da tabela acima. dados &lt;- readxl::read_excel(&quot;Arquivos/dadosBioestatistica.xlsx&quot;) Construir uma tabela de contingência dados$preteste &lt;- factor(dados$preteste, levels = c(&quot;sim&quot;, &quot;não&quot;)) dados$posteste &lt;- factor(dados$posteste, levels = c(&quot;sim&quot;, &quot;não&quot;)) tab4 &lt;- table(dados$preteste, dados$posteste, dnn = c(&quot;Pré-teste&quot;, &quot;Pós-teste&quot;)) tab4 ## Pós-teste ## Pré-teste sim não ## sim 20 8 ## não 22 150 Agora, pode-se calcular o teste de McNemar53 com a função mcnemar_test() do pacote rstatix: rstatix::mcnemar_test (tab4, correct = TRUE) ## # A tibble: 1 × 6 ## n statistic df p p.signif method ## * &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 200 5.63 1 0.0176 * McNemar test O resultado do teste de McNemar com correção de continuidade é exatamente igual ao calculado manualmente. 16.4.6 Conclusão Houve uma modificação estatisticamente significativa na opinião dos alunos após o curso de Bioestatística em relação à confiança nas análises estatísticas (86% no pré-teste de respostas não x 79% no pós-teste, \\(\\chi^{2} = 5,63, gl = 1, P = 0,018\\)). Alguns alunos (14) mudaram de opinião em relação a sua confiança nas análises estatísticas dos periódicos médicos. Ver Seção 5.3↩︎ Quando não se está trabalhando com uma tabela \\(2 \\times 2\\) e a regra geral for obedecida e o n for grande, pode-se usar o qui-quadrado de Pearson sem correção.↩︎ É possível também obter o teste de McNemar de outras formas, como, por exemplo, usando a função CrossTable() do pacote gmodels.↩︎ "],["métodos-não-paramétricos.html", "Capítulo 17 Métodos não paramétricos 17.1 Pacotes necessários neste capítulo 17.2 Distribuição livre 17.3 Postos 17.4 Teste de Mann-Whitney 17.5 Teste de Wilcoxon 17.6 Teste de Kruskal-Wallis", " Capítulo 17 Métodos não paramétricos 17.1 Pacotes necessários neste capítulo pacman::p_load (coin, confintr, dplyr, ggplot2, ggpubr, ggsci, kableExtra, knitr, readxl, rstatix, tidyr) 17.2 Distribuição livre A maioria dos testes estatísticos que discutidos são testes paramétricos. Nestes, o interesse estava focado em estimar ou testar uma hipótese sobre um ou mais parâmetros populacionais. Além disso, o aspecto central desses procedimentos era o conhecimento da forma funcional da população da qual foram retiradas as amostras que forneceram a base para a inferência. Por exemplo, o teste t de Student para amostras independentes e a ANOVA são baseados no pressuposto de que os dados foram amostrados de populações que têm distribuição normal. Os testes não paramétricos não fazem suposições em relação à distribuição da população. Não têm, portanto, os pressupostos restritivos, comuns nos testes paramétricos. Têm distribuição livre. São baseados em uma ideia simples de ordenação por postos, do valor mais baixo ao mais alto. Analisam somente os postos, ignorando os valores. Podem ser usados tanto com variáveis ordinais como quantitativas numéricas. 17.3 Postos Os métodos estatísticos não paramétricos não lidam diretamente com os valores observados. Em função disso, para poder usar a informação fornecida pelas observações, sem trabalhar diretamente com os valores observados, utiliza-se os postos das observações. Posto (rank) de uma observação é a sua posição em relação aos demais valores. A atribuição dos postos de uma variável é realizada da seguinte maneira: Colocam-se as observações em ordem crescente; Associam-se valores, correspondendo às suas posições relativas na amostra. O primeiro elemento recebe o valor 1, o segundo o valor 2 e, assim por diante, até que a maior observação receba o valor n; Se todas as observações são distintas, os postos são iguais aos valores associados às observações no passo anterior. Para observações iguais (empates), associam-se postos iguais à média das suas posições relativas na amostra. Por exemplo, suponha uma amostra contendo os escores de Apgar no primeiro minuto de 10 recém-nascidos a termo (Tabela 17.1). Em primeiro lugar, os valores são colocados em ordem crescente e, após, atribui-se postos aos valores. Observe que os postos atribuídos aos valores das posições 3 e 4 são iguais e correspondentes a média de 3 e 4, que é igual a 3,5. O mesmo ocorreu com os outros valores onde houve empate. A soma dos postos, no exemplo, é igual a 55. Para verificar a correção do cálculo, haja ou não empates, a soma dos postos será sempre \\(\\frac {n\\ \\times \\ (n+1)}{2}\\). No exemplo, n = 10, logo \\(\\frac {10\\ \\times \\ (10+1)}{2}=55\\). Tabela17.1: Construção dos postos Apgar 1 Ordem Posto 4 1 1.0 5 2 2.0 7 3 3.0 8 4 5.0 8 5 5.0 8 6 5.0 9 7 7.5 9 8 7.5 10 9 9.5 10 10 9.5 17.4 Teste de Mann-Whitney O teste de Mann-Whitney é usado para analisar a diferença na variável dependente (desfecho) para dois grupos independentes. O teste classifica todos os valores dependentes, ou seja, o valor mais baixo obtém o posto um e, em seguida, usa a soma dos postos de cada grupo no cálculo da estatística de teste. É o substituto do teste t para amostras independentes quando os pressupostos deste teste são violados. Para a aplicação do teste de Mann-Whitney a variável de interesse deve ser ordinal ou numérica. Este teste é equivalente ao desenvolvido por Frank Wilcoxon (1892 – 1965), assim algumas vezes é denominado de Wilcoxon Rank Sum Test. O R usa esta denominação e é importante não confundir com o teste não paramétrico para amostra pareadas, discutido mais adiante. 17.4.1 Dados usados nesta seção O arquivo dadosCirurgia.xlsx, já usado na Seção 16.2.5.1, fornecerá os dados para esta seção. Ele contém 144 recém-nascidos que foram submetidos a diferentes procedimentos cirúrgicos. A questão de pesquisa a ser respondida é: Existe diferença no tempo de hospitalização (tempohosp) dos recém-nascidos de acordo com a presença ou não de infecção (infec)? Essa pergunta foi respondida de outra maneira, na Seção 16.2.5. Agora, será usado o teste de Mann-Whitney. 17.4.1.1 Leitura, exploração e visualização dos dados Os dados serão lidos com a função read_excel() do pacote readxl: cirurgia &lt;- readxl::read_excel (&quot;Arquivos/dadosCirurgia.xlsx&quot;) str(cirurgia) ## tibble [144 × 7] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:144] 1 2 3 4 5 6 7 8 9 10 ... ## $ sexo : chr [1:144] &quot;masc&quot; &quot;masc&quot; &quot;masc&quot; &quot;masc&quot; ... ## $ peso : num [1:144] 2020 1850 2540 1150 2900 ... ## $ ig : num [1:144] 36 30 38 31 36 37 38 39 38 39 ... ## $ tempohosp: num [1:144] 37 37 37 46 37 36 30 18 25 14 ... ## $ infec : chr [1:144] &quot;não&quot; &quot;não&quot; &quot;sim&quot; &quot;sim&quot; ... ## $ cirurgia : chr [1:144] &quot;abdominal&quot; &quot;abdominal&quot; &quot;abdominal&quot; &quot;outra&quot; ... A variável infec aparece como caractere e será transformada como fator: cirurgia$infec &lt;- as.factor(cirurgia$infec) Os boxplots (Figura 17.1), construídos com a função ggboxplot() do pacote ggpubr com as cores da paleta do New England Journal of Medicine (NEJM), são uma boa maneira de visualizar os dados: ggpubr::ggboxplot(cirurgia, x = &quot;infec&quot;, y = &quot;tempohosp&quot;, bxp.errorbar = TRUE, bxp.errorbar.width = 0.1, fill = &quot;infec&quot;, palette = &quot;nejm&quot;, legend = &quot;none&quot;, ggtheme = theme_bw(), xlab = &quot;Presença de infecção&quot; , ylab = &quot;Tempo de hospitalização (dias)&quot;) Figura17.1: Impacto da infecção no tempo de hopsitalização. Os boxplots exibem uma série de valores atípicos, indicando que existe uma assimetria em ambos os grupos. Essa assimetria também pode ser verificada usando o teste de Shapiro-Wilk, que mostrando valores P &lt; 0,05, confirma que os dados não seguem a distribuição normal. Este teste não é pré-requisito para o teste. Foi realizado como uma demonstração. cirurgia %&gt;% dplyr::group_by(infec) %&gt;% rstatix::shapiro_test(tempohosp) ## # A tibble: 2 × 4 ## infec variable statistic p ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 não tempohosp 0.565 9.87e-15 ## 2 sim tempohosp 0.692 1.47e- 9 17.4.1.2 Sumarização dos dados Como a variável tempohosp é assimétrica conforme mostrado acima, onde ambos os valores P são menores do que 0,05, será realizado um sumário numérico com a obtenção da mediana e IIQ. Isto será feito através da função group_by() e summarise(), incluídas no pacote dplyr. resumo &lt;- cirurgia %&gt;% dplyr::group_by(infec) %&gt;% dplyr::summarise(n = n(), mediana = median (tempohosp, na.rm = TRUE), p25=quantile(tempohosp, probs = 0.25, na.rm = TRUE), p75=quantile(tempohosp, probs = 0.75, na.rm = TRUE)) resumo ## # A tibble: 2 × 5 ## infec n mediana p25 p75 ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 não 88 23 18 37 ## 2 sim 56 37 22.8 49 Os dados mostram que a mediana de tempo de internação dos neonatos infectados é bem maior do que os não infectados. A diferença mediana e os intervalos de confiança para essa diferença pode ser calculada com a função ci_quantile_diff() do pacote confintr (145). infectado &lt;- dplyr::filter(cirurgia, infec == &quot;sim&quot;) %&gt;% select(tempohosp) sem_infec &lt;- dplyr::filter(cirurgia, infec == &quot;não&quot;) %&gt;% select(tempohosp) dif_mediana &lt;- median(infectado$tempohosp)- median(sem_infec$tempohosp) dif_mediana ## [1] 14 ci_quantile_diff(infectado, sem_infec) ## ## Two-sided 95% bootstrap confidence interval for the population value of ## 50% quantile(x) - 50% quantile(y) based on 9999 bootstrap replications ## and the bca method ## ## Sample estimate: 14 ## Confidence interval: ## 2.5% 97.5% ## 4.5 19.5 Dessa forma, a mediana da diferença entre os dois grupos foi de 14 dias (IC95%; 5 - 20). 17.4.2 Hipóteses estatísticas Da mesma maneira que o teste t, as hipóteses estabelecidas comparam dois grupos independentes. Se não houver diferença entre os grupos, ou seja, os grupos são provenientes de uma mesma população, as somas dos postos em cada grupo devem ficar próximas. Desta forma, \\(H_{0}\\): As duas populações são iguais. \\(H_{1}\\): As duas populações não são iguais. Não foi escrita a hipótese nula como sendo de que as médias (ou as medianas) são iguais, pois o teste não usa as medidas de posição tradicionais e sim os postos. 17.4.3 Pressupostos do teste de Mann_Whitney O teste de Mann-Whitney é baseado nos seguintes pressupostos: Os dados são aleatórios; As amostras são de dois grupos independentes; Um dos grupos é denominado de 1 e o outro de 2; A variável a ser comparada nos grupos deve ser ordenável; O grupo 1 será o grupo de menor tamanho e, se tiverem o mesmo tamanho, o grupo 1 é aquele cuja soma dos postos é a menor. 17.4.4 Cálculo da estatística de teste 17.4.4.1 Lógica do teste U de Mann-Whitney De acordo com as hipóteses estabelecidas, o teste é bicaudal. Se as observações nos dois grupos forem provenientes da mesma população, a soma dos postos em cada grupo devem ficar próximas. Para calcular o teste, procede-se da seguinte maneira: Deve haver uma variável que identifique o grupo a que pertence cada uma das observações. No exemplo proposto, a variável desfecho é tempohosp e a variável agrupadora é infec, categorizada como sim e não. Ordenar de forma crescente todos os valores da variável tempohosp, sem levar em consideração a que grupo pertence. Para realizar este procedimento, será usada a função rank() do R base com o método para empates igual à média dos valores empatados (ties.method=\"average). Ao executar a função, será criada uma nova variável, denotada postos. cirurgia$postos &lt;- rank(cirurgia$tempohosp, ties.method = &quot;average&quot;) head(cirurgia) ## # A tibble: 6 × 8 ## id sexo peso ig tempohosp infec cirurgia postos ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 masc 2020 36 37 não abdominal 94.5 ## 2 2 masc 1850 30 37 não abdominal 94.5 ## 3 3 masc 2540 38 37 sim abdominal 94.5 ## 4 4 masc 1150 31 46 sim outra 120 ## 5 5 masc 2900 36 37 não abdominal 94.5 ## 6 6 fem 2480 37 36 sim abdominal 91 Verificar o tamanho (n) de cada grupo (presença ou não de infecção) e somar os postos em cada um dos grupos, usando a função group_by() junto com a função summarise(), resumo &lt;- cirurgia %&gt;% dplyr::group_by(infec) %&gt;% dplyr::summarise(n = n(), soma = sum(postos)) resumo ## # A tibble: 2 × 3 ## infec n soma ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 não 88 5564. ## 2 sim 56 4876. Denominar de grupo_1 o grupo com menor soma: grupo_1 &lt;- min(resumo$soma) grupo_1 ## [1] 4875.5 Denotar o grupo_1 como T T &lt;- grupo_1 Consequentemente, n1 &lt;- 56 n2 &lt;- 88 Calcular a estística do teste, usando a fórmula preconizada por Altman (146): \\[ U =n_{1} \\times n_{2} \\ +\\left [\\frac{n_{1} \\times \\left (n_{1} + 1 \\right )}{2} \\right ] - T \\] U &lt;- (n1*n2 + ((n1*(n1 + 1))/2)) - T U ## [1] 1648.5 Obs.: O U de Mann-Whitney aparece no teste de Wilcoxon como W, eles são iguais Se \\(n_{1}\\), \\(n_{2}\\) \\(\\ge\\) 10, a distribuição da estatística do teste pode ser aproximada por uma distribuição normal com média igual a \\[ \\mu_{U} =\\left [\\frac{n_{S} \\times \\left (n_{L} + 1 \\right )}{2} \\right ] \\] onde \\(n_{S}\\) e \\(n_{L}\\), são, respectivamente, o grupo de menor e maior tamanho. No exemplo, \\(n_{1}\\) e \\(n_{2}\\). m_U &lt;- (n1*(n1+n2+1))/2 m_U ## [1] 4060 E desvio padrão igual a \\[ \\sigma_{U}= \\sqrt {\\frac{n_{L}\\times \\sigma_{U}}{6}} \\] dp_U &lt;- sqrt((n2*m_U)/6) dp_U ## [1] 244.0219 Os resultados fornecem os dados para calcular a estatística \\(Z_{U}\\) com correção de continuidade e, a partir dela, calcular o valor P. \\[ Z_{U}= \\frac{(T -0,5) - \\mu_{U}}{\\sigma_{U}} \\] Z_U &lt;- ((T - 0.5) - m_U)/dp_U round(Z_U, 2) ## [1] 3.34 Finalmente, calcula-se o valor P, usando a função pnorm(), multiplicada por 2, pois o teste é bicaudal. P &lt;- pnorm(Z_U, lower.tail = FALSE) * 2 round(P, 4) ## [1] 8e-04 Na prática, não há necessidade de fazer todos esses cálculos, pois o R calcula facilmente o teste. Os cálculos foram mostrados para melhorar o entendimento de como o teste de Mann-Whitney funciona. 17.4.4.2 Cálculo do U de Mann-Whitney no R O teste pode ser realizado com a função wilcox_test() 54 do pacote rstatix: teste &lt;- rstatix::wilcox_test(formula = tempohosp ~ infec, data = cirurgia) teste ## # A tibble: 1 × 7 ## .y. group1 group2 n1 n2 statistic p ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 tempohosp não sim 88 56 1648. 0.00083 Assim como no cálculo manual, o teste no R, mostra uma diferença estatisticamente significativa (P = 0,00083) entre os tempos de hospitalização dos recém-nascidos que realizaram cirurgia no período neonatal que se infectaram ou não (diferença mediana = 14 dias; IC95% : 5,0 – 19,5). 17.4.5 Tamanho do efeito É interessante calcular o tamanho do efeito, a magnitude do efeito. O tamanho do efeito r é calculado como a estatística \\(Z_{U}\\) dividida pela raiz quadrada do tamanho da amostra (\\(n = n_{1} + n_{2}\\)). \\[ r = \\frac {Z_{U}}{\\sqrt{n}} \\] O valor de \\(Z_{U}\\) é igual a 3.3398648, logo r &lt;- Z_U/sqrt(n1+n2) round(r,3) ## [1] 0.278 O R possui a função wilcox_effsize() do pacote rstatix e necessita também do pacote coin (147) , instalado para calcular a estatística r.55 A saída exibirá junto a magnitude o efeito, que no caso é pequena (veja Tabela 17.2). wilcox_effsize(cirurgia, tempohosp~infec) ## # A tibble: 1 × 7 ## .y. group1 group2 effsize n1 n2 magnitude ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt; ## 1 tempohosp não sim 0.279 88 56 small Tabela17.2: Interpretação do valor r (sem considerar o sinal) Valor r Magnitude 0,10 &lt; 0,30 pequeno 0,30 &lt; 0,50 médio &gt;= 0,50 grande 17.4.6 Conclusão O valor \\(P&lt;0,0001\\), bem abaixo do nível de significância estabelecido (\\(\\alpha = 0,05)\\). Pode-se concluir que o tempo de hospitalização nos dois grupos é estatisticamente diferente. Entretanto, a magnitude dessa diferença é pequena. Isto pode ser visualizado no gráfico (Figura 17.2): ggpubr::ggboxplot(cirurgia, x = &quot;infec&quot;, y = &quot;tempohosp&quot;, bxp.errorbar = TRUE, bxp.errorbar.width = 0.1, fill = &quot;infec&quot;, palette = &quot;nejm&quot;, legend = &quot;none&quot;, ggtheme = theme_bw(), xlab = &quot;Presença de infecção&quot; , ylab = &quot;Tempo de hospitalização (dias)&quot;) + labs(subtitle = rstatix::get_test_label(teste, detailed = TRUE)) Figura17.2: Impacto da infecção no tempo de hopsitalização. 17.5 Teste de Wilcoxon O teste de Wilcoxon, também conhecido como teste dos postos com sinais de Wilcoxon (Wilcoxon Signed-Rank Test), é um teste não paramétrico utilizado em situações em que existem dois conjuntos de dados emparelhados, ou seja, dados provenientes do mesmo participante. O teste não examina os dois grupos individualmente; em vez disso, ele se concentra na diferença existente entre cada par de observações. É um equivalente não paramétrico do teste t pareado. 17.5.1 Dados usados nesta seção Pata verificar se a realização de exercícios aeróbicos modifica a função respiratória de 10 escolares asmáticos, foi medido o Pico de Fluxo Expiratório Máximo (Peak Flow Meter) no início e no final do programa, após 120 dias. O Pico de Fluxo Expiratório Máximo (PFE) serve como uma forma simples de avaliar a força e a velocidade de saída do ar de dentro dos pulmões. É medido em L/min. Os resultados do estudo tem apenas três variáveis, id, basal e final. id &lt;- c(1:10) basal &lt;- c(120, 200, 140, 200, 110, 240, 150, 120, 250, 190) final &lt;- c(220, 300, 230, 180, 300, 330, 230, 250, 300, 200) dados &lt;- tibble(id, basal, final) head (dados) ## # A tibble: 6 × 3 ## id basal final ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 120 220 ## 2 2 200 300 ## 3 3 140 230 ## 4 4 200 180 ## 5 5 110 300 ## 6 6 240 330 A questão de pesquisa a ser respondida, portanto, é: Existe diferença entre as medidas iniciais e finais do PFE dos escolares asmáticos que entraram em um programa de exercícios aeróbicos? 17.5.1.1 Exploração e transformação dos dados Os dados estão no formato amplo com as variáveis basal e final classificadas como númericas. Será transformado para o formato longo, usando a função pivot_longer() do pacote tidyr. Este processo é opcional, mas, como foi feito com o teste t pareado, será repetido aqui como treinamento: dadosL &lt;- dados %&gt;% tidyr::pivot_longer(c(basal, final), names_to = &quot;momento&quot;, values_to = &quot;medidas&quot;) str(dadosL) ## tibble [20 × 3] (S3: tbl_df/tbl/data.frame) ## $ id : int [1:20] 1 1 2 2 3 3 4 4 5 5 ... ## $ momento: chr [1:20] &quot;basal&quot; &quot;final&quot; &quot;basal&quot; &quot;final&quot; ... ## $ medidas: num [1:20] 120 220 200 300 140 230 200 180 110 300 ... 17.5.1.2 Medidas resumidoras Como o número de participantes é de apenas 10, a medida de posição mais adequada para resumir os dados é mediana e a medida de dispersão é o intervalo interquartil (IIQ). Para isso, se fará uso das funções group_by() e summarise() do pacote dplyr: resumo &lt;- dadosL %&gt;% dplyr::group_by(momento) %&gt;% dplyr::summarise(n = n(), mediana = median (medidas, na.rm = TRUE), p25=quantile(medidas, probs = 0.25, na.rm = TRUE), p75=quantile(medidas, probs = 0.75, na.rm = TRUE), media = mean (medidas, na.rm = TRUE), dp = sd (medidas, na.rm = TRUE), ep = dp/sqrt(n), me = ep * qt(1 - (0.05/2), n - 1)) resumo ## # A tibble: 2 × 9 ## momento n mediana p25 p75 media dp ep me ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 basal 10 170 125 200 172 50.9 16.1 36.4 ## 2 final 10 240 222. 300 254 50.4 15.9 36.0 17.5.1.3 Visualização dos dados Pode-se fazer visualização gráfica dos dados usando um boxplot (Figura 17.3 ou um gráfico de linha (Figura 17.4). Boxplot ggpubr::ggboxplot(dadosL, x = &quot;momento&quot;, y = &quot;medidas&quot;, bxp.errorbar = TRUE, bxp.errorbar.width = 0.1, fill = &quot;momento&quot;, palette = c(&quot;cyan4&quot;, &quot;cyan3&quot;), legend = &quot;none&quot;, ggtheme = theme_bw(), xlab = &quot;Momento&quot; , ylab = &quot;PEF (L/min) &quot;)+ theme (text = element_text (size = 13), axis.text.x= element_text(size = 12)) Figura17.3: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos. Gráfico de linha ggpubr::ggline(dadosL, x = &quot;momento&quot;, y = &quot;medidas&quot;, color = &quot;cyan4&quot;, linetype = &quot;dashed&quot;, size = 0.7, add = &quot;mean_ci&quot;, point.size = 2, xlab = &quot;Momento&quot; , ylab = &quot;PEF (L/min) &quot;, ggtheme = theme_bw()) + theme (text = element_text (size = 13), axis.text.x= element_text(size = 12)) Figura17.4: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos. 17.5.1.4 Criação de uma variável que represente a diferença entre os momentos A diferença entre as média basal e final será atribuída ao nome D. Esta ação será realizada, utilizando o banco de dados amplo (dados): dados$D &lt;- dados$basal - dados$final head (dados) ## # A tibble: 6 × 4 ## id basal final D ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 120 220 -100 ## 2 2 200 300 -100 ## 3 3 140 230 -90 ## 4 4 200 180 20 ## 5 5 110 300 -190 ## 6 6 240 330 -90 Resumo da variável D Ao resumo será atribuído ao nome resumo2: resumo2 &lt;- dados %&gt;% dplyr::summarise(n = n (), mediana = median (D, na.rm = TRUE), p25=quantile(D, probs = 0.25, na.rm = TRUE), p75=quantile(D, probs = 0.75, na.rm = TRUE)) resumo2 ## # A tibble: 1 × 4 ## n mediana p25 p75 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10 -90 -100 -57.5 O sinal negativo demonstra que houve um aumento do PFM do momento basal para o final. 17.5.2 Definição das hipóteses estatísticas Da mesma maneira que o teste t pareado, as hipóteses estabelecidas comparam dois grupos dependentes. O teste de Wilcoxon é usado para avaliar a hipótese nula de que a distribuição das diferenças entre os grupos tem uma diferença mediana igual a 0. \\(H_{0}\\): \\(D_{i} = 0\\) . \\(H_{A}\\): \\(D_{i} \\ne 0\\). Note que a \\(H_{A}\\) estabelece que a diferença pode aumentar ou diminuir. Logo, o teste é bicaudal. 17.5.3 Execução do teste estatístico 17.5.3.1 Lógica do teste de Wilcoxon A ideia do teste é verificar se as diferenças positivas são maiores ou menores, em grandeza absoluta, que as diferenças negativas. Para isso, foi criada, anteriormente, a variável D. Agora, será criada outra variável, iguala a variável D, apenas ignorando o sinal, denominada D_abs, diferença absoluta entre as variáveis final e basal. dados$D_abs &lt;- abs(dados$basal - dados$final) Excluir os casos com diferença igual a 0 (zero). Para isso, uma maneira possível é extrair um subconjunto de dados do conjunto principal (dados), criando um conjunto de dados com a função filter() do pacote dplyr, que receberá o nome de dados1. O argumento D_absbs != 0 significa todas as diferenças absolutas diferentes de 0: dados1 &lt;- dados %&gt;% dplyr::filter(D_abs != 0) Observe que como não há diferenças zeradas. Ou seja, o novo conjunto de dados continua o mesmo. O que pode ser confirmado, executando a função str(): str (dados1) ## tibble [10 × 5] (S3: tbl_df/tbl/data.frame) ## $ id : int [1:10] 1 2 3 4 5 6 7 8 9 10 ## $ basal: num [1:10] 120 200 140 200 110 240 150 120 250 190 ## $ final: num [1:10] 220 300 230 180 300 330 230 250 300 200 ## $ D : num [1:10] -100 -100 -90 20 -190 -90 -80 -130 -50 -10 ## $ D_abs: num [1:10] 100 100 90 20 190 90 80 130 50 10 Ordenar de forma crescente todos os valores da variável D_abs do banco de dados dados1, usando a função arrange() do pacote dplyr: dados1 &lt;- dados1 %&gt;% dplyr::arrange(dados1$D_abs) Estabelecer postos para os valores ordenados da variável D_abs, do conjunto de dados dados1, fazendo a média das ordens quando houver empate. A execução deste comando cria uma nova variável, chamada postos: dados1$postos &lt;- rank(dados1$D_abs) Estabelecer sinais para os postos, criando dois subconjuntos de dados do conjuntos dados1, um com os escolares com postos positivos (pos) e outros com postos negativos(neg): neg &lt;- dados1 %&gt;% dplyr::filter(D &lt; 0) pos &lt;- dados1 %&gt;% dplyr::filter(D &gt; 0) Somar todos os postos (variável posto) em cada um dos subconjuntos criados (neg e pos): soma_neg &lt;- sum(neg$postos) soma_pos &lt;- sum(pos$postos) print (c(soma_neg, soma_pos)) ## [1] 53 2 Atribuir a menor soma à estatística do teste, denotada T: T &lt;- min (soma_neg : soma_pos) T ## [1] 2 Para dados com tamanhos grandes (&gt; 20 pares), a significância de T pode ser determinada (148), considerando que a distribuição de T tem aproximadamente distribuição normal com média igual a \\[ \\mu_{T} =\\frac{n \\times \\left (n + 1 \\right )}{4} \\] onde n é o tamanho da amostra. n &lt;- length(dados$D) mu_T &lt;- (n * (n + 1))/4 mu_T ## [1] 27.5 E desvio padrão igual a: \\[ \\sigma_{T}= \\sqrt {\\frac{n\\left (n + 1 \\right )\\times \\left (2n + 1 \\right )}{24}} \\] dp_T &lt;- sqrt ((n*(n + 1)) * (2 * n + 1) /24) dp_T ## [1] 9.810708 Os resultados da execução das equações fornecem os dados para calcular a estatística Z_T com correção de continuidade e, a partir dela, calcular o valor P. \\[ Z_{T}= \\frac{\\left |T - \\mu_{T} \\right | - 0,5}{\\sigma_{T}} \\] Z_T &lt;- (abs(T - mu_T)- 0.5)/dp_T Z_T ## [1] 2.548236 Concluindo, o valor da estatística de teste T é superior ao \\(Z_{crítico} = 1,96\\), para um \\(\\alpha = 0,05\\). Dessa forma, a \\(H_{0}\\) é rejeitada. Existe uma diferença significativa entre o PFE basal e o PFE final, neste grupo de escolares asmáticos. O valor P pode ser obtido com a função pnorm() e multiplicando o resultado por 2, pois o teste é bilateral. P &lt;- pnorm (Z_T, lower.tail = FALSE) * 2 P ## [1] 0.01082692 Como já dito anteriormente, na prática, não há necessidade de fazer todos esses cálculos, pois o R calcula facilmente o teste. Eles são apenas uma demonstração de como o teste funciona. 17.5.3.2 Cálculo do teste de Wilcoxon no R Usando o conjunto de dados no formato longo (dadosL), calcula-se o teste com a função wilcox_test() do pacote rstatix. É a mesma função utilizada para o teste de U de Mann-Whitney, mudando apenas o argumento paired=FALSE para paired=TRUE: teste1 &lt;- dadosL %&gt;% rstatix::wilcox_test(medidas ~ momento, paired = TRUE) %&gt;% rstatix::add_significance() teste1 ## # A tibble: 1 × 8 ## .y. group1 group2 n1 n2 statistic p p.signif ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 medidas basal final 10 10 2 0.0107 * Observe que o resultado é o mesmo calculado manualmente. 17.5.4 Tamanho do efeito O tamanho do efeito pode ser calculado da mesma forma que para o teste de Mann-Whitney (Seção 17.4.5), usando a mesma equação e os dados obtidos acima, onde 2.548236 e n = 10 tem-se \\[ r = \\frac {Z_{T}}{\\sqrt{n}} \\] Pode-se usar também a função wilcox_effsize() para calcular a estatística r. A Saída exibe junto a magnitude o efeito, que no caso é grande (&gt; 0,5 como mostra a Tabela 17.2 do teste de Mann-Whitney). dadosL %&gt;% wilcox_effsize(medidas ~ momento, paired = TRUE) ## # A tibble: 1 × 7 ## .y. group1 group2 effsize n1 n2 magnitude ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt; ## 1 medidas basal final 0.823 10 10 large 17.5.5 Conclusão Assumindo um \\(\\alpha = 0,05\\), se o valor P, obtido pelo teste, for menor do que 0,05, rejeita-se a hipótese nula (V = 2, P = 0,01, n = 10). Pode-se concluir que existe diferença nas medidas do pico de fluxo expiratório máximo no início e no fim do programa de exercícios aeróbicos realizados pelos escolares asmáticos e a magnitude do efeito foi grande (r = 0,82). Isto pode ser visualizado na Figura 17.5: bxp &lt;- ggpubr::ggboxplot(dadosL, x = &quot;momento&quot;, y = &quot;medidas&quot;, bxp.errorbar = TRUE, bxp.errorbar.width = 0.1, fill = &quot;momento&quot;, palette = c(&quot;cyan4&quot;, &quot;cyan3&quot;), legend = &quot;none&quot;, ggtheme = theme_bw(), xlab = &quot;Momento&quot; , ylab = &quot;PEF (L/min) &quot;) + theme (text = element_text (size = 13), axis.text.x = element_text(size = 11)) teste &lt;- dadosL %&gt;% rstatix::wilcox_test (medidas ~ momento, paired = TRUE) %&gt;% rstatix::add_significance () teste &lt;- teste %&gt;% rstatix::add_xy_position () bxp + stat_pvalue_manual (teste, tip.length = 0) + labs (subtitle = get_test_label (stat.test = teste, detailed = TRUE)) Figura17.5: Impacto de exercícios aeróbicos na função respiratória de 10 escolares asmáticos. 17.6 Teste de Kruskal-Wallis Quando os pressupostos subjacentes a ANOVA não são atendidos, é possível usar o teste não paramétrico de Kruskal-Wallis (KW) para testar a hipótese de que os parâmetros de localização são iguais. Pode ser considerado uma extensão do teste de Wilcoxon-Mann-Whitney. Enquanto a ANOVA depende da hipótese de que todas as populações são independentes e normalmente distribuídas, o teste de Kruskal-Wallis exige apenas amostras aleatórias independentes provenientes de suas respectivas populações. Entretanto, este teste somente deve ser aplicado se a amostra for pequena e/ou os pressupostos para a ANOVA forem seriamente violados. O teste não usa diretamente medições de quantidade conhecida, utiliza, como outros testes não paramétricos, os postos dos valores analisados. Em função disso, é também conhecido como análise de variância de um fator em postos. 17.6.1 Dados usados nesta seção Um experimento foi realizado para verificar se o álcool ou o café afetam os tempos de reação ao dirigir (149). O estudo tem três grupos diferentes de participantes: 10 bebendo água (controle), 10 bebendo cerveja contendo duas unidades de álcool e 10 bebendo café. O tempo de reação em uma simulação de direção foi medido para cada participante. Os dados encontram-se no arquivo dadosResposta.xlsx. Clique aqui para baixar e, após, salve o mesmo no seu diretório de trabalho. As variáveis são: id \\(\\to\\) identificação do participante; tempo \\(\\to\\) tempo de reação na simulação de direção em segundos; bebida \\(\\to\\) três grupo: água, álcool e café. O estudo pretende verificar se existe diferença no tempo de reação dos participantes em um teste de direção com a ingesta de água, café e álcool. 17.6.1.1 Leitura e exploração dos dados Como o dados estão contidos em um arquivo Excel (.xlsx), serão lidos com a função read_excel() do pacote readxl e a sua estrutura será observada com função str(): dados &lt;- read_excel (&quot;Arquivos/dadosResposta.xlsx&quot;) str(dados) ## tibble [30 × 3] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:30] 1 2 3 4 5 6 7 8 9 10 ... ## $ tempo : num [1:30] 0.37 0.38 0.61 0.78 0.83 0.86 0.9 0.95 1.63 1.97 ... ## $ bebida: chr [1:30] &quot;agua&quot; &quot;agua&quot; &quot;agua&quot; &quot;agua&quot; ... O formato do arquivo é o longo. A variável bebida encontra-se como caracter e deve ser transformada em fator e as categorias na sequência: agua, cafe e alcool. dados$bebida &lt;- factor(dados$bebida, levels = c(&quot;agua&quot;, &quot;cafe&quot;, &quot;alcool&quot;)) Os dados serão observados visualmente através de boxplots (Figura 17.6), usando a função ggplot() do pacote ggplot2, com cores do nejm (New England Journal of Medicine) o pacote ggsci. ggpubr::ggboxplot(dados, x = &quot;bebida&quot;, y = &quot;tempo&quot;, bxp.errorbar = T, bxp.errorbar.width = 0.1, fill = &quot;bebida&quot;, palette = &quot;nejm&quot;, legend = &quot;none&quot;, ggtheme = theme_bw(), xlab = &quot;Tipo de bebida&quot; , ylab = &quot;Tempo de reação (seg)&quot;) + theme(text = element_text(size = 12)) Figura17.6: Impacto do tipo de bebida no tempo de reação ao dirigir. Os boxplots exibem dados com medianas visualmente diferentes, bigodes diferentes e grupos com presença de outliers. Para verificar o impacto desses achados, pode-se usar a função identify_outliers(), do pacote rstatix que confirma na sua Saída a presença de outliers no grupo agua e cafe, sendo dois extremos. dados %&gt;% dplyr::group_by(bebida) %&gt;% rstatix::identify_outliers(tempo) ## # A tibble: 4 × 5 ## bebida id tempo is.outlier is.extreme ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 agua 9 1.63 TRUE FALSE ## 2 agua 10 1.97 TRUE TRUE ## 3 cafe 19 2.56 TRUE FALSE ## 4 cafe 20 3.07 TRUE TRUE Para avaliar a normalidade será usado o teste de Shapiro-Wilk, com a função shapiro_test() e a função group_by() do pacote dplyr: dados %&gt;% dplyr::group_by (bebida) %&gt;% rstatix::shapiro_test (tempo) ## # A tibble: 3 × 4 ## bebida variable statistic p ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 agua tempo 0.863 0.0837 ## 2 cafe tempo 0.815 0.0220 ## 3 alcool tempo 0.875 0.114 A variável cafe tem uma distribuição que não se ajusta a distribuição normal. Para completar a exploração dos dados, será solicitado, usando as funções group_by () e summarise, do pacote dplyr, medidas de localização e dispersão adquadas para variáveis bem assimétricas. resumo &lt;- dados %&gt;% dplyr::group_by(bebida) %&gt;% dplyr::summarise(n = n(), mediana = median (tempo, na.rm = TRUE), p25=quantile(tempo, probs = 0.25, na.rm = TRUE), p75=quantile(tempo, probs = 0.75, na.rm = TRUE)) resumo ## # A tibble: 3 × 5 ## bebida n mediana p25 p75 ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 agua 10 0.845 0.653 0.937 ## 2 cafe 10 1.44 1.28 1.68 ## 3 alcool 10 2.25 1.77 2.85 17.6.2 Hipóteses estatísticas Se não houver diferença entre os grupos, ou seja, os grupos são provenientes de uma mesma população, as somas dos postos em cada grupo devem ficar próximas. Desta forma, \\(H_{0}\\): As populações são iguais. \\(H_{1}\\): Pelo menos uma das populações tende a exibir valores diferentes do que as outras populações. 17.6.3 Pressupostos do teste O teste de Kruskal-Wallis pressupõe as seguintes condições para o seu adequado uso: As amostras são amostras aleatórias independentes de suas respectivas populações; A escala de medição utilizada é pelo menos ordinal e, se houver apenas três grupos, deve haver pelo menos 5 casos em cada grupo; As distribuições dos valores nas populações amostradas são idênticas, exceto pela possibilidade de que uma ou mais das populações sejam compostas por valores que tendem a ser maiores do que os das outras populações. 17.6.4 Execução do teste estatístico 17.6.4.1 Lógica do teste de Kruskall-Wallis A teoria do teste Kruskal-Wallis é semelhante à do teste de Mann-Whitney, ou seja, tem como base a soma dos postos. Em primeiro lugar, os escores são ordenados do menor para o maior, independentemente do grupo que pertençam. O menor recebe o posto 1 e assim por diante. Após a atribuição dos postos, soma-se os postos por grupo. A soma dos postos de cada grupo é representada por \\(R_{1}\\), \\(R_{2}\\), \\(R_{3}\\), …, \\(R_{i}\\). A estatística do teste, H, é calculada com a equação (150): \\[ H =\\frac {12}{N \\times \\left (N + 1 \\right )} \\sum_{i=1}^{k} \\frac {R_{i}^{2}}{n_{{i}}}-3 \\times\\left (N + 1\\right) \\] onde \\(n_{i}\\) é o número de observações no grupo i, \\(N = \\sum_{i=1}^{k}\\times n_{i}\\) (o número total de observações em todos os k grupos) e \\(R_{i}\\) é a soma dos postos das \\(n_{i}\\) observações no grupo i. Uma boa verificação (mas não uma garantia) de que os postos foram atribuídos corretamente é ver se a soma de todos os postos é igual a \\(\\frac {N \\times \\left (N + 1\\right )}{2}\\). Criar a variável posto com os postos ordenados de forma crescente, independente do grupo, como realizado no teste de Mann-Whitney: dados$posto &lt;- rank(dados$tempo, ties.method = &quot;average&quot;) str(dados) ## tibble [30 × 4] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:30] 1 2 3 4 5 6 7 8 9 10 ... ## $ tempo : num [1:30] 0.37 0.38 0.61 0.78 0.83 0.86 0.9 0.95 1.63 1.97 ... ## $ bebida: Factor w/ 3 levels &quot;agua&quot;,&quot;cafe&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ posto : num [1:30] 1 2 3 4 5 6 7 8 16 22.5 ... Somar os postos de cada grupo separadamente: resumo1 &lt;- dados %&gt;% dplyr::group_by(bebida) %&gt;% dplyr::summarise(n = n(), soma = sum(posto)) resumo1 ## # A tibble: 3 × 3 ## bebida n soma ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 agua 10 74.5 ## 2 cafe 10 157 ## 3 alcool 10 234. Cálculo da estatística do teste H N &lt;- 30 n &lt;- 10 R_agua &lt;- resumo1[1,3] R_alcool &lt;- resumo1[2,3] R_cafe &lt;- resumo1[3,3] H &lt;- (12/(N*(N+1))) * ((R_agua^2/n) + (R_alcool^2/n) + (R_cafe^2/n)) - (3*(N+1)) H ## soma ## 1 16.31806 Cálculo do Valor P Se existir três grupos, com cinco ou menos participantes em cada grupo, há necessidade de usar a tabela especial para tamanhos de amostra pequenos (151). Se você tiver mais de cinco participantes por grupo, trate H como qui-quadrado. A estatística H é estatisticamente significativo se for igual ou maior que o valor crítico qui-quadrado para o grau de liberdade específico, igual a \\(k - 1\\). Aqui, tem-se 10 participantes por grupo e, assumindo um \\(\\alpha = 0,05\\), o \\(H_{crítico}\\) é igual a: alpha &lt;- 0.05 k &lt;- 3 gl = k - 1 H_critico &lt;- qchisq(1 - alpha, gl) H_critico ## [1] 5.991465 Uma vez que o \\(H_{calculado} = 16,3\\) é maior que \\(H_{crítico} = 6,0\\) , rejeita-se a \\(H_{0}\\). O valor Pé obtido através da função pchisq(): H &lt;- 16.32 pchisq(H, 2, lower.tail = FALSE) ## [1] 0.0002858624 O R tem funções que fazem facilmente esses cálculos enfadonhos. Eles são colocados aqui apenas para ilustrar o raciocínio de como o teste de Kruskal-Wallis funciona. Sempre existem curiosos lendo o livro! 17.6.4.2 Teste de Kruskal-Wallis no R No R, pode-se calcular o teste, usando a função kruskal_test() do pacote rstatix, cujos argumentos podem ser consultados na ajuda do RStudio. teste &lt;- rstatix::kruskal_test (data = dados, formula = tempo ~ bebida) teste ## # A tibble: 1 × 6 ## .y. n statistic df p method ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 tempo 30 16.3 2 0.000286 Kruskal-Wallis 17.6.5 Tamanho do efeito O eta quadrado (\\(\\eta^{2}\\)), com base na estatística H, pode ser usado como a medida do tamanho do efeito do teste de Kruskal-Wallis. É calculado pela equação: \\[ \\eta_{H}^{2} = \\frac {\\left (H - k + 1 \\right)}{\\left (N - k\\right)} \\] onde H é a estatística obtida no teste de Kruskal-Wallis; k é o número de grupos; N é o número total de observações (152). A estimativa eta ao quadrado assume valores de 0 a 1 e, multiplicada por 100, indica a porcentagem de variância na variável dependente explicada pela variável independente. Pode ser obtido no R com a função kruskal_effsize() do pacote rstatix: dados %&gt;% kruskal_effsize (tempo~bebida) ## # A tibble: 1 × 5 ## .y. n effsize method magnitude ## * &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;ord&gt; ## 1 tempo 30 0.530 eta2[H] large Um efeito \\(\\ge 0,14\\) é considerado grande e \\(&lt;0,06\\) é pequeno (117). 17.6.6 Testes post hoc A partir do resultado do teste de Kruskal-Wallis, sabe-se que há uma diferença significativa entre os grupos, mas não se sabe quais pares de grupos são diferentes. Um teste de Kruskal-Wallis significativo é geralmente seguido pelo teste de Dunn (153) para identificar quais grupos são diferentes. Para realizar as múltiplas comparações, no R, pode ser usada a função dunn_test(), incluído no pacote rstatix. O ajuste de P é feito pelo método de Bonferroni: pwc &lt;- dados %&gt;% dunn_test (tempo ~ bebida, p.adjust.method = &quot;bonferroni&quot;) pwc ## # A tibble: 3 × 9 ## .y. group1 group2 n1 n2 statistic p p.adj p.adj.signif ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 tempo agua cafe 10 10 2.10 0.0361 0.108 ns ## 2 tempo agua alcool 10 10 4.04 0.0000537 0.000161 *** ## 3 tempo cafe alcool 10 10 1.94 0.0520 0.156 ns A saída do teste de Dunn, mostra que existe uma diferença estatisticamente significativa apenas entre a água e o álcool. 17.6.7 Conclusão Um teste de Kruskal-Wallis foi realizado para comparar os tempos de reação em uma simulação de direção após beber água, café ou álcool. Houve evidência de uma diferença (P = 0,00029) de pelo menos um par de grupos (Figura 17.7). O teste de comparações de pares, usando o teste de Dunn, foi realizado para os três pares de grupos. Houve evidencia de diferença entre o grupo que consumiu duas unidades de álcool e o grupo que ingeriu água (P ajustado (Bonferroni) = 0,00016). Entre os demais pares não houve diferença significativa. O tempo mediano de reação para o grupo que recebeu água foi de 0,84 (0,65 – 0,94) segundos, em comparação com 2,25(1,77 – 2,85) segundos no grupo que bebeu cerveja equivalente a duas unidades de álcool, enquanto para o café foi de 1,45(1,28 – 1,69) segundos. pwc &lt;- pwc %&gt;% rstatix::add_xy_position(x= &quot;bebida&quot;) ggpubr::ggboxplot(dados, x = &quot;bebida&quot;, y = &quot;tempo&quot;, bxp.errorbar = TRUE, bxp.errorbar.width = 0.1, fill = &quot;bebida&quot;, palette = &quot;nejm&quot;, legend = &quot;none&quot;, ggtheme = theme_bw())+ ggpubr::stat_pvalue_manual (pwc, label = &quot;p = {scales::pvalue(p.adj)}&quot;, label.size = 3.2, hide.ns = FALSE) + ggplot2::labs(x = &quot;Tipo de bebida&quot;, y = &quot;Tempo de reação (seg)&quot;, subtitle = get_test_label (teste, detailed = TRUE), caption = get_pwc_label(pwc)) Figura17.7: Impacto do tipo de bebida no tempo de reação ao dirigir. O cálculo pode também ser realizado, usando a função wilcox.test() do pacote stats, incluído no R base.↩︎ Para maiores detalhes consulte a ajuda da função.↩︎ "],["sec-cap18.html", "Capítulo 18 Estatística em Epidemiologia 18.1 Pacotes necessários 18.2 Raciocínio bayesiano no diagnóstico médico 18.3 Estatística kappa 18.4 Medidas de frequência 18.5 Medidas de associação 18.6 Medidas de impacto 18.7 Análise de sobrevida 18.8 Regressão logística binária", " Capítulo 18 Estatística em Epidemiologia 18.1 Pacotes necessários pacman::p_load(BiocManager, car, cowplot, DescTools, dplyr, epiR, epitools, ggplot2, gmodels, kableExtra, MASS, MKmisc, mlbench, pROC, readxl, survival, survminer, vcd) O pacote limma também vai ser usado indiretamente neste capítulo e a sua instalação é feita através do BiocManager da seguinte maneira: BiocManager::install(&quot;limma&quot;) ## Bioconductor version 3.17 (BiocManager 1.30.22), R 4.3.2 (2023-10-31 ucrt) ## Warning: package(s) not installed when version(s) same as or greater than current; use ## `force = TRUE` to re-install: &#39;limma&#39; ## Installation paths not writeable, unable to update packages ## path: C:/Program Files/R/R-4.3.2/library ## packages: ## lattice ## Old packages: &#39;BiasedUrn&#39;, &#39;bookdown&#39;, &#39;checkmate&#39;, &#39;classInt&#39;, &#39;covr&#39;, ## &#39;crosstable&#39;, &#39;curl&#39;, &#39;dbplyr&#39;, &#39;dotCall64&#39;, &#39;emmeans&#39;, &#39;evaluate&#39;, &#39;fansi&#39;, ## &#39;flextable&#39;, &#39;gdata&#39;, &#39;gdtools&#39;, &#39;ggeffects&#39;, &#39;ggpmisc&#39;, &#39;ggpp&#39;, ## &#39;GPArotation&#39;, &#39;htmlTable&#39;, &#39;htmltools&#39;, &#39;httpuv&#39;, &#39;httr2&#39;, &#39;knitr&#39;, &#39;lessR&#39;, ## &#39;lifecycle&#39;, &#39;lme4&#39;, &#39;magick&#39;, &#39;maps&#39;, &#39;markdown&#39;, &#39;Matrix&#39;, &#39;MatrixModels&#39;, ## &#39;matrixStats&#39;, &#39;officer&#39;, &#39;openssl&#39;, &#39;parameters&#39;, &#39;performance&#39;, &#39;plotly&#39;, ## &#39;plotrix&#39;, &#39;pracma&#39;, &#39;pROC&#39;, &#39;promises&#39;, &#39;purrr&#39;, &#39;ragg&#39;, &#39;rbibutils&#39;, ## &#39;RcppArmadillo&#39;, &#39;RcppEigen&#39;, &#39;RCurl&#39;, &#39;Rdpack&#39;, &#39;rlang&#39;, &#39;rprojroot&#39;, ## &#39;RSQLite&#39;, &#39;RVAideMemoire&#39;, &#39;sp&#39;, &#39;spam&#39;, &#39;stringi&#39;, &#39;stringr&#39;, &#39;svglite&#39;, ## &#39;systemfonts&#39;, &#39;textshaping&#39;, &#39;units&#39;, &#39;utf8&#39;, &#39;uuid&#39;, &#39;vctrs&#39;, &#39;waldo&#39;, ## &#39;wesanderson&#39;, &#39;withr&#39;, &#39;wk&#39;, &#39;WRS2&#39;, &#39;xfun&#39;, &#39;XML&#39; library (limma) 18.2 Raciocínio bayesiano no diagnóstico médico O processo diagnóstico é o centro da atenção da atividade médica na busca de reduzir as incertezas e reconhecer a que classe pertence determinado paciente. Portanto, é extremamente importante saber quão bem os testes diagnósticos podem prever que um indivíduo é portador de certa condição ou doença. Entende-se aqui como teste diagnóstico todo o processo diagnótico, desde o exame clínico até o mais sofisticado exame de imagem ou laboratorial. A ideia é saber como o teste diagnóstico se comporta para separar um “doente” e um “não doente”; qual a sua validade neste processo? Deve-se sempre ter em mente que o estabelecimento do diagnóstico é um processo imperfeito que resulta em uma probabilidade ao invés de uma certeza de estar correto. Ou seja, cada vez mais os médicos têm que aplicar as leis da probabilidade na avaliação de testes diagnósticos e sinais clínicos. A abordagem bayesiana denomina de probabilidade a priori a probabilidade estabelecida inicialmente, baseada apenas na experiência do médico, em seu conhecimento em relação a doença suspeitada. Diante de uma evidência de doença, pode ser solictado um teste diagnóstico. Quando ele recebe um teste positivo para uma doença, a probabilidade muda, passa a ser uma probabilidade condicional, probabilidade da doença dado que o teste é positivo, denominada probabilidade a posteriori. Um teste que define corretamente quem é doente e quem não é doente é denominado de padrão-ouro ou padrão de referência. Algumas vezes, o teste padrão de referência é simples e barato. Outras vezes, é caro, difícil de obter, tecnicamente complexo, arriscado ou pouco prático. Algumas vezes, não há padrão-ouro. Em função dessas limitações, outros testes são usados e, como consequência, podem ocorrer erros. Em outras palavras, no processo diagnóstico podem ocorrer falsos positivos e falsos negativos. Esta incerteza, na utilização de testes diagnósticos, gera a necessidade de o médico conferir a probabilidade de falsos positivos e falsos negativos na elaboração de um diagnóstico ao receber o resultado positivo ou negativo de um exame. Uma maneira simples de mostrar as relações de um teste diagnóstico e o verdadeiro diagnóstico, é mostrada na tabela de contingência \\(2\\times2\\) (Figura 18.1). Figura18.1: Falsos positivos e falsos negativos 18.2.1 Sensibilidade e Especificidade As estatísticas mais utilizadas para descrever a validade dos testes de diagnóstico em contextos clínicos são sensibilidade e a especificidade. Sensibilidade é a habilidade do teste em identificar corretamente quem tem a doença. É a taxa de verdadeiros positivos (VP) de um teste e corresponde a probabilidade de um indivíduo com a doença ter um teste positivo. Um teste sensível raramente deixará passar pessoas que tenham a doença. Testes com sensibilidade alta são úteis para excluir a presença de uma doença. Isto é, um teste negativo exclui virtualmente a possibilidade de o paciente ter a doença de interesse, pois tem pouca probabilidade de produzir resultados falsos negativos. Isto pode ser lembrado pelo mnemônico SnNout, do inglês: High Sensivity, a Negative result rules out the diagnosis (154). Especificidade é a habilidade do teste em identificar corretamente quem não tem a doença. É a taxa de verdadeiros negativos (VN) de um teste e corresponde a probabilidade de um indivíduo sem a doença ter um teste negativo. Um teste específico raramente classificará de forma errônea indivíduos sendo portadores da doença quando eles não são. Os testes muito específicos são usados para confirmar a presença da doença. Se o teste é altamente específico, um teste positivo sugere fortemente a presença da doença de interesse. De forma similar que a sensibilidade pode-se usar o mnemônico SpPin, do inglês: High Specificity, a Positive result rules in the diagnosis (154). Estas estatísticas de diagnóstico podem ser calculadas a partir das equações, cujas letras representam as caselas da tabela \\(2 \\times 2\\), acima; \\[ Sensibilidade = \\frac {a}{\\left (a + c\\right )} \\quad \\quad Especificidade = \\frac {d}{\\left (b + d\\right )} \\] A taxa de falsos negativos (TFN) é a proporção de indivíduos que têm a doença e que têm um resultado de teste negativo e a taxa de falsos positivos (TFP) é a proporção de pacientes que não possuem a doença e que apresentam resultados positivos. Podem ser expressas pelas equações: \\[ TFN= \\frac {c}{\\left (a + c\\right )} \\quad ou \\quad \\left (1 - sensibilidade\\right) \\] \\[ TFP= \\frac {b}{\\left (b + d\\right )} \\quad ou \\quad \\left (1 - especifcidade\\right) \\] Idealmente, um teste de diagnóstico deveria ter altos níveis de sensibilidade e especificidade. No entanto, isso não é possível, pois existe um balanço entre sensibilidade e especificidade. À medida que a especificidade aumenta, a sensibilidade diminui e vice-versa. As curvas ROC, que serão discutidas mais adiante neste capítulo, podem ser usadas para identificar um ponto de corte em uma medição contínua que maximize a sensibilidade e a especificidade. Quando um clínico tem um paciente cujo teste apresentou resultado positivo, a pergunta mais importante é a seguinte: dado que o teste é positivo, qual é a probabilidade de o paciente ter a doença? A sensibilidade do teste não responde a este questionamento, mas sim a probabilidade de um resultado positivo, dado que o paciente tem a doença (155). 18.2.1.1 Exemplo O conjunto de dados dadosApendicite.xlsx contém informações de 156 pacientes que realizaram ultrassonografia abdominal para o diagnóstico de apendicite aguda.Para obter arquivo, clique aqui e salve o mesmo em seu diretório de trabalho. Foram avaliados pacientes com diagnóstico clínico de apendicite aguda, submetidos à ultrassonografia abdominal e apendicectomia laparoscópica, acompanhado de estudo anatomopatológico dos apêndices extirpados (156). Será avaliado o teste diagnóstico usado. Leitura e observação do conjunto de dados Será usado a função read_excel()do pacote readxl e a função glimpse() do pacote dplyr: dados &lt;- readxl::read_excel (&quot;Arquivos/dadosApendicite.xlsx&quot;) glimpse(dados) ## Rows: 156 ## Columns: 3 ## $ id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, … ## $ apendicite &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ eco &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… As variáveis apendicite e eco foram exibidas como variáveis numéricas e serão transformadas em fatores: dados$apendicite &lt;- factor(dados$apendicite, levels = c(1,2), labels = c(&quot;Presente&quot;, &quot;Ausente&quot;)) dados$eco &lt;- factor(dados$eco, levels = c(1,2), labels = c(&quot;Positivo&quot;, &quot;Negativo&quot;)) Construção de uma tabela de contingência \\(2\\times2\\) tab_ap &lt;- table (dados$eco, dados$apendicite, dnn = c (&quot;Eco&quot;, &quot;Apendicite&quot;)) tab_ap ## Apendicite ## Eco Presente Ausente ## Positivo 85 7 ## Negativo 46 18 Cálculo da sensibilidade e da especificidade Pode-se usar a função epi.tests() do pacote epiR (157) que calcula, junto com os intervalos de confiança, a prevalência aparente e verdadeira, sensibilidade, especificidade, valores preditivos positivos e negativos e razões de probabilidade positivas e negativas a partir de dados de contagem fornecidos em uma tabela \\(2\\times2\\). Utiliza os argumentos dat   dados sob a forma de vetor ou matriz conf.level   magnitude do intervalode confiança, entre 0 e 1. Os resultados serão atribuídos a um objeto de nome diag: diag &lt;- epiR::epi.tests(tab_ap, conf.level = 0.95) print(diag) ## Outcome + Outcome - Total ## Test + 85 7 92 ## Test - 46 18 64 ## Total 131 25 156 ## ## Point estimates and 95% CIs: ## -------------------------------------------------------------- ## Apparent prevalence * 0.59 (0.51, 0.67) ## True prevalence * 0.84 (0.77, 0.89) ## Sensitivity * 0.65 (0.56, 0.73) ## Specificity * 0.72 (0.51, 0.88) ## Positive predictive value * 0.92 (0.85, 0.97) ## Negative predictive value * 0.28 (0.18, 0.41) ## Positive likelihood ratio 2.32 (1.22, 4.40) ## Negative likelihood ratio 0.49 (0.35, 0.68) ## False T+ proportion for true D- * 0.28 (0.12, 0.49) ## False T- proportion for true D+ * 0.35 (0.27, 0.44) ## False T+ proportion for T+ * 0.08 (0.03, 0.15) ## False T- proportion for T- * 0.72 (0.59, 0.82) ## Correctly classified proportion * 0.66 (0.58, 0.73) ## -------------------------------------------------------------- ## * Exact CIs Assim, a sensibilidade é igual a 65% (IC95%: 56 – 73%) e a especificidade é igual a 72% (IC95%: 51 – 88%). Isto significa que um indivíduo com apendicite aguda tem 65% de probabilidade de ter uma ecografia alterada; um indivíduo sem apendicite aguda tem 72% de probabilidade de ter uma ecografia normal. O objetivo do teste de diagnóstico é usá-lo para fazer um diagnóstico, então há necessidade de saber a probabilidade que o teste fornece para um diagnóstico correto. A sensibilidade e a especificidade não fornecem esta informação. Para atingir esse objetivo, usa-se o valor preditivo (158). 18.2.2 Valor Preditivo O propósito de um teste diagnóstico é usar seus resultados para fazer um diagnóstico, portanto, é necessário conhecer a probabilidade de que o resultado do teste forneça o diagnóstico correto (158). Os valores preditivos positivo e negativo descrevem a probabilidade de um paciente ter doença, uma vez que os resultados de seus testes são conhecidos. O valor preditivo positivo (VPP) de um teste é definido como a proporção de pessoas com um resultado de teste positivo que realmente têm a doença. O valor preditivo negativo (VPN) é a proporção de pacientes com resultados de teste negativos que não têm doença. Como a sensibilidade e a especificidade, estas estatísticas de diagnóstico também podem ser calculadas a partir da tabela \\(2\\times2\\), mostrada no início: \\[ VPP = \\frac {a}{\\left (a + b\\right )} \\quad \\quad VPN = \\frac {d}{\\left (c + d\\right )} \\] Observando os resultados anteriores da função epi.tests(), verifica-se que 92% (85/92) dos indivíduos que tiveram teste positivo (ultrassonografia alterada) tinham doença (apendicite aguda). Isso significa que seu VPP é igual a 92% (IC95%: 18 – 41%), ou dito de outra forma, uma pessoa com ultrassonografia positiva tem 92% de probabilidade de ter a apendicite aguda. O VPP é também conhecido como probabilidade pós-teste de doença dado um teste positivo. Dos 64 pacientes que tiveram ultrassonografia sem alterações, 18 não apresentaram apendicite aguda, portanto, um VPN de 28% (IC95%: 56 – 73%). Isso significa que uma pessoa quem tem um teste negativo tem 28,1% de probabilidade de não ter apendicite aguda. Entretanto, essas proporções são de validade limitada. Os valores preditivos de um teste, na prática clínica, dependem criticamente da prevalência da anormalidade nos pacientes testados. No estudo, a prevalência de apendicite aguda é igual a \\[ \\frac {total\\ de\\ casos\\ de \\ apendicite \\ aguda}{total\\ de\\ casos\\ no\\ estudo} = \\frac {131}{156} = 0,84\\ ou\\ 84\\% \\left(IC_{95\\%}:77\\ a\\ 89\\%\\right) \\] Levando-se em consideração que a prevalência de apendicite aguda na população é de 7% (159), mantendo a sensibilidade (64%) e a especificidade (72%) da ultrassonografia, entre 156 pacientes, selecionados aleatoriamente, se esperaria encontrar aproximadamente 11 casos (7% de 156) de apendicite aguda. Para facilitar a compreensão, observe a a tabela \\(2\\times2\\) (Figura 18.2): Figura18.2: Prevalencia e valor preditivo O VPP e o VPN são iguais a: a &lt;- 7 b &lt;- 41 c &lt;- 4 d &lt;- 104 vpp = a/(a + b) round(vpp, 3)*100 ## [1] 14.6 vpn = d/(c + d) round(vpn, 3)*100 ## [1] 96.3 Ao se comparar o VPP obtido, agora, com o VPP do estudo, observa-se que o mesmo diminuiu bastante, de 92% para 14,6%. O contrário ocorre com a VPN que aumenta substancialmente de 28% para 96,3%, mostrando claramente a influência da prevalência. Se a prevalência diminui, o VPP diminui e o VPN aumenta. Portanto, será errado aplicar diretamente os valores preditivos publicados de um teste ao seu pacciente, quando a prevalência da doença em sua população for diferente da prevalência da doença na população em que o estudo publicado foi realizado. Um teste pode ser útil em um lugar e não ter validade em outro onde a prevalência é muito baixa. Pode-se chegar aos mesmos resultados, usando as equações: \\[ VPP =\\frac{sens \\times prev}{\\left(sens \\times prev\\right) + \\left [\\left (1- espec\\right) \\times \\left (1- prev\\right)\\right ]} \\] \\[ VPN =\\frac{espec\\times \\left (1- prev\\right)}{\\left[\\left (1 - sens \\right)\\times prev\\right]+\\left[espec\\times \\left (1 - prev\\right)\\right]} \\] A prevalência pode ser interpretada como a probabilidade antes da realização do teste, conhecida como probabilidade pré-teste. A diferença entre as probabilidades pré e pós-teste é uma forma de avaliar a utilidade do teste. Esta diferença pode ser mensurada pela razão de probabilidade (likelihood ratio). 18.2.3 Razão de Probabilidade A Razão de Probabilidades (likelihood ratio) é uma forma alternativa de descrever o desempenho de um teste diagnóstico. Alguns autores a denominam de razão de verossimilhança. A razão de probabilidades para um resultado de teste é definida como a razão entre a probabilidade de observar aquele resultado em indivíduos com a doença em questão e a probabilidade desse resultado em indivíduos sem a doença (160). Razões de probabilidade são, clinicamente, mais úteis do que sensibilidade e especificidade. Fornecem um resumo de quantas vezes mais (ou menos) a probabilidade de os indivíduos com a doença apresentarem aquele resultado específico do que os indivíduos sem a doença, e também podem ser usados para calcular a probabilidade de doença para pacientes individuais (161). Cada vez mais as razões de probabilidade estão se tornando populares para relatar a utilidade dos testes de diagnóstico. Quando os resultados do teste são relatados como sendo positivos ou negativos, dois tipos de razões de probabilidades podem ser descritos, a razão de probabilidades para um teste positivo (denotada LR +) e a razão de probabilidades para um teste negativo (denotada LR−). A razão de probabilidades para um teste positivo é definida como a probabilidade de um indivíduo com doença ter um teste positivo dividida pela probabilidade de um indivíduo sem doença ter um teste positivo. A fórmula para calcular LR + é Ou seja, \\[ LR(+)=\\frac{sensibilidade}{1 - especificidade} \\] Razão de probabilidades positiva maior que 1 significa que um teste positivo tem mais probabilidade de ocorrer em pessoas com a doença do que em pessoas sem a doença. De um modo geral, para os indivíduos que apresentam um resultado positivo, LR (+) &gt; 10 aumenta significativamente a probabilidade de doença (“confirma” a doença), enquanto LR (+) &lt; 0,1, virtualmente, exclui a probabilidade de uma pessoa ter a doença (162). Usando os dados da do objeto diag, obtido com a função epi.tests() do pacote epiR, tem-se que a LR (+) da ultrassonografia para o diagnóstico de apendicite aguda é igual 2.32 (IC95%: 1,22 – 4,40). Significa que uma pessoa com apendicite aguda tem cerca de 2,32 vezes mais probabilidade de ter um teste positivo do que uma pessoa que não tem a doença. A razão de probabilidade negativa é definida como a probabilidade de um indivíduo com doença ter um teste negativo dividido pela probabilidade de um indivíduo sem doença ter um teste negativo. A fórmula para calcular a LR− é: Ou seja, \\[ LR(-)=\\frac{sensibilidade}{1-especificidade} \\] Razão de probabilidade negativa menor que 1 significa que um teste negativo é menos provável de ocorrer em pessoas com a doença do que em pessoas sem a doença. Um LR muito baixo (abaixo de 0,1) praticamente exclui a chance de que uma pessoa tenha a doença (162). Voltando aos dados anteriores, a LR (-) para a ultrassonografia é igual a 0.49 (IC95%: 0.35 - 0.68). Significa que a probabilidade de ter um teste negativo para indivíduos com doença A é 0,49 vezes ou cerca de metade daqueles sem a doença. Dito de outra forma, os indivíduos sem a doença têm cerca o dobro probabilidade de ter um teste negativo do que os indivíduos com a doença. 18.2.3.1 Estimando a probabilidade de doença Uma grande vantagem das razões de probabilidade é que elas podem ser usadas para ajudar o médico a adaptar a sensibilidade e a especificidade dos testes aos pacientes individuais. Ao se atender um paciente em uma clínica, pode-se decidir realizar um teste específico, após uma anamnese e um exame físico. A decisão de fazer o teste baseia-se nos sintomas e sinais do paciente e na experiência pessoal. Existe suspeita de um determinado diagnóstico e o objetivo é excluir ou confirmar esse diagnóstico. Antes de solicitar o teste, geralmente existe uma estimativa aproximada da probabilidade do paciente de ter essa doença, conhecida como probabilidade pré-teste ou a priori, que geralmente é estimada com base na experiência pessoal do médico, dados de prevalência local e publicações científicas. A razão mais importante pela qual um teste é realizado é tentar modificar a probabilidade de doença. Um teste positivo pode aumentar a probabilidade pós-teste e um teste negativo pode reduzir essa probabilidade. A probabilidade pós-teste de doença é o que mais interessa aos médicos e pacientes, pois isso pode ajudar a decidir se devem confirmar, descartar um diagnóstico ou realizar outros testes. Os resultados dos testes clínicos são geralmente usados não para fazer ou excluir categoricamente um diagnóstico, mas para modificar a probabilidade do pré-teste a fim de gerar a probabilidade do pós-teste. O teorema de Bayes é uma relação matemática que permite estimar a probabilidade pós-teste. Para se compreender este conceito, é importante entender a diferença entre probabilidade e odds (163). Probabilidade é a proporção de pessoas que apresentam uma determinada característica (teste positivo, sinal clínico). Odds (chance) representa a razão entre duas características complementares, ou seja, a probabilidade de um evento dividido pela probabilidade do não evento (1 – evento). Ambos contêm as mesmas informações de maneiras diferentes. Por exemplo, usando os dados da tabela tab_ap, , verifica-se que a probabilidade (p) de uma ultrassonografia positiva para apendicite aguda é igual a &lt;- 85 b &lt;- 7 c &lt;- 46 d &lt;- 18 p &lt;- (a + b)/(a + b + c + d) p ## [1] 0.5897436 e que o odds da ultrassonografia positiva56 é odds &lt;- (a + b)/(c + d) odds ## [1] 1.4375 Para transformar a odds em probabilidades e vice-versa, procede-se da seguinte maneira: \\[ p=\\frac{odds}{1+odds} \\] Voltando ao exemplo: p = odds/(1 + odds) p ## [1] 0.5897436 e \\[ odds=\\frac{p}{1-p} \\] odds = p/(1-p) odds ## [1] 1.4375 Pelo teorema de Bayes, sabendo-se a probabilidade a priori ou probabilidade pré-teste, é possível obter a probabilidade pós-teste ou a posteriori, usando a razão de probabilidades. Para atingir este objetivo, basta, inicialmente, multiplicar o odds pré-teste pela razão de probabilidades: \\[ odds_{pos} = odds_{pre \\times LR} \\] Após, para encontrar a probabilidade pós-teste, basta converter o odds pós-teste em probabilidade: \\[ p_{pos} = \\frac{odds_{pos}}{1-odds_{pos}} \\] No exemplo da tab_ap, foi verificado que o LR (+) é igual a 2,32 e a prevalência de apendicite aguda é em torno de 7% pode-se prever a probabilidade de haver apendicite aguda, diante de uma ultrassonografia alterada: prev &lt;- 0.07 LR &lt;- 2.32 odds_pre &lt;- 0.07/(1 -0.07) odds_pos &lt;- odds_pre * LR p_pos &lt;- odds_pos/(odds_pos +1) round(p_pos, 3) ## [1] 0.149 Ou, em outras palavras, diante de um teste positivo, a probabilidade de o paciente ter apendicite aguda passa de 7% antes do teste para praticamente 15%! Estes cálculos podem ser simplificados, utilizando o nomograma de Fagan (164), extremamente fácil de se usar (165), pois basta unir a probabilidade pré-teste ao LR que a reta apontará para a probabilidade pós-teste (Figura 18.3). Figura18.3: Nomograma de Fagan 18.2.4 Curva ROC Nem sempre o resultado de um teste é dicotômico (positivo/negativo). Com frequência, trabalha-se com variáveis contínuas (pressão arterial, glicemia, dosagem do sódio, dosagens hormonais, etc.). Neste caso, não há um resultado “positivo” ou “negativo”. Um “ponto de corte” precisa ser criado, para definir quem será considerado positivo ou negativo. A escolha do ponto de corte depende das consequências de um resultado falso positivo ou de um falso negativo. Falsos positivos estão associados com custos (emocional ou financeiro) e com a dificuldade de “desrotular” alguém que recebeu o rótulo de “positivo”. Resultados falsos negativos podem “tranquilizar” pessoas doentes que não são seguidas ou tratadas precocemente. A distribuição dos níveis glicêmicos em diabéticos e não diabéticos não tem um ponto de corte bem nítido. As duas populações se sobrepõem (Figura 18.4), gerando falso positivos ou falso negativos, dependendo do ponto de corte escolhido (163). Figura18.4: Populações de indivíduos normais (curva em azul) e diabétticos (curva em vermelho Suponha que ao se examinar uma população fosse escolhido o ponto de corte de 80mg/dL, haveria um aumento no número de indivíduos com teste positivo com uma taxa de falsos positivos elevada, diminuindo a especificidade do teste. Se, por outro lado, o ponto de corte fosse elevado para 200mg/dL, o número de falsos negativos teria um grande aumento, reduzindo a sensibilidade. Esta oscilação entre a sensibilidade e a especificidade ocorre pelo fato de a localização do ponto de corte ser uma decisão arbitrária num contínuo entre o normal e anormal. Ao se escolher um ponto de corte deve-se fazer um balanço entre a sensibilidade e a especificidade, levando em conta as consequências da escolha. Por exemplo, a triagem para fenilcetonúria em recém-nascidos valoriza a sensibilidade em vez de especificidade; o custo da perda de um caso é alto, pois existe tratamento eficaz. Uma desvantagem é que ocorre um grande número de testes falso positivos que causam angústia e a realização de mais testes. Em contraste, a triagem para o câncer de mama deve favorecer a especificidade sobre a sensibilidade, uma vez que uma avaliação mais aprofundada daquelas com teste positivo, implica em biopsias dispendiosas e invasivas. As curvas ROC (Receiver Operating Characteristic) são uma ferramenta inestimável para encontrar o ponto de corte em uma medida com distribuição contínua que melhor prediz se uma condição está presente, por exemplo, se pacientes são positivos ou negativos para a presença de uma doença (166). As curvas ROC são usadas para encontrar um ponto de corte que separa um resultado de teste “normal” de um “anormal” quando o resultado do teste é uma medida contínua. As curvas ROC são traçadas calculando a sensibilidade e a especificidade do teste na predição do diagnóstico para cada valor da medida. A curva permite determinar um ponto de corte para a medição que maximiza a taxa de verdadeiros positivos (sensibilidade) e minimiza a taxa de falsos positivos (1 – especificidade) e, portanto, maximiza a razão de probabilidades (likelihood ratio). 18.2.4.1 Exemplo O conjunto de dados dadosTestes.xlsx contém informações para os resultados hipotéticos de três testes bioquímicos diferentes e uma variável (doença) que indica se foi confirmada a doença (padrão-ouro). Para obter arquivo, clique aqui e salve o mesmo em seu diretório de trabalho. Leitura e observação dos dados Como é um arquivo em Excel, a leitura será realizada pela função read_excel() do pacote readxl: dados &lt;- readxl::read_excel(&quot;Arquivos/dadosTestes.xlsx&quot;) str(dados) ## tibble [145 × 5] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:145] 1 2 3 4 5 6 7 8 9 10 ... ## $ teste1: num [1:145] 25 2.2 46.2 9.9 46.5 36.1 34.8 44.9 36.9 7.1 ... ## $ teste2: num [1:145] 25 2.2 15.6 20.4 15.7 35.7 34.8 55.4 36.9 7.1 ... ## $ teste3: num [1:145] 15 2.2 25 20.4 15.7 36.1 24 55.4 36.9 7.1 ... ## $ doenca: num [1:145] 2 2 1 1 2 2 2 1 2 2 ... A variável doença será transformada em fator: dados$doenca &lt;- as.factor(dados$doenca) As curvas ROC são usadas para avaliar qual teste é mais útil para prever quais pacientes serão positivos para a doença. A hipótese nula é que a área sob a curva ROC é igual a 0,5, ou seja, a habilidade do teste para identificar casos positivos e negativos é a esperada por acaso. A Figura 18.5 mostra a quantidade de sobreposição na distribuição da medição dos testes bioquímicos contínuos em ambos os grupos doença positiva e doença negativa. No Teste 1, a sobreposição é completa e não haverá um ponto de corte que separe efetivamente os dois grupos. Nos Testes 2 e 3, há uma maior separação das medidas de teste entre os grupos, particularmente para Teste 3. Figura18.5: Resultado do teste vs doença. 18.2.4.2 Construção da curva ROC A validade dos testes, na distinção entre os grupos doença-positivo e doença-negativo, pode ser quantificada pelas curvas ROC, usando a função roc() do pacote pROC (167). Este pacote tem várias funções: auc: calcula a área da curva ROC; ci: calcula o intervalo de confiança da curva ROC; ci.auc: calcula o intervalo de confiança da AUC; ci.se: calcula o intervalo de confiança de sensibilidades em determinadas especificidades; ci.sp: calcula o intervalo de confiança de especificidades em determinadas sensibilidades; ci.thresholds: calcula o intervalo de confiança dos limites; coords: Retorna as coordenadas (sensibilidades, especificidades, pontos de corte) de uma curva ROC; roc: Constroi uma curva ROC; roc.test: Compara a AUC de duas curvas ROC correlacionadas; smooth: suaviza a curva ROC Usar a função com os argumentos variável resposta (doenca), variável preditora (teste3, teste2 e teste1), indicação de que o gráfico deve ser desenhado (plot = TRUE). Como por padrão o gráfico é plotado com a sensibilidade no eixo x e a especificidade no eixo y; deve-se acrescentar o argumento legacy.axes = TRUE para aparecer o seu complemento, os falsos positivos (\\(1 – especificidade\\)). Além desses, pode-se usar vários outros argumentos como: print.auc = TRUE, que imprime no gráfico a AUC e ci que é o intervalo de confiança da AUC. Para que a sensibilidade e especificidade apareçam como uma percentagem, deve-se usar o argumento percent = TRUE, pois o padrão é FALSE. Os demais argumentos são os rótulos dos eixos, cor da curva, largura da curva (lwd). roc3 &lt;- roc (dados$doenca, dados$teste3, plot=TRUE, quiet = TRUE, legacy.axes=TRUE, print.auc=TRUE, print.auc.y = 0.2, ci = TRUE, ylab=&quot;Sensibilidade&quot;, xlab=&quot;1 - Especificdade&quot;, col=&quot;steelblue&quot;, smooth = TRUE, lwd=2) roc2 &lt;- roc (dados$doenca, dados$teste2, plot=TRUE, quiet = TRUE, legacy.axes=TRUE, print.auc=TRUE, ci = TRUE, print.auc.y=0.13, col=&quot;chartreuse4&quot;, lwd=2, smooth = TRUE, add=TRUE) roc1 &lt;- roc (dados$doenca, dados$teste1, plot=TRUE, quiet = TRUE, legacy.axes=TRUE, print.auc=TRUE, ci = TRUE, print.auc.y=0.06, col=&quot;tomato&quot;, lwd=2, smooth = TRUE, add=TRUE) # Legendas das curvas ROC text (0.73,0.80,&quot;Teste 3&quot;, col=&quot;steelblue&quot;, cex = 1) text (0.53,0.73,&quot;Teste 2&quot;, col=&quot;chartreuse4&quot;, cex = 1) text (0.35,0.65,&quot;Teste 1&quot;, col=&quot;tomato&quot;, cex = 1) Figura18.6: Curvas ROC para os Testes 1, 2 e 3. Interpretação do resultado A Figura 18.6, que exibe o desempenho diagnóstico dos três testes. Em uma curva ROC, a sensibilidade é calculada usando cada valor do teste no conjunto de dados como um ponto de corte e é plotada em relação à (1 – especificidade) correspondente nesse ponto, como mostrado na Figura 18.6. Assim, a curva são os Verdadeiros Positivos (VP) plotados em relação aos Falsos Positivos (FP), calculados usando cada valor do teste como ponto de corte. A reta diagonal indica onde o teste cairia se os resultados não fossem melhores do que o acaso para predizer a presença de uma doença. O Teste 1 está próximo desta reta, confirmando que ele tem pouca capacidade de discriminar os pacientes doentes e não doentes. A área abaixo da reta diagonal é equivalente a 0,5 da área total. Quanto maior a área sob a curva ROC, mais útil é o teste para predizer os pacientes que têm a doença. Uma curva que cai substancialmente abaixo da linha diagonal indica que o teste tem pouca capacidade de diagnosticar a doença. Quando há uma separação perfeita dos valores dos dois grupos, isto é, sem sobreposição das distribuições, a área sob a curva ROC é igual a 1 (a curva ROC alcançará o canto superior esquerdo do gráfico). A área sob a curva (Area Under the Curve – AUC) e seu intervalo de confiança de 95% podem ser obtidos com os comandos usados na construção da Figura 18.6 ou separadamente usando as funções auc() e ci.auc() do pacote pROC. auc (roc1) ## Area under the curve: 0.5891 ci.auc (roc1) ## 95% CI: 0.4856-0.681 (2000 stratified bootstrap replicates) auc(roc2) ## Area under the curve: 0.7616 ci.auc(roc2) ## 95% CI: 0.6743-0.8385 (2000 stratified bootstrap replicates) auc (roc3) ## Area under the curve: 0.898 ci.auc(roc3) ## 95% CI: 0.8337-0.9409 (2000 stratified bootstrap replicates) A acurácia geral de um teste pode ser descrita como a área sob a curva; quanto maior for a área, melhor será o teste. Na Figura 18.6, o Teste 3 tem uma AUC maior que os outros dois testes. Usa-se a seguinte estimativa (Tabela 18.1 para avaliar a acurácia de um teste ou da capacidade de identificar corretamente uma condição usando curva ROC (168): Tabela18.1: Acurácia do teste diagnóstico AUC Qualidade do Teste &gt;0,90 excelente 0,80 a 0,90 muito bom 0,70 a 0,80 bom 0,60 a 0,70 suficiente 0,50 a 0,60 ruim &lt;0,50 ignorar teste Desta forma, o Teste 3 pode ser considerado um bom teste e o Teste 1 é um teste ruim. Comparando duas curvas Pode-se comparar duas curvas ROC com a função roc.test(), por exemplo, comparando as curvas dos Teste 3 e 2 (169): roc.test(roc3, roc2) ## ## Bootstrap test for two correlated ROC curves ## ## data: roc3 and roc2 ## D = 4.6643, boot.n = 2000, boot.stratified = 1, p-value = 3.097e-06 ## alternative hypothesis: true difference in AUC is not equal to 0 ## sample estimates: ## Smoothed AUC of roc1 Smoothed AUC of roc2 ## 0.8980454 0.7616201 O Teste 3 tem uma AUC que o caracteriza como um bom teste e o teste de DeLong, entregue na saída do roc.test(), resultou que a diferença entre ele o Teste 2 é estatisticamente significativa (P &lt; 0,0001). 18.2.4.3 Melhor ponto de corte O melhor ponto de corte (Best Critical Value), que às vezes é chamado de ponto de diagnóstico ótimo ou de Youden, é o ponto da curva mais próximo da parte superior do eixo y (Figura 18.6, Teste 3). Este é o ponto em que a taxa de verdadeiros positivos é otimizada e a taxa de falsos positivos é minimizada. O melhor ponto de corte para o Teste 3 é mostrado na Figura 18.7. Este melhor ponto de corte pode ser identificado a partir dos pontos de coordenadas da curva, usando a função roc() com os seguintes argumentos: best &lt;- roc (dados$doenca, dados$teste3, plot = TRUE, ci=TRUE, thresholds=&quot;best&quot;, print.thres=&quot;best&quot;, legacy.axes=TRUE, main=&quot;&quot;, ylab=&quot;Sensibilidade&quot;, xlab=&quot;1 - Especificidade&quot;, col=&quot;steelblue&quot;, lwd=2) Figura18.7: Curvas ROC para os Testes 1, 2 e 3. best ## ## Call: ## roc.default(response = dados$doenca, predictor = dados$teste3, ci = TRUE, plot = TRUE, thresholds = &quot;best&quot;, print.thres = &quot;best&quot;, legacy.axes = TRUE, main = &quot;&quot;, ylab = &quot;Sensibilidade&quot;, xlab = &quot;1 - Especificidade&quot;, col = &quot;steelblue&quot;, lwd = 2) ## ## Data: dados$teste3 in 48 controls (dados$doenca 1) &gt; 97 cases (dados$doenca 2). ## Area under the curve: 0.8973 ## 95% CI: 0.8444-0.9502 (DeLong) Assim, para o Teste 3, o ponto de corte ideal é 24,8, onde a especificidade é igual a 0,854 e a sensibilidade é igual 0,845. Estes dados, fornecem um LR para um resultado positivo igual a: \\[ LR \\left(+\\right) = \\frac{0.845}{\\left (1-0.854\\right)} = 5,79 \\] As coordenadas da curva ROC podem ser obtida com a seguinte programação, a partir de uma sensibilidade e especificidade acima de 0 (zero): coordenadas &lt;- dados %&gt;% roc(doenca, teste3) %&gt;% coords (transpose = F) head(coordenadas, 10) ## threshold specificity sensitivity ## 1 Inf 0.00000000 1 ## 2 57.50 0.02083333 1 ## 3 54.65 0.04166667 1 ## 4 53.45 0.06250000 1 ## 5 52.80 0.08333333 1 ## 6 51.30 0.10416667 1 ## 7 49.65 0.12500000 1 ## 8 48.65 0.16666667 1 ## 9 47.50 0.18750000 1 ## 10 46.50 0.35416667 1 A estatística J de Youden (170) é calculada deduzindo 1 a partir da soma de sensibilidade e especificidade do teste e não é expressa como porcentagem, mas como parte de um número inteiro: \\(\\left (sensibilidade + especificidade\\right) - 1\\). A estatística J de Youden no melhor ponto de corte do Teste 3 é igual a \\(\\left (0,845+ 0,854\\right) - 1 = 0,699\\). Este é o maior valor de todos os valores das coordenadas (91 valores) usadas. youden &lt;- max(coordenadas$sensitivity + coordenadas$specificity) - 1 youden ## [1] 0.6995275 A Figura 18.7 mostra o ponto de corte ideal. Ele também pode ser obtido com a função coords() do pacote pRoc: roc3 &lt;- dados %&gt;% roc(doenca, teste3) coords(roc3, x = &quot;best&quot;, ret=&quot;threshold&quot;, transpose = FALSE, best.method=&quot;youden&quot;) ## threshold ## 1 24.8 O método para obter o melhor ponto de corte (best.method) pode ser pelo método de youden ou closest.topleft. No exemplo, o resultado é o mesmo. Para maiores detalhes consulte a ajuda da função (?coord). 18.3 Estatística kappa A estatística de concordância kappa (k) de Cohen é utilizada para descrever a concordância entre dois ou mais avaliadores quando realizam uma avaliação nominal ou ordinal de uma mesma amostra (171). A estatística kappa corrige a chance do acaso nas avaliações e é obtida pela fórmula igual a: \\[ k= \\frac{p_{o} - p_{e}}{1 - p_{e}} \\] Onde \\(p_{o}\\) = proporção observada de concordância e \\(p_{e}\\) = proporção esperada de concordância apenas pelo acaso. Por exemplo, dois radiologistas podem revisar independentemente uma série de radiografias do tórax de pacientes para determinar a presença ou ausência de pneumonia. Para avaliar o grau de concordância entre as classificações dos dois médicos, pode ser relatado o percentual de concordância entre os avaliadores (por exemplo, 50% dos avaliadores responderam “sim” nas duas ocasiões). No entanto, esse percentual pode ser enganoso, pois não leva em conta o nível de concordância entre os dois avaliadores que pode ocorrer por acaso. A estatística kappa pode ser usada para avaliar a concordância das respostas para dois ou mais avaliadores após considerar a concordância casual. Portanto, a estatística kappa é uma estimativa da proporção de concordância entre avaliadores que excede a concordância que ocorreria por acaso. A interpretação dos valores de kappa é mostrada na Tabela 18.2 (172). Quando a proporção observada de concordância é menor que a esperada por acaso, o kappa terá um valor negativo indicando não concordância. Um valor de kappa igual a 0 indica que a concordância observada é igual à concordância casual. O teste de hipóteses testa a hipótese de que a concordância entre os dois avaliadores seja puramente aleatória. Quando o valor P é menor que 0,05, rejeitamos a hipótese de que a concordância foi puramente aleatória. As premissas para o kappa de Cohen são que os participantes ou itens a serem classificados são independentes e também que os avaliadores e categorias são independentes. Tabela18.2: Valor Kappa e nível de concordância correspondente Valor kappa Concordância &lt;0,00 pobre 0,00 - 0,20 leve 0,21 - 0,40 razoável 0,41 - 0,60 moderada 0,61 - 0,80 substancial 0,81 - 1,00 quase perfeita Existem diferentes tipos de estatísticas kappa. Para dados com três ou mais categorias possíveis (por exemplo, concordo, concordo parcialmente, discordo) ou para dados categóricos ordenados, o kappa ponderado deve ser usado para que as respostas que estão mais distantes da concordância tenham maior peso do que aquelas próximas à concordância. No exemplo usado, as categorias possíveis são dicotômicas (sim e não), portanto, o kappa não ponderado (unweighted) e o ponderado (weighted) retornam o mesmo resultado. 18.3.1 Exemplo O arquivo dadosPneumonia.xlsx contém os dados de 54 crianças com suspeita de pneumonia, cujas radiografias foram avaliadas por dois radiologistas. O objetivo foi medir a concordância diagnóstica dos dois profissionais. Para o cálculo do coeficiente kappa será usada a função Kappa() do pacote vcd (173). Essa função tem os seguintes argumentos: x \\(\\longrightarrow\\) matriz ou tabela weights \\(\\longrightarrow\\) matriz especificada pelo usuário com as mesmas dimensões de x, desnecessário para kappa não ponderado. Na impressão do kappa pode-se usar print (k, digits = 3, CI = TRUE, level = 0.95). Onde k é o coeficiente de kappa, calculado pela função Kappa(), CI é o intervalo de confiança e o nível de confiança padrão é 95%. 18.3.1.1 Leitura e exploração dos dados O conjunto de dados dadosPneumonia.xlsx pode ser obtido aqui. Após salvar o arquivo em seu diretório, ele pode ser carregado com a função read_excel() do pacote readxl: dados &lt;- readxl::read_excel(&quot;Arquivos/dadosPneumonia.xlsx&quot;) 18.3.1.2 Construção da tabela O cálculo do kappa com a função Kappa() exige uma tabela, onde os dados dos dois radiologistas são cruzados. As variáveis a serem cruzadas são rx1 e rx2: dados$rx1 &lt;- factor(dados$rx1, ordered=TRUE, levels = c(&quot;sim&quot;, &quot;não&quot;)) dados$rx2 &lt;- factor(dados$rx2, ordered=TRUE, levels = c(&quot;sim&quot;, &quot;não&quot;)) tabk &lt;- table (dados$rx1, dados$rx2, dnn = c (&quot;Radiologista 1&quot;, &quot;Radiologista 2&quot;)) tabk ## Radiologista 2 ## Radiologista 1 sim não ## sim 32 5 ## não 3 14 18.3.1.3 Cálculo do kappa O kappa é dado pela execução da função: k &lt;- vcd::Kappa(tabk) print (k, digits= 3, CI=TRUE, level=0.95) ## value ASE z Pr(&gt;|z|) lower upper ## Unweighted 0.667 0.107 6.21 5.42e-10 0.456 0.878 ## Weighted 0.667 0.107 6.21 5.42e-10 0.456 0.878 A saída exibe o kappa pontual e os intervalos de confiança de 95%, podendo-se concluir, desses resultados, que existe uma boa confiabilidade nos diagnósticos dos radiologistas (k = 0,67, concordância substancial,de acordo com a Tabela 18.2). 18.4 Medidas de frequência 18.4.1 Prevalência A prevalência, ou mais adequadamente, a prevalência pontual de uma doença é a proporção da população portadora da doença em um determinado ponto do tempo. É uma medida instantânea por excelência e fornece uma medida estática da frequência da doença. É também conhecida como taxa de prevalência e é expressa em percentagem ou por \\(10^{n}\\) habitantes. As medidas de prevalência geram informações úteis para o planejamento e administração de serviços de saúde. A prevalência por período descreve os casos que estavam presentes em qualquer momento durante um determinado período de tempo. Diz o número total de casos de uma doença que se sabe haver existido durante um período de tempo. Um tipo especial de prevalência de período é a prevalência ao longo da vida, que mede a frequência cumulativa ao longo da vida de um resultado até o momento presente (ou seja, a proporção de pessoas que tiveram o evento em qualquer momento no passado). As doenças, quanto a sua duração, podem ser agudas e de longa duração ou crônicas. A prevalência é proporcional ao tempo de duração da doença. Hipoteticamente, se o surgimento de novos casos de doença ocorre em ritmo constante e igual para doenças agudas e crônicas, estas últimas acumularão casos, aumentando a prevalência. As doenças agudas tenderão a manter uma prevalência constante. A terapêutica, diminuindo o tempo de duração das doenças, também reduz a prevalência. A prevalência é dada pela razão: \\[ prevalência = \\frac{número \\ de \\ casos \\ conhecidos \\ da \\ doença}{total \\ da \\ População} \\times 10^{n} \\] 18.4.1.1 Exemplo Como exemplo, será verificada a frequência de tabagismo entre as puérperas da maternidade do HGCS. O banco de dados dadosMater.xlsx contém informação de 1368 nascimentos e pode ser consultado na Seção 5.3. Depois de salvo em seu diretório de trabalho, ele pode ser carregado com a função read_excel() do pacote readxl. dados &lt;- readxl::read_excel (&quot;Arquivos/dadosMater.xlsx&quot;) Inicialmente, será verificado quantas fumantes existem. O conjunto de dados contém uma variável fumo, onde 1 = fumante e 2 = não fumante. Portanto, há necessidade de transformar a variável numérica em um fator: dados$fumo &lt;- factor (dados$fumo, ordered = TRUE, levels = c(1,2), labels = c(&quot;fumante&quot;, &quot;não fumante&quot;)) tabFumo &lt;- table(dados$fumo) tabFumo ## ## fumante não fumante ## 301 1067 Além de relatar a estimativa pontual da frequência da doença, é importante fornecer uma indicação da incerteza em torno dessa estimativa pontual. A função epi.conf(), do pacote epiR (157), permite calcular intervalos de confiança para prevalência, motivo da escolha dessa função. A função epi.conf() usa os seguintes argumentos: dat \\(\\longrightarrow\\) matriz ou tabela; ctype \\(\\longrightarrow\\) tipo de intervalo de confiança a ser calculado. Opções: mean.single, mean.unpair, mean.pair, prop.single, prop.unpaired, prevalence, inc.risk, inc.rate, odds e smr (standardized mortality rate); method \\(\\longrightarrow\\) método a ser usado. Quando ctype = \"inc.risk\" ou ctype = \"prevalence\", as opções são exact, wilson e fleiss Quando ctype = \"inc.rate\" as opções são exact e byar; N \\(\\longrightarrow\\) tamanho da população; conf.level \\(\\longrightarrow\\) magnitude do intervalo de confiança retornado. Deve ser um único número entre 0 e 1. Construção da matriz Com os dados da tabFumo, constrói-se uma matriz de duas colunas: n1 &lt;- 301 N1 &lt;- 301 + 1067 mat1 &lt;- as.matrix(cbind (n1, N1)) mat1 ## n1 N1 ## [1,] 301 1368 Cálculo da prevalência Usando a função epiR(), tem-se: epiR::epi.conf(mat1, ctype = &quot;prevalence&quot;, method = &quot;exact&quot;, conf.level = 0.95) ## est lower upper ## 1 0.2200292 0.1983313 0.2429365 A saída mostra que a prevalência de fumantes entre as puérperas do HGCS é igual a 22,0% (IC95%: 19,8 – 24,3%). 18.4.2 Incidência A incidência fornece uma medida da frequência com que os indivíduos suscetíveis se tornam casos de doenças, à medida que são observados ao longo do tempo. Um caso de incidente ocorre quando um indivíduo deixa de ser suscetível e passa a ser doente. A contagem de casos de incidentes é o número de tais eventos que ocorrem em uma população durante um período de acompanhamento definido. Existem duas maneiras de expressar a incidência: A incidência cumulativa (risco) é a proporção de indivíduos inicialmente suscetíveis em uma população que se tornam novos casos durante um período de acompanhamento definido. Para calcular a incidência cumulativa, é necessário primeiro identificar os doentes e após acompanhar por um determinado tempo os não doentes (Figura 18.8). Figura18.8: Incidência A taxa de incidência (densidade de incidência ou taxa de incidência) é o número de novos casos da doença que ocorrem por unidade de tempo em risco durante um período de acompanhamento definido. Este período é expresso como pessoas-tempo (pessoas-ano, por exemplo). O conceito de pessoas-tempo pode ser ilustrado com o seguinte exemplo: a Figura 18.9 representa um estudo epidemiológico hipotético com duração de cinco anos, onde D é o desfecho e C representa os sujeitos que deixaram o estudo por migração ou morte (censurados) por causa não relacionada ao desfecho Figura18.9: Pessoas-tempo (estudo epidemiológico hipotético). Nesse estudo hipotético, o indivíduo 1 permaneceu no estudo 3,5 anos; o indivíduo 2,5 anos; o indivíduo 3, 4,5 anos e, assim por diante, totalizando 32,5 pessoas-anos. Em outras palavras, ocorreram 4 desfechos durante os 5 anos do estudo, consequentemente, a taxa de incidência (TI) foi de \\[ TI = \\frac{4}{32,5} \\times 1000 = \\frac{123}{1000\\ pessoas-ano} \\] Isto significa que se fossem acompanhadas 1000 pessoas por um ano, 123 delas apresentariam o desfecho D. 18.4.2.1 Exemplo Aparentemente, pessoas cegas tem uma menor incidência de câncer e esse efeito parece ser mais pronunciado em pessoas totalmente cegas do que em pessoas com deficiência visual grave. Para testar essa hipótese, foi identificada uma coorte de 1.567 pessoas totalmente cegas e 13.292 sujeitos com deficiência visual grave. As informações sobre a incidência de câncer foram obtidas do Registro Sueco de Câncer (174). Foram diagnosticados de 136 casos de câncer em 22050 pessoas-ano em risco totalmente cegas e 1709 casos de câncer em 127650 pessoas-anos em risco com deficiência visual grave. A taxa de incidência pode ser calculada, usando-se a mesma função epi.conf(), usada para o cálculo da prevalência, mudando o argumento ctype = “prevalence” para ctype = “inc.rate”, conforme recomendado: Pessoas totalmente cegas Inicialmente, contrói-se a matriz: n2 &lt;- 136 N2 &lt;- 22050 mat2 &lt;- as.matrix(cbind (n2, N2)) mat2 ## n2 N2 ## [1,] 136 22050 Logo, a incidência de câncer nos totalmente cegos é: epiR::epi.conf(mat2, ctype = &quot;inc.rate&quot;, method = &quot;exact&quot;, conf.level = 0.95)*1000 ## est lower upper ## n2 6.1678 5.174806 7.295817 Pessoas com grave deficiência visual Inicialmente, contrói-se a matriz: n3 &lt;- 1709 N3 &lt;- 127650 mat3 &lt;- as.matrix(cbind (n3, N3)) mat3 ## n3 N3 ## [1,] 1709 127650 Logo, a incidência de câncer nos com grave deficiência visual é: epiR::epi.conf(mat3, ctype = &quot;inc.rate&quot;, method = &quot;exact&quot;, conf.level = 0.95)*1000 ## est lower upper ## n3 13.38817 12.76088 14.03832 As saídas mostram que para cada 1000 pessoas cegas (a função foi multiplicada por 1000) acompanhadas por um ano, ocorreu 6,2 ((IC95%: 5,2 – 7,3) casos de câncer. Uma taxa de incidência, praticamente, metade da taxa de incidências das pessoas com deficiência visual grave. Os IC95% não são coincidentes, o que significa que essa diferença é significativa. Houve, na amostra, uma incidência menor de câncer entre os indivíduos totalmente cegos, sugerindo que a melatonina possa ser um fator protetor contra o câncer. 18.4.3 Relação entre prevalência e incidência A incidência é uma medida de risco. A prevalência, por não levar em consideração o tempo de duração da doença (t), não tem esta capacidade. Em uma população onde a situação da doença encontra-se em estado estacionário (ou seja, sem grandes migrações ou mudanças ao longo do tempo na incidência/prevalência), a relação entre prevalência e incidência e duração da doença pode ser expressa pela seguinte fórmula (175): \\[ prevalência \\ pontual = incidência \\times prevalência \\] Por exemplo, se a incidência da doença for de 0,8% ao ano e sua duração média (sobrevida após o diagnóstico) for de 10 anos, a prevalência pontual será de aproximadamente 8%. 18.5 Medidas de associação 18.5.1 Odds Ratio Odds Ratio (OR) é a razão entre dois odds. A Odds Ratio, traduzida como Razão de Chances, está associada, usualmente, com estudos retrospectivos tipo caso-controle com desfechos dicotômicos. A odds ratio (OR) expressa a odds de exposição entre os que têm o desfecho (casos) pela odds de exposição nos livres de desfecho (controles). Figura18.10: Tabela de contingência 2 x 2 Usando a Figura 18.10, a fórmula \\(odds =\\frac{p}{1 -p}\\) e que \\[ p_{exp \\ doentes} = \\frac{a}{a+c} \\] \\[ p_{exp \\ não \\ doentes} = \\frac{b}{b+d} \\] tem-se: \\[ odds_{exp} \\ {casos} = \\frac{\\frac{a}{a+c}}{1- \\frac{a}{a+c}}=\\frac{a}{c} \\] \\[ odds_{exp} \\ {controles} = \\frac{\\frac{b}{b+d}}{1- \\frac{b}{b+d}}=\\frac{b}{d} \\] Portanto, a OR é igual a: \\[ OR = \\frac{odds_{exp}\\ {casos}}{odds_{exp}\\ {controles}}=\\frac{\\frac{a}{c}}{\\frac{b}{d}}=\\frac{a \\times d}{c \\times b} \\] Em decorrência da última fórmula, a OR é definida como a razão dos produtos cruzados em uma tabela de contingência 2×2. 18.5.1.1 Exemplo Em um estudo de caso-controle hipotético, a distribuição das exposições entre os casos e um grupo de pessoas saudáveis (“controles”) é comparada entre si. Os casos correspondem a um tipo raro de câncer, onde se suspeita que exista uma associação à exposição a um determinado fator de risco. Os dados desse estudo hipotético estão no arquivo dadosCasoControle.xlsx. O conjunto de dados pode ser obtido aqui. Depois de salvo em seu diretório de trabalho, ele pode ser carregado com a função read_excel() do pacote readxl. cc &lt;- readxl::read_excel (&quot;Arquivos/dadosCasoControle.xlsx&quot;) As variáveis cc$exposto e cc$desfecho devem ser transformadas em fatores e na ordem sim, não, uma vez que o R coloca em ordem alfabética (não, sim): cc$exposto &lt;- factor (cc$exposto, levels = c(&quot;sim&quot;, &quot;não&quot;)) cc$desfecho &lt;- factor (cc$desfecho, levels = c(&quot;sim&quot;, &quot;não&quot;)) Após essa etapa, construir uma tabela \\(2 \\times 2\\): tab_cc &lt;- table (cc$exposto, cc$desfecho, dnn = c(&quot;Exposição&quot;, &quot;Desfecho&quot;)) addmargins(tab_cc) ## Desfecho ## Exposição sim não Sum ## sim 48 20 68 ## não 12 40 52 ## Sum 60 60 120 A OR será obtida utilizando a função epi.2by2() do pacote epiR (157). Esta função tem os seguintes argumentos: dat \\(\\longrightarrow\\) tabela de contingência \\(2 \\times 2\\); method \\(\\longrightarrow\\) as opções são “cohort.count”, “cohort.time”, “case.control” ou “cross.sectional”.; conf.level \\(\\longrightarrow\\) padrão = 0.95; units \\(\\longrightarrow\\) multiplicador para incidência e prevalência; outcome \\(\\longrightarrow\\) indicação de como a variável desfecho é representada na tabela de contingência (“as.columns” ou “as.rows”). epiR::epi.2by2(tab_cc, method = &quot;case.control&quot;, conf.level = 0.95, units = 100, outcome = &quot;as.columns&quot;) ## Outcome + Outcome - Total Odds ## Exposed + 48 20 68 2.40 (1.43 to 4.23) ## Exposed - 12 40 52 0.30 (0.13 to 0.53) ## Total 60 60 120 1.00 (0.69 to 1.45) ## ## Point estimates and 95% CIs: ## ------------------------------------------------------------------- ## Exposure odds ratio 8.00 (3.49, 18.34) ## Attrib fraction (est) in the exposed (%) 87.24 (69.26, 95.03) ## Attrib fraction (est) in the population (%) 70.00 (48.69, 82.46) ## ------------------------------------------------------------------- ## Uncorrected chi2 test that OR = 1: chi2(1) = 26.606 Pr&gt;chi2 = &lt;0.001 ## Fisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001 ## Wald confidence limits ## CI: confidence interval A saída exibe os dados em uma tabela \\(2 \\times 2\\), mostrando as odds e os IC95% e outras estatísticas epidemiológicas relacionadas. A OR varia de zero ao infinito. Quando o valor da OR se aproxima de 1, a doença e o fator de risco não estão associados. Acima de 1 significa que existe associação e valores menores de 1 indicam uma associação negativa (efeito protetor). No exemplo hipotético, os indivíduos que se expuseram ao fator de risco têm uma chance 8 vezes maior de apresentar este tipo de câncer. O valor P do qui-quadrado é altamente significativo (P &lt; 0,001). 18.5.2 Risco Relativo O Risco relativo (RR) é a razão entre a incidência de desfecho em indivíduos expostos e a incidência de desfecho em indivíduos não expostos. O RR estima a magnitude da associação entre a exposição e o desfecho (doença). Em outras palavras, compara a probabilidade de ocorrência do desfecho entre os indivíduos expostos com a probabilidade de ocorrência do desfecho nos indivíduos não expostos. A partir da tabela de contingência \\(2 \\times 2\\) (Figura 18.10), tem-se que o estimador do RR é dado por: \\[ RR = \\frac{incidência_{exp}}{incidência_{não \\ exp}}=\\frac{\\frac{a}{a + b}}{\\frac{c}{c + d}} \\] 18.5.2.1 Exemplo Em 1940, ocorreu um surto de gastroenterite, após um jantar, em uma igreja, na cidade de Lycoming, Condado de Oswego, Nova York. Das 80 pessoas presentes, 75 foram entrevistadas. Quarenta e seis relataram doença gastrointestinal, atendendo à definição de caso. As taxas de ataque (incidência) foram calculadas para aqueles que comeram e não comeram cada um dos 14 itens alimentares consumidos na ceia (176). O pacote epitools (177) contém os dados desta investigação no arquivo oswego. data(oswego) dplyr::glimpse(oswego) ## Rows: 75 ## Columns: 21 ## $ id &lt;int&gt; 2, 3, 4, 6, 7, 8, 9, 10, 14, 16, 17, 18, 20, 21, 2… ## $ age &lt;int&gt; 52, 65, 59, 63, 70, 40, 15, 33, 10, 32, 62, 36, 33… ## $ sex &lt;chr&gt; &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, … ## $ meal.time &lt;chr&gt; &quot;8:00 PM&quot;, &quot;6:30 PM&quot;, &quot;6:30 PM&quot;, &quot;7:30 PM&quot;, &quot;7:30 … ## $ ill &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, … ## $ onset.date &lt;chr&gt; &quot;4/19&quot;, &quot;4/19&quot;, &quot;4/19&quot;, &quot;4/18&quot;, &quot;4/18&quot;, &quot;4/19&quot;, &quot;4… ## $ onset.time &lt;chr&gt; &quot;12:30 AM&quot;, &quot;12:30 AM&quot;, &quot;12:30 AM&quot;, &quot;10:30 PM&quot;, &quot;1… ## $ baked.ham &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, … ## $ spinach &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, … ## $ mashed.potato &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, … ## $ cabbage.salad &lt;chr&gt; &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, … ## $ jello &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, … ## $ rolls &lt;chr&gt; &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, … ## $ brown.bread &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, … ## $ milk &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, … ## $ coffee &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, … ## $ water &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, … ## $ cakes &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, … ## $ vanilla.ice.cream &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, … ## $ chocolate.ice.cream &lt;chr&gt; &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;N&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, … ## $ fruit.salad &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, … Existem 75 observações de 21 variáveis, algumas características dos indivíduos como idade, sexo, etc. Importante para a análise é a variável ill (Y – sim, doente; N – não doente) e a variáveis relacionadas aos alimentos ingeridos durante o jantar na igreja. O sorvete de baunilha foi considerado o principal responsável pelo surto. A seguir, as variáveis oswego$vanilla.ice.cream e oswego$ill 57 serão transformadas em fator e os níveis colocados na ordem Y, N, uma vez que o R coloca em ordem alfabética (N, Y) : oswego$ill &lt;- factor (oswego$ill, levels = c (&quot;Y&quot;, &quot;N&quot;)) oswego$vanilla.ice.cream &lt;- factor (oswego$vanilla.ice.cream, levels = c (&quot;Y&quot;, &quot;N&quot;)) Realizada essa etapa, será construída uma tabela para o cálculo do RR: tab_vanilla &lt;- table (oswego$vanilla.ice.cream, oswego$ill, dnn = c (&quot;Vanilla&quot;, &quot;Ill&quot;)) tab_vanilla ## Ill ## Vanilla Y N ## Y 43 11 ## N 3 18 O RR será obtido, utilizando a função epi.2by2() do pacote epiR, cujos argumentos foram mostrados no cálculo da OR, mudando a tabela para tab_vanilla e method = “cohort.count”: epiR::epi.2by2(tab_vanilla, method = &quot;cohort.count&quot;, conf.level = 0.95, units = 100, outcome = &quot;as.columns&quot;) ## Outcome + Outcome - Total Inc risk * ## Exposed + 43 11 54 79.63 (66.47 to 89.37) ## Exposed - 3 18 21 14.29 (3.05 to 36.34) ## Total 46 29 75 61.33 (49.38 to 72.36) ## ## Point estimates and 95% CIs: ## ------------------------------------------------------------------- ## Inc risk ratio 5.57 (1.94, 16.03) ## Inc odds ratio 23.45 (5.84, 94.18) ## Attrib risk in the exposed * 65.34 (46.92, 83.77) ## Attrib fraction in the exposed (%) 82.06 (48.41, 93.76) ## Attrib risk in the population * 47.05 (28.46, 65.63) ## Attrib fraction in the population (%) 76.71 (37.11, 91.37) ## ------------------------------------------------------------------- ## Uncorrected chi2 test that OR = 1: chi2(1) = 27.223 Pr&gt;chi2 = &lt;0.001 ## Fisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001 ## Wald confidence limits ## CI: confidence interval ## * Outcomes per 100 population units Os resultados da saída indicam que os indivíduos que ingeriram sorvete de baunilha (n = 54) tiveram um risco maior de desenvolver gastrenterite aguda quando comparado aos que não ingeriram (n = 21). Dividindo o risco dos indivíduos expostos (incidência = 79,6) pelo risco dos não expostos (incidência = 14,3), encontra-se o RR = 5,57. Isso confirma que o sorvete de baunilha foi o principal responsável. Quanto maior o RR mais forte é a associação entre a doença em questão e a exposição ao fator de risco. Um RR = 1 indica que a doença e a exposição ao fator de risco não estão associadas. Valores &lt; 1 indicam uma associação negativa entre o fator de risco e a doença (efeito protetor). 18.5.3 Odds Ratio vs Risco Relativo A OR não deve ser entendida como uma medida aproximada do RR, exceto para doenças raras (doenças, em geral com prevalência menor do que 10%). Caso contrário, a OR tenderá a superestimar a magnitude da associação e o OR afasta-se da hipótese nula da não associação (OR =1), independentemente de ser um fator de risco ou de proteção. A discrepância (d)58 entre as estimativas do RR e OR pode ser definido como a razão entre o OR e o RR estimados (178). Em outras palavras, a discrepância corresponde a uma proporção do RR (179). \\[ d = \\frac {1- p_{não \\ exp}}{1- p_{exp}}= \\frac{\\frac{c}{c + d}}{\\frac{a}{a + b}} \\] Logo, \\[ OR = RR \\times d \\] Para finalizar, uma comparação entre OR e RR é mostrada na Tabela 18.3 (180). Tabela18.3: Força de associação do RR comparado com o OR. OR RR Magnitude 1,0 1,0 insignificante 1,5 1,2 pequena 3,5 1,9 moderada 9,0 3,0 grande 32 5,7 muito grande 360 19 quase perfeita infinito infinito perfeita 18.5.4 Razão de Prevalência Quando dados transversais estão disponíveis, muitas vezes as associações são avaliadas, usando a razão de prevalência pontual (RPP). Tendo o mesmo princípio das duas medidas anteriores, a razão de prevalência (RPP) compara a prevalência do desfecho entre os expostos com a prevalência do desfecho entre os não expostos. Matematicamente, a RPP é calculada de maneira semelhante ao RR. Apenas, deve-se ter em mente que o desfecho e a exposição foram medidos no mesmo momento, enquanto para o cálculo do RR há necessidade de calcular a incidência. Usando uma tabela de contingência 2 x 2 (Figura 18.10), tem-se: \\[ RPP = \\frac{prevalência \\ de \\ doença_{exp}}{prevalência \\ de \\ doença_{não \\ exp}}=\\frac{\\frac{a}{a + b}}{\\frac{c}{c + d}} \\] Também é possível verificar a prevalência de exposição entre doentes e não doentes: \\[ RPP = \\frac{prevalência \\ de \\ exposição_{doentes}}{prevalência \\ de \\ exposição_{não \\ doentes}}=\\frac{\\frac{a}{a + c}}{\\frac{b}{b + d}} \\] 18.5.4.1 Exemplo Em um estudo transversal (181), foi verificada a prevalência de infecções congênitas entre as puérperas com idade igual ou acima de 20 anos comparadas às mulheres com menos de 20 anos (adolescentes). A hipótese foi de que as adolescentes tinham uma prevalência maior de infecções. Parte dos dados estão no arquivo dadosMater.xlsx, que contém, como já mencionado, informações de 1368 nascimentos. Entre essas, tem-se a idade das mães (idadeMae) e se foi diagnosticada infecção congênita (infCong). O arquivo pode ser obtido aqui. Depois de salvo em seu diretório de trabalho, ele pode ser carregado com a função read_excel() do pacote readxl. dados &lt;- readxl::read_excel (&quot;Arquivos/dadosMater.xlsx&quot;) A partir da variável idadeMae, criar a variável faixaEtaria, dividindo as parturientes em menores de 20 anos (adolescentes) e ≥ 20 anos. Para isso, usou-se a função cut() do pacote base. Revise os argumentos desta função. dados$faixaEtaria &lt;- cut (dados$idadeMae, breaks=c(13,20,46), labels = c(&quot;&lt;20a&quot;,&quot;=&gt;20a&quot;), right = FALSE, include.lowest = TRUE) A variável ìnfCong encontra-se como uma variável numérica e deve ser transformada em fator: dados$infCong &lt;- factor (dados$infCong, ordered = TRUE, levels = c (1,2), labels = c (&quot;sim&quot;, &quot;não&quot;)) Após estes procedimentos, constroi-se uma tabela \\(2 \\times 2\\): tab_infCong &lt;- table(dados$faixaEtaria, dados$infCong, dnn = c(&quot;Faixa Etária&quot;, &quot;Inf. Cong.&quot;)) addmargins(tab_infCong) ## Inf. Cong. ## Faixa Etária sim não Sum ## &lt;20a 7 212 219 ## =&gt;20a 119 1030 1149 ## Sum 126 1242 1368 Cálculo da RPP Usando a tabela tab_infCong com a função epi.2by2() do pacote epiR, cujos argumentos foram mostrados no cálculo da OR e RR, e mudando a tabela para tab_infCong e method = “cross.sectional”, obtem-se: epiR::epi.2by2(tab_infCong, method = &quot;cross.sectional&quot;, conf.level = 0.95, units = 100, outcome = &quot;as.columns&quot;) ## Outcome + Outcome - Total Prev risk * ## Exposed + 7 212 219 3.20 (1.29 to 6.47) ## Exposed - 119 1030 1149 10.36 (8.65 to 12.26) ## Total 126 1242 1368 9.21 (7.73 to 10.87) ## ## Point estimates and 95% CIs: ## ------------------------------------------------------------------- ## Prev risk ratio 0.31 (0.15, 0.65) ## Prev odds ratio 0.29 (0.13, 0.62) ## Attrib prev in the exposed * -7.16 (-10.08, -4.24) ## Attrib fraction in the exposed (%) -224.02 (-584.89, -53.29) ## Attrib prev in the population * -1.15 (-3.48, 1.19) ## Attrib fraction in the population (%) -12.45 (-17.53, -7.58) ## ------------------------------------------------------------------- ## Uncorrected chi2 test that OR = 1: chi2(1) = 11.278 Pr&gt;chi2 = &lt;0.001 ## Fisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001 ## Wald confidence limits ## CI: confidence interval ## * Outcomes per 100 population units A saída exibe várias informações. Foi feita a hipótese de uma maior prevalência entre as mulheres com menos de 20 anos. Por este motivo, elas aparecem como as expostas (Exposed +) e tem uma prevalência de 3,20/100, enquanto as mulheres com mais de 20 anos tiveram uma prevalência de 10,36/100. Isto mostra que a razão de prevalência é igual a 0,31 (IC95%: 0,15-0,65)59, ou seja, &lt; 1, sugerindo que ao contrário da hipótese inicial, as adolescentes têm, neste estudo, uma menor prevalência de infecções congênitas. 18.6 Medidas de impacto 18.6.1 Risco Atribuível O Risco Atribuível (RA) possui características de medida de impacto. O RA, ao invés de concentrar-se na associação em si, refere-se mais às consequências e às repercussões da exposição sobre a ocorrência do desfecho. O RA é a medida do excesso ou acréscimo absoluto de risco que pode ser atribuído à exposição (182). Com o RA é possível estimar o número de casos que podem ser prevenidos se a exposição for eliminada e assim estimar a magnitude do impacto, em termos de saúde pública, imposto por esta exposição. O risco de desenvolver o desfecho (incidência) está aumentado em RA nos indivíduos expostos em comparação com os que não estão expostos. Nos estudos de coorte, costuma-se usar mais a expressão Risco Atribuível ou Diferença de Risco. Nos ensaios clínicos, usa-se mais a expressão Redução Absoluta do Risco (RAR), pois se espera que a intervenção reduza o risco. Calcula-se o RA ou a RAR pela diferença absoluta entre as incidências dos expostos e não expostos: \\[ RA = \\left|I_{expostos} - I_{não \\ expostos}\\right| \\] Utilizando a tabela de contingência \\(2 \\times 2\\) (Figura 18.10), o RA fica expresso da seguinte maneira: \\[ RA = \\left|\\frac{a}{a + b} -\\frac{c}{c + d}\\right| \\] No exemplo do Risco Relativo, o RA pode ser calculado usando a mesma tabela de contingência, repetida aqui para facilitar a leitura (Figura 18.11): Figura18.11: Taxa de ataque de gastrenterite com sorvete de baunilha - Oswego Logo, \\[ RA = \\left|\\frac{43}{43 + 11} -\\frac{3}{3 + 18}\\right| = \\left|0,796 - 0,143\\right| = 0,653 \\] O risco atribuível na exposição mede o excesso de risco associado a uma determinada categoria de exposição. Por exemplo, com base no exemplo, a incidência cumulativa de gastrenterite aguda entre os indivíduos que comeram o sorvete de baunilha é de 79,6% e para os que não ingeriram o sorvete (categoria de referência ou não exposta) foi de 14,3%. Desta forma, o risco excessivo associado à exposição 79,6 – 14,3 = 65,3%. Ou seja, assumindo uma associação causal (sem confusão ou viés), a não ocorrência da festa diminuiria o risco no grupo exposto de 79,6% para 14,3%. O RA expresso em relação à incidência nos expostos e apresentado em percentual é denominado de Risco Atribuível Proporcional (RAP) ou Fração Atribuível nos Expostos. O RAP informa qual a proporção de desfecho, expresso em percentagem, entre os expostos que poderia ter sido prevenida se a exposição fosse eliminada. É dado pela fórmula: \\[ RAP = \\left(\\frac{I_{expostos} - I_{não \\ expostos}}{I_{expostos}}\\right) \\times 100 \\] No exemplo do surto de gastrenterite aguda no jantar da igreja de Oswego, tem-se: \\[ RAP = \\left(\\frac{0,796 - 0,143}{0,796}\\right) \\times 100 = 82,06 \\% \\] Se a causalidade foi estabelecida, essa medida pode ser interpretada como a porcentagem do risco total de gastrenterite aguda que é atribuível à ingesta de sorvete de baunilha. Outra maneira de se chegar a este mesmo resultado é através do RR, usando a seguinte fórmula \\[ RAP = \\left(\\frac{I_{expostos} - I_{não \\ expostos}}{I_{expostos}}\\right) \\times 100 \\] \\[ RAP = \\left(\\frac{I_{expostos}}{I_{expostos}} - \\frac{I_{não \\ expostos }}{I_{expostos}}\\right) \\times 100 \\] \\[ RAP = \\left(1 - \\frac{1}{\\frac{I_{expostos }}{I_{não \\ expostos}}}\\right) \\times 100 \\] \\[ RAP = \\left(1 - \\frac{1}{RR}\\right) \\times 100 \\] \\[ RAP = \\left(\\frac{RR - 1}{RR}\\right) \\times 100 \\] No exemplo, o RR é igual a 5,57, logo: \\[ RAP = \\left(\\frac{5,57 - 1}{5,57}\\right) \\times 100 = 82,05\\% \\] 18.6.2 Redução Relativa do Risco Quando se avalia um tratamento ou alguma intervenção onde se supõe que haja uma redução do risco, por exemplo, uso da aspirina para reduzir infarto agudo de miocárdio, o termo Risco Atribuível é substituído por Redução do Risco Atribuível e é calculado da mesma forma visto na equação do Risco Atribuível. Neste caso, ao invés de usar o Risco Atribuível Proporcional (RAP), onde se pressupõe que a exposição é um fator de risco para a doença e o RR &gt; 1, usa-se a Redução Relativa do Risco, pois a exposição é supostamente um fator protetor e o RR &lt; 1, como se espera que ocorra nos ensaios clínicos. Esta medida análoga ao RAP é também chamada de Eficácia, definida como a proporção da incidência nos indivíduos não tratados (por exemplo, o grupo de controle) que é reduzida pela intervenção (183). O cálculo da Redução Relativa do Risco (RRR) é semelhante ao Risco Atribuível Proporcional (RAP), onde a incidência nos expostos é a incidência no grupo que recebeu a intervenção (ou taxa de eventos no grupo tratamento) e a incidência nos não expostos é incidência nos controles (ou taxa de eventos nos controles – TEC). Como se supõe que a incidência nos controles seja maior que a incidência no grupo de tratamento, a equação fica: \\[ RRR = \\left(\\frac{I_{controle} - I_{tratamento}}{I_{controle}}\\right) \\times 100 \\] Alternativamente, a RRR pode ser estimada pela equação: \\[ RRR = \\left(1 - RR\\right) \\times 100 \\] O Physicians’ Health Study (50) é um ensaio clinico randomizado controlado, duplo cego, desenhado com o objetivo de determinar se uma dose baixa de aspirina (325 mg a cada 48 horas) diminui a mortalidade cardiovascular e se o betacaroteno reduz a incidência de câncer. Participaram deste estudo 22071 indivíduos por uma média de 60,2 meses. O estudo do componente aspirina mostrou os seguintes resultados (Figura 18.12): Figura18.12: Physicians’ Health Study, componente aspirina e IAM. A incidência cumulativa de Infarto Agudo de Miocárdio (IAM) em ambos os grupos foi: \\[ Incidencia_{aspirina} = \\frac{139}{11037} = 0,0126 \\] \\[ Incidencia_{placebo} = \\frac{239}{11034} = 0,0217 \\] \\[ RR = \\frac{0,0126}{0,0217} = 0,58 \\] Logo, a RRR é igual a: \\[ RRR = \\left(1 - 0,58\\right) \\times 100 = 42\\% \\] Ou seja, houve uma redução de 42% no risco de IAM no grupo que usou aspirina e a conclusão dos autores foi que este ensaio clínico demonstrou, em relação à prevenção primária de doença cardiovascular, uma diminuição no risco de IAM. Estes cálculos podem ser realizados com a função risks() do pacote MKmisc (184). Esta função calcula o risco relativo (RR), odds ratio (OR), redução relativa do risco (RRR) e outras estatísticas epidemiológicas, como RAR, NNT. A função risks() usa como argumento: p0 \\(\\longrightarrow\\) incidência do desfecho de interesse no grupo não exposto; p1 \\(\\longrightarrow\\) incidência do desfecho de interesse no grupo exposto. Além disso, para o seu funcionamento, deve-se ter instalado o pacote BiocManager para poder instalar o pacote limma, necessário para a execução do pacote MKmisc. Veja início do capítulo em pacotes usados neste capítulo. A função risks() será usada dentro da função round() para reduzir o número de dígitos decimais: p0 &lt;- 0.0217 p1 &lt;- 0.0126 round(MKmisc::risks(p0,p1), 4) ## p0 p1 RR OR RRR ARR NNT ## 0.0217 0.0126 0.5806 0.5753 0.4194 0.0091 109.8901 18.6.3 Número Necessário para Tratar Os resultados da função risks() entrega junto o Número Necessário para Tratar (NNT) que deve ser arredondado para o número inteiro mais próximo (no caso, 110) e significa a estimativa do número de indivíduos que devem receber uma intervenção terapêutica, durante um período específico de tempo, para evitar um efeito adverso ou produzir um desfecho positivo. O NNT equivale à recíproca do RAR (Redução Absoluta do Risco ou Diferença de Risco): \\[ NNT = \\frac{1}{RAR} = \\frac{1}{I_{não \\ expostos} - I_{expostos}} \\] No exemplo do Physicians’ Health Study, o RAR igual a: \\[ RA = \\left|I_{expostos} - I_{não \\ expostos}\\right| = \\left|0,0126 - 0,0217\\right| = 0,0091 \\] \\[ NNT = \\frac{1}{0,0091} = 109,89 \\simeq 110 \\] Pode-se calcular os IC95%, calculando o NNT para os limites do RAR usando a seguinte equação (185): \\[ IC_{95\\%} \\longrightarrow RAR \\pm z_{\\left({1 - \\frac{\\alpha}{2}}\\right)} \\times EP_{RAR} \\] Onde, \\[ EP_{RAR} = \\sqrt{\\frac{p0\\left(1 - p0\\right)}{n_{1}}+\\frac{p1\\left(1 - p1\\right)}{n_{2}}} \\] Usando os dados do Physicians’ Health Study, pode-se criar um script no RStudio para os cálculos: Vetor dos dados a &lt;- 139 b &lt;- 10898 c &lt;- 239 d &lt;- 10795 dados &lt;- c (a, b, c, d) Matriz dos dados60 mat_iam &lt;- matrix (dados, byrow = TRUE, nrow = 2) tratamento &lt;- c (&quot;aspirina&quot;, &quot;placebo&quot;) desfecho &lt;- c (&quot;IAM&quot;, &quot;s/IAM&quot;) rownames (mat_iam) &lt;- tratamento colnames (mat_iam) &lt;- desfecho mat_iam ## IAM s/IAM ## aspirina 139 10898 ## placebo 239 10795 Cálculo das incidências no grupo tratamento e no grupo placebo Na matriz o que está entre colchetes [1,1] significa: linha 1 e coluna 1, ou seja, o valor 139. n1 &lt;-mat_iam [1,1] + mat_iam [1,2] n1 ## [1] 11037 p1 &lt;- mat_iam [1,1] / n1 round (p1, 4) ## [1] 0.0126 n0 &lt;- mat_iam [2,1] + mat_iam [2,2] n0 ## [1] 11034 p0 &lt;- mat_iam [2,1] / n0 round (p0, 4) ## [1] 0.0217 Os resultados da matriz de dados e o cálculo das incidências p0 (incidência no grupo placebo) e p1 (incidência no grupo de tratamento) já eram conhecidos e foram repetidos apenas para entrar na programação do cálculo do IC95%. Cálculo do erro padrão da RAR RAR &lt;- abs(p0 - p1) NNT &lt;- 1/RAR alpha &lt;- 0.05 z &lt;- qnorm (1 - (alpha/2)) round (z, 3) ## [1] 1.96 EP_RAR &lt;- sqrt((((p0*(1-p0)) / n0)) + (((p1*(1-p1)) / n1))) # Limite inferior li_RAR &lt;- RAR - (z * EP_RAR) round (li_RAR, 4) ## [1] 0.0056 # Limite superior ls_RAR &lt;- RAR + (z * EP_RAR) round (ls_RAR, 4) ## [1] 0.0125 round(print(c(li_RAR, RAR, ls_RAR), 4)) ## [1] 0.005645 0.009066 0.012488 ## [1] 0 0 0 Portando, ao Redução Absoluta do Risco foi igual a 0,0091 (IC95%: 0,0056-0,0125). A partir destes resultados, pode-se calcular o intervalo de confiança para o NNT: li_NNT &lt;- 1/ls_RAR ls_NNT &lt;- 1/li_RAR li_NNT ## [1] 80.07881 ls_NNT ## [1] 177.1497 Concluindo, o uso da aspirina no Physicians’ Health Study reduziu o risco de infarto agudo do miocárdio em 42% (RRR), ou seja, foi eficaz. Por outro lado, para ter este impacto será necessário tratar 110 (IC95%: 80-177) pacientes para que um tenha benefício. Este NNT é grande; o ideal é um NNT &lt; 10. Apesar disso, como a aspirina tem baixo custo e seus benefícios suplantam os efeitos adversos, seu uso pode estar justificado. 18.6.4 Número Necessário para Causar Dano Deve-se comparar o NNT com o Número Necessário para causar Dano (NND), em inglês, Number Needed to Harm (NNH). Deve ser interpretado como o número de pacientes tratados para que um deles apresente um efeito adverso. O NND é calculado pela recíproca do aumento absoluto do risco (ARA), equivalente a diferença de risco ou redução absoluta do risco: \\[ NND = \\frac{1}{ARA} = \\frac{1}{I_{expostos} - I_{não \\ expostos}} \\] 18.6.4.1 Exemplo No Physicians’ Health Study sobre o uso de aspirina na prevenção de IAM, foi verificado também os efeitos colaterais da aspirina, como acidentes vasculares cerebrais (AVC), Figura 18.13. Figura18.13: Physicians’ Health Study, componente aspirina e AVC. Cálculo das incidências p0 &lt;- 98/11034 round(p0, 4) ## [1] 0.0089 p1 &lt;- 119/11037 round(p1, 4) ## [1] 0.0108 Para o cálculo do NND, usa-se a função risk(), como mencionado antes: p0 &lt;- 0.0089 p1 &lt;- 0.0108 round (MKmisc::risks (p0, p1), 4) ## p0 p1 RR OR RRI ARI NNH ## 0.0089 0.0108 1.2135 1.2158 0.2135 0.0019 526.3158 Os resultados mostram que o NND61 é igual a 526. Ou seja, para evitar um IAM há necessidade de tratar 110 pacientes e a cada 526 tratados espera-se um caso de AVC, havendo um benefício bem maior quando comparado ao risco de AVC. 18.7 Análise de sobrevida A análise de sobrevida é utilizada quando se pretende investigar o tempo entre o início de um estudo e a ocorrência subsequente de um evento que modifica o estado de saúde do indivíduo. É bastante usada em estudos sobre câncer, por exemplo, analisando o tempo desde a cirurgia até a morte, o tempo desde o início do tratamento até a progressão da doença, o tempo desde a resposta até a recorrência da doença. Ela também é usada para medir a ocorrência de outros eventos como o tempo desde a infecção pelo vírus da imunodeficiência humana (HIV) até o desenvolvimento da Síndrome de Imunodeficiência Adquirida (SIDA), o tempo de hospitalização, tempo de amamentação, etc. O interesse está centrado na verificação do efeito dos fatores de risco ou de prognóstico sobre o tempo de sobrevida de um indivíduo ou de um grupo, bem como definir as probabilidades de sobrevida em diversos momentos no seguimento do grupo. Considera-se tempo de sobrevida, ou simplesmente sobrevida, o tempo a entre a entrada do indivíduo no estudo e a ocorrência do evento de interesse. Com relação aos dados relacionados ao tempo, podem ocorrer problemas. O tempo para um evento geralmente não tem distribuição normal. Além disso, nem sempre se pode esperar até que o evento ocorra em todos os pacientes e alguns pacientes abandonam o estudo mais cedo. Todos devem ser considerados e as análises de sobrevida contornam esses problemas. Em estudos de sobrevida, os indivíduos são observados até a ocorrência de um evento final que, geralmente, corresponde à morte, ou à variação de um parâmetro biológico ou outro evento que indique a modificação do estado inicial (cura, recorrência, retorno ao trabalho, etc.) O evento final é denominado de falha, por referir-se, em geral, a algo indesejável. 18.7.1 Dados Censurados Quando, em um estudo de sobrevida, os pacientes que saem do estudo ou que não vivenciam o evento são chamados de observações censuradas. Esses tempos de sobrevida censurados subestimam o verdadeiro (mas desconhecido) tempo para o evento. Quando o evento (supondo que ocorreria) está além do final do período de acompanhamento, a censura costuma ser chamada de censura à direita. A censura também pode ocorrer quando se observa a presença de um evento, mas não se sabe onde começou. Por exemplo, considere um estudo que investigue o tempo para a recorrência de um câncer após a remoção cirúrgica do tumor primário. Se os pacientes forem examinados 3 meses após a cirurgia e já tinham recorrência, então o tempo de sobrevida será censurado a esquerda, porque o tempo real (desconhecido) de recorrência ocorreu menos de 3 meses após a cirurgia. Os dados de tempo do evento também podem ser censurados em intervalos, o que significa que os indivíduos entram e saem da observação. Se considerarmos o exemplo anterior e os pacientes também forem examinados aos 6 meses, aqueles que estão livres da doença aos 3 meses e perdem o acompanhamento entre 3 e 6 meses são considerados censurados no intervalo. A maioria dos dados de sobrevivência incluem observações censuradas à direita (186). 18.7.2 Método de Kaplan-Meier O método de Kaplan-Meier (KM) é um método não paramétrico usado para estimar a probabilidade de sobrevivência a partir dos tempos de sobrevivência observados (187). A função de sobrevida é a probabilidade de sobreviver a pelo menos um determinado ponto no tempo e o gráfico desta probabilidade é a curva de sobrevida. O método de sobrevida de Kaplan-Meier pode ser usado para comparar as curvas de sobrevida de dois ou mais grupos, como comparar um grupo tratado a um grupo não tratado (placebo), ou homens comparados a mulheres. A curva de sobrevida KM, um gráfico da probabilidade de sobrevida de Kaplan-Meier em relação ao tempo, fornece um resumo útil dos dados que podem ser usados para estimar medidas como a mediana de sobrevida. 18.7.2.1 Pressupostos do método de Kaplan-Meier Os pressupostos para o uso da análise de sobrevida são as seguintes (188): os participantes devem ser independentes, ou seja, cada participante aparece apenas uma vez no grupo; os grupos devem ser independentes, ou seja, cada participante está apenas em um grupo; todos os participantes são livres de eventos quando se inscrevem no estudo; a medição do tempo até o evento deve ser precisa; o ponto inicial e o evento são claramente definidos; as perspectivas de sobrevida dos participantes permanecem constantes, ou seja, os participantes inscritos no início ou no final do estudo devem ter as mesmas perspectivas de sobrevida; a probabilidade de censura não está relacionada à probabilidade do evento. Como em todas as análises, se o número total de pacientes em qualquer grupo for pequeno, digamos menos de 30 participantes em cada grupo, os erros padrão em torno das estatísticas resumidas serão grandes e, portanto, as estimativas de sobrevida serão imprecisas. Para estudos de sobrevida, recomenda-se fazer o cálculo do tamanho amostral previamente. O R dispõe de um pacote que possibilita este cálculo, o powerSurvEpi (189). 18.7.2.2 Exemplo O arquivo dadosSobrevida.xlsx contém as informações de 60 pacientes selecionados para um ensaio clínico randomizado hipotético de dois tratamentos nos quais 32 pacientes receberam o novo tratamento e 28 pacientes receberam o tratamento padrão. Para obter o arquivo, clique aqui e salve o mesmo em seu diretório de trabalho. Destes pacientes, 33 eram mulheres e 27 homens. Durante o estudo (65 meses), um total de 21 pacientes morreram (7 mulheres e 14 homens). Carregar o conjunto de dados A partir do diretório de trabalho, carregue para um objeto que será denominado de sobrevida, usando a função read_excel() do pacote readxl e observe os dados com a função head(). sobrevida &lt;- readxl::read_excel(&quot;Arquivos/dadosSobrevida.xlsx&quot;) head (sobrevida) ## # A tibble: 6 × 5 ## id evento tempo sexo grupo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 22 0 5 fem novo ## 2 21 0 7 masc novo ## 3 19 0 8 fem novo ## 4 13 0 9 fem novo ## 5 50 1 9 masc novo ## 6 20 1 12 masc novo A Saída exibe um banco de dados com cinco variáveis: id \\(\\longrightarrow\\) Identificação do indivíduo evento \\(\\longrightarrow\\) Desfecho. 0 = censurado; 1 = morte tempo \\(\\longrightarrow\\) Sobrevida em meses sexo \\(\\longrightarrow\\) 1 = masculino; 2 = feminino grupo \\(\\longrightarrow\\) Grupo de tratamento: 1 = nova droga; 2 = padrão Construir uma tabela tratamento vs evento table (sobrevida$grupo, sobrevida$evento, dnn = c(&quot;Tratamento&quot;, &quot;Evento&quot;)) ## Evento ## Tratamento 0 1 ## novo 24 8 ## padrão 15 13 A saída mostra o número em cada grupo, o número de eventos e o número censurados. Houve menos eventos, mas mais pacientes censurados no grupo do tratamento novo. Calcular as estimativas de sobrevida de Kaplan-Meier para a construção da Curva de Sobrevida de cada tratamento Para isso, usa-se a função survfit() do pacote survival(190). Seus principais argumentos incluem: objeto de sobrevida, criado usando a função Surv() aninhada na função survfit() e o conjunto de dados contendo as variáveis. Para a construção da tabela e da curva de sobrevida, digite e execute o seguinte: tabsurv &lt;- survfit (Surv (tempo, evento) ~ grupo, data = sobrevida) summary(tabsurv) ## Call: survfit(formula = Surv(tempo, evento) ~ grupo, data = sobrevida) ## ## grupo=novo ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 9 29 1 0.966 0.0339 0.9013 1.000 ## 12 27 1 0.930 0.0479 0.8404 1.000 ## 15 26 1 0.894 0.0579 0.7874 1.000 ## 16 25 1 0.858 0.0657 0.7387 0.997 ## 32 15 1 0.801 0.0826 0.6545 0.980 ## 36 13 1 0.739 0.0965 0.5725 0.955 ## 40 11 1 0.672 0.1086 0.4897 0.923 ## 58 2 1 0.336 0.2438 0.0811 1.000 ## ## grupo=padrão ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 28 3 0.893 0.0585 0.7853 1.000 ## 2 25 1 0.857 0.0661 0.7369 0.997 ## 3 24 1 0.821 0.0724 0.6911 0.976 ## 4 23 2 0.750 0.0818 0.6056 0.929 ## 7 20 1 0.712 0.0859 0.5625 0.902 ## 17 19 1 0.675 0.0892 0.5210 0.875 ## 21 17 2 0.596 0.0947 0.4361 0.813 ## 38 9 1 0.529 0.1048 0.3592 0.780 ## 52 2 1 0.265 0.1944 0.0628 1.000 A Tabela de sobrevida é uma tabela descritiva com a coluna time, indicando o dia em que o evento ocorreu. A coluna n.risk indica o número de pacientes sob risco naquele momento. A coluna denominada n.event indica o número total de pacientes que sofreram o evento desde o início do estudo até o momento avaliado. A coluna survival indica a proporção de pacientes que sobreviveram desde o início do estudo até aquele momento. Por exemplo, a sobrevida cumulativa é de 0,801 aos 32 meses no grupo tratamento novo e de 0,529 aos 38 meses no grupo tratamento padrão. O método Kaplan-Meier produz uma única estatística resumida do tempo de sobrevida, isto é, a média ou mediana. O tempo médio de sobrevida é estimado a partir dos tempos observados e é mostrado para cada grupo na tabela de médias e medianas para o tempo de sobrevida. A sobrevida média é calculada como a soma do tempo dividido pelo número de pacientes que permanecem sem censura. Essa estatística pode ser usada para indicar o período de tempo em que um paciente pode sobreviver. O tempo mediano de sobrevida é o ponto em que metade dos pacientes experimentou o evento. Se a curva de sobrevida não cair para 0,5 (ou seja, probabilidade de sobrevida de 50%), o tempo mediano de sobrevida não poderá ser calculado. Estes dados podem ser visualizados na Saída, obtida com o comando: summary(tabsurv)$table ## records n.max n.start events rmean se(rmean) median 0.95LCL ## grupo=novo 32 32 32 8 49.92533 4.078218 58 40 ## grupo=padrão 28 28 28 13 36.62437 5.403382 52 21 ## 0.95UCL ## grupo=novo NA ## grupo=padrão NA Visualização da curva de sobrevida Pode-se visualizar a curva (Figura 18.14) de uma maneira simples, utilizando a função plot() do pacote básico do R: plot (tabsurv, col = c (&quot;steelblue&quot;, &quot;rosybrown&quot;), lwd = 2) legend (legend = c (&quot;Tratamento novo&quot;, &quot;Tratamento padrão&quot;), fill = c (&quot;steelblue&quot;, &quot;rosybrown&quot;), bty=&quot;n&quot;, cex = 1, y = 0.3, x = 5) Figura18.14: Curva de sobrevida comparando dois grupos de tratamento. Outra maneira, mais sofisticada, de produzir a curva de KM é usando a função ggsurvplot(), incluída no pacote survminer (191) que utiliza o pacote ggplot2 (Figura 18.15) Com essa função é possível mostrar: os intervalos de confiança de 95% da função de sobrevida, usando o argumento conf.int = TRUE; o número e/ou a porcentagem de indivíduos em risco por tempo, utilizando a opção risk.table. Os valores permitidos para a risk.table incluem: TRUE ou FALSE especificando se deve mostrar ou não a tabela de risco. O padrão é FALSE. absolute ou percentage: para mostrar o número absoluto e o percentual de sujeitos em risco por tempo, respectivamente. Use abs_pct para mostrar o número absoluto e a porcentagem. o nrisk_cumcensor e nrisk_cumevents . Mostra o número em risco e o número acumulado de censura e eventos, respectivamente. o valor P do teste Log-Rank comparando os grupos usando pval = TRUE. linha horizontal/vertical na sobrevida mediana usando o argumento surv.median.line. Os valores permitidos incluem um de c(“nenhum”, “hv”, “h”, “v”). Onde v = vertical, h = horizontal. ggsurvplot (tabsurv, pval = TRUE, conf.int = FALSE, risk.table = &quot;abs_pct&quot;, risk.table.col = &quot;strata&quot;, surv.median.line = &quot;hv&quot;, ggtheme = theme_bw (), legend.labs = c (&quot;Tratamento Novo&quot;, &quot;Tratamento padrão&quot;), palette = c (&quot;steelblue&quot;, &quot;tomato&quot;)) Figura18.15: Curva de sobrevida comparando dois grupos de tratamento, usando ggsurvplot(). O teste Log Rank pondera todos os pontos de tempo igualmente e é a estatística de sobrevida mais usada (192). O teste de log rank é um teste não paramétrico, que não faz suposições sobre as distribuições de sobrevivência. Os pressupostos deste teste são os mesmos do método de Kaplan-Meier. No exemplo, o valor P do teste é fornecido na Figura 18.15 e é igual a 0,083, ou seja, acima de 0,05, indicando não rejeição da \\(H_{0}\\). A hipótese nula diz que não há diferença na sobrevivência entre os dois grupos. Essencialmente, o teste de log rank compara o número observado de eventos em cada grupo com o que seria esperado se a hipótese nula fosse verdadeira (ou seja, se as curvas de sobrevivência fossem idênticas). A estatística de log rank é aproximadamente distribuída como uma estatística de teste qui-quadrado. A função survdiff(), também do pacote survival, pode ser usada para calcular o teste de log-rank comparando duas ou mais curvas de sobrevida e pode ser usado da seguinte forma: dif_sobrevida &lt;- survdiff (Surv (tempo, evento) ~ grupo, data = sobrevida) dif_sobrevida ## Call: ## survdiff(formula = Surv(tempo, evento) ~ grupo, data = sobrevida) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## grupo=novo 32 8 11.91 1.28 3.01 ## grupo=padrão 28 13 9.09 1.68 3.01 ## ## Chisq= 3 on 1 degrees of freedom, p= 0.08 A suposição de que o risco de um evento em um grupo em comparação com o outro grupo não muda ao longo do tempo é chamado de risco proporcional. Se as curvas de sobrevida se cruzam, isso sugere que os riscos não são proporcionais. Nessa situação, o teste log rank será menos poderoso e um teste alternativo deve ser considerado, como a Regressão de Cox ou Modelo de Riscos Proporcionais. Regressão de Cox ou Modelo de Riscos Proporcionais O modelo tem como objetivo a examinar simultaneamente como os fatores especificados influenciam a taxa de ocorrência de um determinado evento (por exemplo, infecção, morte) em um determinado ponto no tempo. Essa taxa é referida como hazard ratio. Geralmente, as variáveis preditoras (ou fatores) são denominadas covariáveis. O modelo de Cox é expresso pela função de risco denotada por h(t). Pode ser interpretada como o risco de morrer no tempo t e estimada da seguinte forma: \\[ h\\left(t\\right) = h_{0} \\left(t\\right) \\times e^{\\left( {b_{1}x_{1}+b_{2}x_{2}+...+b_{n}x_{n}} \\right)} \\] Onde, t é o tempo de sobrevida, indica que o risco varia com o tempo; h(t) é a função de risco (hazard) determinada por um conjunto de n covariáveis (\\(x_{1}, x_{2}, ..., x_{n}\\)); Os coeficientes (\\(b_{1}, b_{2}, ..., b_{n}\\)) medem o tamanho do efeito das covariáveis; h(0) é o risco basal, o valor do risco se todos os \\(x_{i}\\) fossem iguais a zero (\\(exp(0) = 1\\)). As quantidades exp(\\(b_{i}\\)) são chamadas de hazard ratio (HR). Uma hazard ratio acima de 1 indica uma covariável que está positivamente associada à probabilidade do evento e, portanto, negativamente associada ao tempo de sobrevida. Resumindo, HR = 1: Sem efeito HR &lt;1: Redução do risco HR&gt; 1: Aumento do risco Para calcular o modelo de Cox no R serão utilizados os mesmos dados da do arquivo dadosSobrevida.xlsx. O pacote survival tem uma função para calcular o modelo de Cox, coxph(), que usa os argumentos: formula \\(\\longrightarrow\\) é o modelo linear com um objeto de sobrevivida como variável desfecho. O objeto de sobrevida é criado usando a função Surv() como segue: Surv(tempo, evento). data \\(\\longrightarrow\\) um banco de dados contendo as variáveis. mod.cox &lt;- coxph (Surv (tempo, evento) ~ grupo, data = sobrevida) mod.cox ## Call: ## coxph(formula = Surv(tempo, evento) ~ grupo, data = sobrevida) ## ## coef exp(coef) se(coef) z p ## grupopadrão 0.7698 2.1593 0.4505 1.709 0.0875 ## ## Likelihood ratio test=3.03 on 1 df, p=0.08171 ## n= 60, number of events= 21 A função summary() fornece um relatório mais completo: summary(mod.cox) ## Call: ## coxph(formula = Surv(tempo, evento) ~ grupo, data = sobrevida) ## ## n= 60, number of events= 21 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## grupopadrão 0.7698 2.1593 0.4505 1.709 0.0875 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## grupopadrão 2.159 0.4631 0.893 5.221 ## ## Concordance= 0.637 (se = 0.054 ) ## Likelihood ratio test= 3.03 on 1 df, p=0.08 ## Wald test = 2.92 on 1 df, p=0.09 ## Score (logrank) test = 3.06 on 1 df, p=0.08 Os resultados da regressão de Cox, podem ser interpretados da seguinte forma: Significância estatística. A coluna marcada com z fornece o valor da estatística Wald. Corresponde à razão de cada coeficiente de regressão para seu erro padrão (\\(z = \\frac{coef}{EP_{coef}}\\)). A estatística Wald avalia se o coeficiente beta (\\(\\beta\\)) de uma determinada variável é estatisticamente diferente de 0. A partir da saída, pode-se concluir que não há diferença estatisticamente significativa entre os grupos (P = 0,0875). Coeficientes de regressão. A seguir deve-se observar, no modelo de Cox, o sinal dos coeficientes de regressão (coef). Um sinal positivo significa que o hazard (risco) é maior e, portanto, pior o prognóstico, para sujeitos com valores mais elevados dessa variável. No exemplo, a variável grupo é codificada como 1=novo, 2=padrão. O resumo do modelo de Cox fornece a hazard ratio (HR) para o segundo grupo em relação ao primeiro grupo, ou seja, tratamento padrão versus tratamento novo. O coeficiente beta para grupo = 0,7698 indica que os indivíduos do tratamento padrão têm maior risco de morte (taxas de sobrevivência mais baixas) do que os do grupo tratamento novo, nesses dados. Entretanto, esta diferença não é estatisticamente significativa. Hazard ratios. Os coeficientes exponenciados (exp(coef) = exp(0,7698) = 2,1593), também conhecidos como hazard ratio, fornecem o tamanho do efeito das covariáveis. Por exemplo, ser do grupo padrão aumenta o risco por um fator de 2,1593. Se esta diferença fosse significativa (P &lt; 0,05), pertencer ao grupo padrão estaria associado a um mau prognóstico. Intervalos de confiança das taxas de risco. O resultado do resumo também fornece intervalos de confiança de 95% para a razão de risco (exp(coef)), limite inferior de 95% = 0,893, limite superior de 95% = 5,221, mostrando a não significância estatística, pois cruza o 1. Significância estatística global do modelo. Finalmente, a saída fornece valores de P para três testes alternativos para significância geral do modelo: O teste de razão de verossimilhança (Likelihood ratio test), teste de Wald e a estatística logrank. Esses três métodos são equivalentes. Para um tamanho amostral grande, eles darão resultados semelhantes. Para n pequeno, eles podem diferir um pouco. O teste de razão de verossimilhança tem melhor comportamento para tamanhos de amostra pequenos, por isso é geralmente preferido. 18.8 Regressão logística binária A regressão logística (também conhecida como regressão logit ou modelo logit) foi desenvolvida como um a extensão do modelo linear pelo estatístico David Cox em 1958. Pertence a uma família, denominada Modelo Linear Generalizado (GLM) e é um modelo de regressão em que o desfecho Y é categórico. A regressão logística permite estimar a probabilidade de uma resposta categórica com base em uma ou mais variáveis preditoras (X). Possibilita informar se a presença de um preditor aumenta (ou diminui) a probabilidade de um determinado desfecho em uma porcentagem específica. No caso em que Y é binário - ou seja, assume apenas dois valores, 0 e 1, que representam desfechos como aprovação/reprovação, sim/não, vivo/morto ou saudável/doente, tem-se a regressão logística binária. Na regressão logística binária, as variáveis que afetam a probabilidade do resultado são medidas como Odds Ratio, que são chamadas de Odds Ratios ajustadas (193). Na regressão linear, os valores das variáveis desfecho são preditos a partir de uma ou mais variáveis explicativas. Na regressão logística, uma vez que o desfecho é binário, a probabilidade de o desfecho ocorrer é calculada com base nos valores das variáveis explicativas. A regressão logística é semelhante à regressão linear na medida em que uma equação de regressão pode ser usada para prever a probabilidade de ocorrência de um desfecho. No entanto, a equação de regressão logística é expressa em termos logarítmicos (ou logits) e, portanto, os coeficientes de regressão devem ser convertidos para serem interpretados. Embora as variáveis explicativas ou preditores no modelo possam ser variáveis contínuas ou categóricas, a regressão logística é mais adequada para medir os efeitos das exposições ou variáveis explicativas que são variáveis binárias. Variáveis contínuas podem ser incluídas, mas a regressão logística produzirá uma estimativa de risco para cada unidade de medida. Assim, a suposição de que o efeito de risco é linear sobre cada unidade da variável deve ser atendida e a relação não deve ser curva ou ter um ponto de corte sobre o qual o efeito ocorre. Além disso, as interações entre variáveis explicativas podem ser incluídas (193). Os casos em que a variável dependente tem mais de duas categorias de resultados podem ser analisados com regressão logística multinomial, não mostrada neste livro. 18.8.1 Pressupostos da regressão logística O método de regressão logística assume que: O desfecho é uma variável binária ou dicotômica como sim vs não, positivo vs negativo, 1 vs 0. Existe uma relação linear entre o logit do desfecho e cada variável preditora. A função logit é \\(logit\\left(P\\right) = log \\ P\\left(\\frac{P}{\\left(1 - P\\right)}\\right)\\) , onde P é a probabilidade do desfecho. Na regressão linear, é assumido que o desfecho tem uma correlação linear com os preditores. Na regressão logística, o desfecho é categórico e, portanto, essa suposição é violada. Por isso que se usa o log (ou logit) dos dados. A suposição de linearidade na regressão logística, portanto, é que existe uma correlação linear entre quaisquer preditores contínuos e o logit da variável de desfecho. Os casos devem ser independentes. Os dados dos casos não devem ser relacionados; por exemplo, não pode medir as mesmas pessoas em pontos diferentes no tempo (medidas repetidas) Não há valores influentes (valores extremos ou outliers) nos preditores contínuos Não há altas intercorrelações (ou seja, multicolinearidade) entre os preditores. Para melhorar a precisão de seu modelo, você deve certificar-se de que essas suposições sejam verdadeiras para seus dados. 18.8.2 Dados para a regressão logística Os dados foram obtidos do banco de dados PimaIndiansDiabetes2, incluído no pacote mlbench (194) que contém 768 observações sobre 9 variáveis. Este conjunto de dados é originalmente do Instituto Nacional de Diabetes e Doenças Digestivas e Renais. São dados de mulheres com 21 anos ou mais com herança indígena Pima. As variáveis traduzidas são: gesta \\(\\longrightarrow\\) Número de vezes que engravidou glicose \\(\\longrightarrow\\) Concentração de glicose plasmática após 2 horas de um teste oral de tolerância à glicose (mg%) pd \\(\\longrightarrow\\) Pressão arterial diastólica (mm Hg) tríceps \\(\\longrightarrow\\) Espessura da dobra cutânea do tríceps (mm) insulina \\(\\longrightarrow\\) Insulina sérica após 2 horas (mu U/ml) imc \\(\\longrightarrow\\) Índice de massa corporal (peso em kg/(altura em m)2) pedigree \\(\\longrightarrow\\) Função de pedigree de diabetes idade \\(\\longrightarrow\\) Idade (anos) diabetes \\(\\longrightarrow\\) 0 = neg, 1 = pos Depois de removidos os dados omissos, o banco de dados ficou como encontrado em dadosPima.xlsx. Estes dados serão utilizados para executar uma regressão logística binária para verificar se alguma dessas variáveis podem predizer a presença de diabetes em mulheres com herança Pima. Para obter arquivo, clique aqui e salve o mesmo em seu diretório de trabalho. 18.8.3 Preparação dos dados Carregar os dados, usando a função read_excel () do pacote readxl: dados &lt;- readxl::read_excel(&quot;Arquivos/dadosPima.xlsx&quot;) str(dados) ## tibble [392 × 9] (S3: tbl_df/tbl/data.frame) ## $ gesta : num [1:392] 1 0 3 2 1 5 0 1 1 3 ... ## $ glicose : num [1:392] 89 137 78 197 189 166 118 103 115 126 ... ## $ pd : num [1:392] 66 40 50 70 60 72 84 30 70 88 ... ## $ triceps : num [1:392] 23 35 32 45 23 19 47 38 30 41 ... ## $ insulina: num [1:392] 94 168 88 543 846 175 230 83 96 235 ... ## $ imc : num [1:392] 28.1 43.1 31 30.5 30.1 25.8 45.8 43.3 34.6 39.3 ... ## $ pedigree: num [1:392] 0.167 2.288 0.248 0.158 0.398 ... ## $ idade : num [1:392] 21 33 26 53 59 51 31 33 32 27 ... ## $ diabetes: num [1:392] 0 1 1 1 1 1 1 0 1 0 ... Para realizar a regressão logística, os dados devem ser inseridos como na regressão linear: organizados em colunas (uma representando cada variável). Olhando a saída da função glimpse, nota-se que as variáveis categóricas foram carregadas como &lt;dbl&gt; (numéricas). A variável diabetes foi codificada como 1 (evento ocorreu) e 0 (evento não ocorreu); neste caso, 1 representa ter diabetes e 0 representa uma ausência de diabetes. Como o R codifica os fatores na ordem 0 e 1, mantém-se assim e transforma-se a variável diabetes em fator. dados$diabetes &lt;- as.factor(dados$diabetes) As variáveis idade e gesta (número de gravidezes) são altamente assimétricas (Figura: 18.16): # Este comando coloca os gráficos em uma mesma linha, em duas colunas: par(mfrow=c(1,2)) # Histogramas hist(dados$idade, breaks = 8, main = &quot;&quot;, ylab = &quot;Frequência&quot;, xlab = &quot;Idade (anos)&quot;) hist(dados$gesta, breaks = 12, main = &quot;&quot;, ylab = &quot;Frequência&quot;, xlab = &quot;Nº de gravidezes&quot;) Figura18.16: Histogramas das variáveis idade e número de gravidezes. # Restaura as configurações basais de plotagem par(mfrow=c(1,1)) Por isso, elas serão categorizadas como fatores com a função cut(), consultar a seção que trata da construção de tabelas de frequência: dados$idadeCateg &lt;- cut (dados$idade, breaks= c (20,31,41,51,81), labels = c (&quot;20-30&quot;, &quot;31-40&quot;,&quot;41-50&quot;, &quot;&gt;50&quot;), right = FALSE, include.lowest = TRUE) dados$gestaCateg &lt;- cut (dados$gesta, breaks= c (0,6,11,17), labels = c (&quot;0-5&quot;, &quot;6-10&quot;, &quot;&gt;10&quot;), right = FALSE, include.lowest = TRUE) Após a visualização e modificações das variáveis, serão retiradas as variáveis contínuas idade e gesta, mantendo as demais variáveis:`: dados &lt;- dados %&gt;% dplyr::select(-idade, -gesta) glimpse(dados) ## Rows: 392 ## Columns: 9 ## $ glicose &lt;dbl&gt; 89, 137, 78, 197, 189, 166, 118, 103, 115, 126, 143, 125, 9… ## $ pd &lt;dbl&gt; 66, 40, 50, 70, 60, 72, 84, 30, 70, 88, 94, 70, 66, 82, 76,… ## $ triceps &lt;dbl&gt; 23, 35, 32, 45, 23, 19, 47, 38, 30, 41, 33, 26, 15, 19, 36,… ## $ insulina &lt;dbl&gt; 94, 168, 88, 543, 846, 175, 230, 83, 96, 235, 146, 115, 140… ## $ imc &lt;dbl&gt; 28.1, 43.1, 31.0, 30.5, 30.1, 25.8, 45.8, 43.3, 34.6, 39.3,… ## $ pedigree &lt;dbl&gt; 0.167, 2.288, 0.248, 0.158, 0.398, 0.587, 0.551, 0.183, 0.5… ## $ diabetes &lt;fct&gt; 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1,… ## $ idadeCateg &lt;fct&gt; 20-30, 31-40, 20-30, &gt;50, &gt;50, &gt;50, 31-40, 31-40, 31-40, 20… ## $ gestaCateg &lt;fct&gt; 0-5, 0-5, 0-5, 0-5, 0-5, 0-5, 0-5, 0-5, 0-5, 0-5, &gt;10, 6-10… 18.8.4 Criação do modelo de regressão (enter) Agora, pode-se prosseguir com a análise com uma regressão logística binária, usando a função glm() do pacote stats que pode usar vários argumentos: formula \\(\\longrightarrow\\) objeto da classe “formula”. Um preditor típico tem a forma “resposta ~ preditor” em que resposta, na regressão logística binária, é uma variável dicotômica e o preditor pode ser uma série de variáveis numéricas ou categóricas; family \\(\\longrightarrow\\) uma descrição da distribuição de erro e função de link a ser usada no modelo glm, pode ser uma string que nomeia uma função de family. O padrão é family = gaussian(). No caso da regressão logística binária family = binomial() ou family = binomial (link = ”logit”). Para outras informações, use help(glm) ou help(family); data \\(\\longrightarrow\\) banco de dados. … \\(\\longrightarrow\\) … A função glm() diz ao R para executar um modelo linear generalizado. Dentro dos parênteses, são fornecidas informações importantes sobre o modelo. À esquerda do til (~) está a variável dependente. Deve ser codificada como 0 e 1 para a função ler como binária. Após o til (~), são listadas as variáveis preditoras. Quando depois do sinal til é colocado um ponto (~.), significa a presença de todas as variáveis preditoras. Quando é colocado o asterisco (*) entre duas variáveis preditoras, isto indica que não se deseja apenas o efeito principal, mas também um termo de interação entre elas. No exemplo, não foi pedido essa análise. Finalmente, após a vírgula, especifica-se que a distribuição é binomial. A função link padrão em glm para uma variável desfecho binomial é o logit, portanto, não há necessidade de especificar no modelo. Inicialmente, será feita uma regressão logística do tipo entrada forçada (enter), método padrão de conduzir uma regressão que consiste em simplesmente colocar todos os preditores no modelo de regressão em um bloco e estimar parâmetros para cada um (195). O dataframe dados será usado no modelo com todos os preditores dentro da função. O objeto criado será denominado de mod1. mod1 &lt;- glm(diabetes ~ ., data = dados, family = binomial()) Para ver o modelo gerado, há necessidade de executar a função summary(): summary(mod1) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial(), data = dados) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.8655465 1.1691937 -7.583 3.39e-14 *** ## glicose 0.0392471 0.0059152 6.635 3.25e-11 *** ## pd -0.0045595 0.0119562 -0.381 0.70294 ## triceps 0.0147965 0.0173948 0.851 0.39497 ## insulina -0.0006838 0.0013559 -0.504 0.61402 ## imc 0.0630043 0.0273349 2.305 0.02117 * ## pedigree 1.0165275 0.4388864 2.316 0.02055 * ## idadeCateg31-40 0.8544642 0.3767035 2.268 0.02331 * ## idadeCateg41-50 1.5745534 0.5203173 3.026 0.00248 ** ## idadeCateg&gt;50 1.3840205 0.6367486 2.174 0.02974 * ## gestaCateg6-10 -0.2430053 0.4202225 -0.578 0.56308 ## gestaCateg&gt;10 0.8284550 0.7673085 1.080 0.28028 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 498.10 on 391 degrees of freedom ## Residual deviance: 337.98 on 380 degrees of freedom ## AIC: 361.98 ## ## Number of Fisher Scoring iterations: 5 Essas estatísticas resumidas ajudam a entender melhor o modelo, fornecendo-nos as seguintes informações: Distribuição dos desvios residuais; Estimativas do Intercepto e inclinação junto com o erro padrão, valor z (estatística de Wald) e valor P; Valor AIC e Desviância residual e desviância nula. Em uma regressão logística, a resposta que está sendo modelada é o log(odds) ou logit de que o desfecho é igual a 1. Os coeficientes de regressão fornecem a mudança no log(odds) no desfecho para a mudança de uma unidade na variável preditora, mantendo todas as outras variáveis preditivas constantes (196). Nas variáveis contínuas, para cada aumento de uma unidade na concentração da glicose, por exemplo, o log(odds) de ser diabético “1 = pos” (versus ser diabético “0 = neg”) aumenta em 0,039. Da mesma forma, para um aumento de unidade na pressão diastólica, a probabilidade de log(odds) de ser diabético “1 = pos” (versus ser diabético “0 = neg”) diminui em 0,0045, pois o coeficiente é negativo. Para variáveis categóricas, o desempenho de cada categoria é avaliado em relação a uma categoria de base. A categoria de base para a variável idadeCateg é 20–30 (primeira categoria que aparece quando se observa os níveis 62) e para gestaCateg é 0–5. A interpretação de tais variáveis é a seguinte: Estar na faixa etária de 31-40 anos, versus faixa etária de 20-30, muda o log(odds) de ser diabético “1 = pos” (versus ser diabético “0 = neg”) em 0,854. - * Estar na faixa de 6–10 gravidezes, versus 0–5 gravidezes, muda o log(odds) de ser diabético “pos” (versus ser diabético “neg”) em -0,24. Como o log(odds) é difícil de interpretar, é possível exponenciar o log(odds) para colocar os resultados em uma escala de odds. O R calcula as odds ratios e seus IC95, usando a função exp() e, dentro dela, a função coef() – do coeficiente b para as variáveis preditoras – e a função confint(), que fornece os intervalos de confiança. A função cbind() combinará as duas. A função round() é dispensável, foi usada apenas para que o resultado tenha 5 dígitos: round (exp(cbind(OR = coef(mod1), confint(mod1))), 5) ## OR 2.5 % 97.5 % ## (Intercept) 0.00014 0.00001 0.00127 ## glicose 1.04003 1.02846 1.05267 ## pd 0.99545 0.97246 1.01935 ## triceps 1.01491 0.98076 1.05019 ## insulina 0.99932 0.99667 1.00200 ## imc 1.06503 1.00999 1.12487 ## pedigree 2.76358 1.18886 6.64511 ## idadeCateg31-40 2.35011 1.11963 4.92734 ## idadeCateg41-50 4.82858 1.75113 13.60151 ## idadeCateg&gt;50 3.99091 1.17784 14.52935 ## gestaCateg6-10 0.78427 0.33953 1.77546 ## gestaCateg&gt;10 2.28978 0.53772 11.29542 A função exp() transformou o log(odds) em odds. A Saída mostra que a chance de ser diabético aumenta em um fator de 1,04 para um aumento de uma unidade na concentração de glicose, mantendo todas as outras variáveis constantes. Todo o IC95% encontra-se acima de 1. Isto ocorre com a triceps (prega cutânea tricipital), com o imc, pedigree (herança Pima), idade acima de 30 anos, indicando haver significância estatística, para essas variáveis, até este momento da análise. 18.8.5 Criação do modelo passo a passo (stepwise) Existem vários métodos para seleção de variáveis, aqui será usado o método passo a passo (stepwise). O procedimento de seleção é realizado automaticamente por pacotes estatísticos como o pacote MASS (197) através da função stepAIC() que utiliza o AIC (Critério de Informação de Akaike 63) para a seleção (198). Esta função usa os seguintes argumentos: object \\(\\longrightarrow\\) um objeto que representa um modelo de uma classe apropriada; direction \\(\\longrightarrow\\) o modo de pesquisa passo a passo pode ser “backward” (para trás), “forward” (para frente) ou “both” (ambos), que é o padrão . Se o argumento scope estiver faltando, o padrão para a direção é “backward”; scope \\(\\longrightarrow\\) define a gama de modelos examinados na pesquisa passo a passo … \\(\\longrightarrow\\) para maiores informações, use o help. Execute o comando abaixo para realizar a seleção do modelo, que será recebido pelo objeto mod2: mod2 &lt;- stepAIC(mod1, direction = &quot;backward&quot;) ## Start: AIC=361.98 ## diabetes ~ glicose + pd + triceps + insulina + imc + pedigree + ## idadeCateg + gestaCateg ## ## Df Deviance AIC ## - pd 1 338.13 360.13 ## - insulina 1 338.24 360.24 ## - gestaCateg 2 340.36 360.36 ## - triceps 1 338.71 360.71 ## &lt;none&gt; 337.98 361.98 ## - imc 1 343.41 365.41 ## - pedigree 1 343.61 365.61 ## - idadeCateg 3 349.12 367.12 ## - glicose 1 391.66 413.66 ## ## Step: AIC=360.13 ## diabetes ~ glicose + triceps + insulina + imc + pedigree + idadeCateg + ## gestaCateg ## ## Df Deviance AIC ## - insulina 1 338.36 358.36 ## - gestaCateg 2 340.43 358.43 ## - triceps 1 338.84 358.84 ## &lt;none&gt; 338.13 360.13 ## - imc 1 343.49 363.49 ## - pedigree 1 343.89 363.89 ## - idadeCateg 3 349.25 365.25 ## - glicose 1 391.88 411.88 ## ## Step: AIC=358.36 ## diabetes ~ glicose + triceps + imc + pedigree + idadeCateg + ## gestaCateg ## ## Df Deviance AIC ## - gestaCateg 2 340.79 356.79 ## - triceps 1 339.10 357.10 ## &lt;none&gt; 338.36 358.36 ## - imc 1 343.49 361.49 ## - pedigree 1 344.00 362.00 ## - idadeCateg 3 349.53 363.53 ## - glicose 1 406.62 424.62 ## ## Step: AIC=356.79 ## diabetes ~ glicose + triceps + imc + pedigree + idadeCateg ## ## Df Deviance AIC ## - triceps 1 341.42 355.42 ## &lt;none&gt; 340.79 356.79 ## - imc 1 346.32 360.32 ## - pedigree 1 346.82 360.82 ## - idadeCateg 3 361.77 371.77 ## - glicose 1 408.50 422.50 ## ## Step: AIC=355.42 ## diabetes ~ glicose + imc + pedigree + idadeCateg ## ## Df Deviance AIC ## &lt;none&gt; 341.42 355.42 ## - pedigree 1 347.78 359.78 ## - imc 1 355.10 367.10 ## - idadeCateg 3 363.70 371.70 ## - glicose 1 409.44 421.44 summary (mod2) ## ## Call: ## glm(formula = diabetes ~ glicose + imc + pedigree + idadeCateg, ## family = binomial(), data = dados) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.995423 1.008994 -8.915 &lt; 2e-16 *** ## glicose 0.037429 0.005086 7.360 1.84e-13 *** ## imc 0.072975 0.020377 3.581 0.000342 *** ## pedigree 1.055869 0.430381 2.453 0.014154 * ## idadeCateg31-40 0.818513 0.334366 2.448 0.014367 * ## idadeCateg41-50 1.633945 0.399650 4.088 4.34e-05 *** ## idadeCateg&gt;50 1.352995 0.528396 2.561 0.010450 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 498.10 on 391 degrees of freedom ## Residual deviance: 341.42 on 385 degrees of freedom ## AIC: 355.42 ## ## Number of Fisher Scoring iterations: 5 Depois de implementar a função stepAIC(), restam agora quatro variáveis independentes - glicose, imc, pedigree e idadeCateg. De todos os modelos possíveis, este modelo (mod2) possui o valor mínimo de AIC. Além disso, as variáveis selecionadas têm valor P &lt; 0,05. Observe o cálculo das odds ratios com o mod2 e compare com o mod1 round(exp(cbind(OR = coef(mod2), confint(mod2))), 5) ## OR 2.5 % 97.5 % ## (Intercept) 0.00012 0.00002 0.00082 ## glicose 1.03814 1.02820 1.04897 ## imc 1.07570 1.03450 1.12092 ## pedigree 2.87447 1.25945 6.80648 ## idadeCateg31-40 2.26713 1.17331 4.36901 ## idadeCateg41-50 5.12405 2.35800 11.37105 ## idadeCateg&gt;50 3.86900 1.40027 11.30915 Na Saída, observa-se que ter idade entre 41 a 50 anos tem uma chance 5,1 (IC95%: 2,4 – 11,4) maior de ser diabético comparado com a faixa etária 20-30 anos. As odds ratios ajustados da regressão logística binária fornecem uma estimativa que não é enviesada por confusão. 18.8.6 Uso do modelo para fazer predições A regressão logística fornece uma curva de probabilidade em forma de S (Figura 18.17). Pode ser utilizada a função predict(), uma função R genérica para fazer previsões a partir de modelos de ajuste. Se nenhum conjunto de dados for fornecido à função predict(), as probabilidades são calculadas a partir dos dados que foram usados para ajustar o modelo de regressão logística, no caso o mod2. A função predict() toma como argumentos o modelo de regressão e os valores da variável preditora (199). Para a função mutate() e o operador pipe, consulte help(). object \\(\\longrightarrow\\) um objeto que representa um modelo de regressão data \\(\\longrightarrow\\) banco de dados type \\(\\longrightarrow\\) “reponse” ou “terms” … \\(\\longrightarrow\\)… Esta função verifica o impacto da variação dos níveis de uma variável preditora sobre a probabilidade do desfecho. Por exemplo, verifica o impacto da concentração sérica de glicose na probabilidade de ser diabético. predito &lt;- predict(mod2, data=dados, type = &quot;response&quot;) dados %&gt;% mutate(predito = ifelse(diabetes == &quot;1&quot;, 1, 0)) %&gt;% ggplot(aes(glicose, predito)) + geom_point() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;)) + labs(title = NULL, x = &quot;Glicemia (mg%)&quot;, y = &quot;Probabilidade de diabetes&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Figura18.17: Probabilidade de diabetes de acordo com a glicemia em mulheres de herança Pima. Uma vez que os coeficientes para a glicose foram estimados para o mod2, é uma questão simples calcular a probabilidade de diabetes para qualquer concentração da glicose. Por exemplo, qual a probabilidade de uma mulher da etnia Pima ser diabética com uma glicose de 180 mg%? Em primeiro lugar constrói-se um modelo logístico novo, usando apenas a glicose como preditor: mod3 &lt;- glm(diabetes ~ glicose, family = binomial(link=&quot;logit&quot;), data = dados) summary(mod3) ## ## Call: ## glm(formula = diabetes ~ glicose, family = binomial(link = &quot;logit&quot;), ## data = dados) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.095521 0.629787 -9.679 &lt;2e-16 *** ## glicose 0.042421 0.004761 8.911 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 498.10 on 391 degrees of freedom ## Residual deviance: 386.67 on 390 degrees of freedom ## AIC: 390.67 ## ## Number of Fisher Scoring iterations: 4 A seguir, utiliza-se a glicose = c(90, 120, 150, 180), criando assim um novo dataframe, chamado diabetes. diabetes &lt;- data.frame (glicose = c(90, 120, 150, 180)) diabetes ## glicose ## 1 90 ## 2 120 ## 3 150 ## 4 180 A partir daí calcula-se as predições, colocando, no dataframe diabetes, uma variável de probabilidade de predição (pred.prob): diabetes$pred.prob = predict(mod3, newdata=diabetes, type=&quot;response&quot;) diabetes ## glicose pred.prob ## 1 90 0.09299244 ## 2 120 0.26795891 ## 3 150 0.56651015 ## 4 180 0.82350197 A Saída mostra que a probabilidade de uma mulher de herança Pima ter diabete com uma glicemia de 90 mg% é 9,2%. Esta probabilidade sobe para 82,3% quando a glicemia é de 180 mg%. Esta ferramenta permite também que se faça a comparação das probabilidades usando outros preditores. Para isso basta acrescentar ao modelo de regressão logística outras variáveis, por exemplo, IMC igual a 25. mod4 &lt;- glm(diabetes ~ glicose + imc, family = binomial(link=&quot;logit&quot;), data = dados) summary(mod4) ## ## Call: ## glm(formula = diabetes ~ glicose + imc, family = binomial(link = &quot;logit&quot;), ## data = dados) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.301460 0.927405 -8.951 &lt; 2e-16 *** ## glicose 0.040713 0.004825 8.437 &lt; 2e-16 *** ## imc 0.071794 0.019606 3.662 0.00025 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 498.10 on 391 degrees of freedom ## Residual deviance: 372.12 on 389 degrees of freedom ## AIC: 378.12 ## ## Number of Fisher Scoring iterations: 4 Acrescentar o ICM = 25 aos dados: diabetes2 &lt;- data.frame (glicose = c(90, 120, 150, 180), imc = c(25, 25, 25, 25)) diabetes2 ## glicose imc ## 1 90 25 ## 2 120 25 ## 3 150 25 ## 4 180 25 A seguir, faz-se as predições: diabetes2$pred.prob = predict(mod4, newdata=diabetes2, type=&quot;response&quot;) diabetes2 ## glicose imc pred.prob ## 1 90 25 0.05507376 ## 2 120 25 0.16506181 ## 3 150 25 0.40139839 ## 4 180 25 0.69460853 Ou seja, uma mulher com herança Pima e um ICM = 25 kg/m2 e uma glicemia de 180 mg% tem 69,4% de probabilidade de ser diabética. Qual a probabilidade de ter diabete mudando o IMC para 30? Tente responder, seguindo os passos mostrados! 18.8.7 Avaliação do modelo 18.8.7.1 Linearidade Esta suposição pode ser testada examinando se o termo de interação entre o preditor e sua log transformação é significativo (200). Portanto há necessidade de criar os termos de interação de cada uma das variáveis independentes contínuas que estão no mod2 com seu log, usando a função log(). Glicose dados$glicoseInt &lt;- log(dados$glicose)*dados$glicose IMC dados$imcInt &lt;- log(dados$imc)*dados$imc Pedigree dados$pedigreeInt &lt;- log(dados$pedigree)*dados$pedigree Essas variáveis foram adicionadas ao conjunto de dados dados. podem ser observadas com a função str(). str(dados) ## tibble [392 × 12] (S3: tbl_df/tbl/data.frame) ## $ glicose : num [1:392] 89 137 78 197 189 166 118 103 115 126 ... ## $ pd : num [1:392] 66 40 50 70 60 72 84 30 70 88 ... ## $ triceps : num [1:392] 23 35 32 45 23 19 47 38 30 41 ... ## $ insulina : num [1:392] 94 168 88 543 846 175 230 83 96 235 ... ## $ imc : num [1:392] 28.1 43.1 31 30.5 30.1 25.8 45.8 43.3 34.6 39.3 ... ## $ pedigree : num [1:392] 0.167 2.288 0.248 0.158 0.398 ... ## $ diabetes : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 2 2 2 2 2 2 1 2 1 ... ## $ idadeCateg : Factor w/ 4 levels &quot;20-30&quot;,&quot;31-40&quot;,..: 1 2 1 4 4 4 2 2 2 1 ... ## $ gestaCateg : Factor w/ 3 levels &quot;0-5&quot;,&quot;6-10&quot;,&quot;&gt;10&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ glicoseInt : num [1:392] 399 674 340 1041 991 ... ## $ imcInt : num [1:392] 93.7 162.2 106.5 104.2 102.5 ... ## $ pedigreeInt: num [1:392] -0.299 1.894 -0.346 -0.292 -0.367 ... As variáveis pd, insulina, triceps e gestaCateg serão removidas, pois mostraram-se não significativas quando se criou o mod2 no método passo a passo (stepwise):: dados &lt;- dados %&gt;% dplyr::select(-c(gestaCateg, pd, insulina, triceps)) names(dados) ## [1] &quot;glicose&quot; &quot;imc&quot; &quot;pedigree&quot; &quot;diabetes&quot; &quot;idadeCateg&quot; ## [6] &quot;glicoseInt&quot; &quot;imcInt&quot; &quot;pedigreeInt&quot; Com as variáveis restantes, será criado um quinto modelo , mod5, para examinar o comportamento da interação entre o preditor e sua log transformação: mod5 &lt;- glm(diabetes~., family = binomial, data = dados) summary(mod5) ## ## Call: ## glm(formula = diabetes ~ ., family = binomial, data = dados) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -17.80900 7.05884 -2.523 0.01164 * ## glicose 0.09919 0.23220 0.427 0.66924 ## imc 0.90065 0.57720 1.560 0.11867 ## pedigree 2.08633 0.77570 2.690 0.00715 ** ## idadeCateg31-40 0.81470 0.33792 2.411 0.01591 * ## idadeCateg41-50 1.62746 0.40570 4.011 6.04e-05 *** ## idadeCateg&gt;50 1.37610 0.54080 2.545 0.01094 * ## glicoseInt -0.01047 0.03948 -0.265 0.79091 ## imcInt -0.17971 0.12529 -1.434 0.15149 ## pedigreeInt -1.62801 0.95970 -1.696 0.08981 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 498.10 on 391 degrees of freedom ## Residual deviance: 336.65 on 382 degrees of freedom ## AIC: 356.65 ## ## Number of Fisher Scoring iterations: 5 A Saída exibe a parte que testa a suposição de linearidade dos logit. O interesse está centrado apenas nos termos de interação. Qualquer interação que seja significativa indica que o efeito principal violou o pressuposto de linearidade do logit. Todas as interações têm valores de significância (valor P) maiores que 0,05, indicando que o pressuposto de linearidade do logit foi atendido. 18.8.7.2 Multicolinearidade A regressão logística é propensa ao efeito do viés da colinearidade e é essencial testar a colinearidade, após uma análise de regressão logística (201). Os problemas de multicolinearidade consistem em incluir, no modelo, diferentes variáveis que possuem relação preditiva semelhante com o desfecho. Isso pode ser avaliado para cada preditor calculando o valor VIF (Variance Inflation Factor), usando a função vif() do pacote car (107) para o mod2 que tem mais de dois termos. car::vif(mod2) ## GVIF Df GVIF^(1/(2*Df)) ## glicose 1.050569 1 1.024973 ## imc 1.023676 1 1.011769 ## pedigree 1.026294 1 1.013062 ## idadeCateg 1.088364 3 1.014213 Qualquer variável com um valor VIF alto (acima de 5 ou 10) deve ser removida do modelo. A Saída mostra que todos os valores estão abaixo de 10. Isso leva a um modelo mais simples sem comprometer a precisão do modelo, o que é bom. 18.8.7.3 Estatística \\(R^2\\) R ao quadrado é uma métrica útil para regressão linear simples e múltipla, mas não tem o mesmo significado na regressão logística. Os estatísticos descobriram uma variedade de análogos do R ao quadrado para regressão logística que eles referem, coletivamente, como pseudo R ao quadrado. Não têm a mesma interpretação, na medida em que não são simplesmente a proporção da variância explicada pelo modelo. Infelizmente, existem muitas maneiras diferentes de calcular um \\(R^2\\) para regressão logística e nenhum consenso sobre qual é a melhor. Os dois métodos mais frequentemente relatados em softwares estatísticos são um proposto por McFadden (1974) e outro por Cox-Snell (1989). No entanto, também é bastante relatado o de Nagelkerke. Os valores mais altos indicam um melhor ajuste do modelo. O \\(R^2\\) de McFadden é uma versão, baseada no log-likelihood para o modelo somente com o intercepto e o modelo estimado completo. O \\(R^2\\) de Cox e Snell é baseado no log-likelihood para o modelo em comparação com o log-likelihood para um modelo basal. No entanto, com resultados categóricos, tem um valor máximo teórico inferior a 1, mesmo para um modelo “perfeito”. O \\(R^2\\) de Nagelkerke é uma versão ajustada do \\(R^2\\) de Cox e Snell que ajusta a escala da estatística para cobrir todo o intervalo de 0 a 1. Em seguida, serão calculados os vários valores de \\(R^2\\), usando a função PseudoR2() do pacote DescTools: PseudoR2(mod2, which =c(&quot;Nagelkerke&quot;, &quot;McFadden&quot;, &quot;CoxSnell&quot;)) ## Nagelkerke McFadden CoxSnell ## 0.4580199 0.3145606 0.3294780 Como se verifica, na Saída, todos os valores de \\(R^2\\) diferem ligeiramente, mas podem ser usados como medidas de tamanho de efeito para o modelo. Então, basicamente, o pseudo R quadrado pode ser interpretado como \\(R^2\\), mas não se espera que seja tão grande. Uma regra prática, bastante útil é que o pseudo R quadrado de McFadden variando de 0,2 a 0,4 indica um ajuste muito bom do modelo (202). 18.8.7.4 Pesquisa de valores atípicos e pontos de alavancagem Um outlier é uma observação que não é bem prevista pelo modelo de regressão ajustado (ou seja, tem um grande resíduo positivo ou negativo). Uma observação com um alto valor de alavancagem possui uma combinação incomum de valores preditores. Ou seja, é um outlier no espaço do preditor. O valor da variável dependente não é usado para calcular a alavancagem de uma observação. Uma observação influente é uma observação que tem um impacto desproporcional na determinação dos parâmetros do modelo. As observações influentes são identificadas usando uma estatística chamada distância de Cook ou D de Cook. A identificação dos outliers é feita essencialmente através dos resíduos padronizados, com a função rstandard(): residuos_p &lt;- rstandard(mod2) summary(residuos_p) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.96350 -0.63838 -0.36367 -0.08147 0.63008 2.68035 Em uma amostra normalmente distribuída, ao redor de 95% dos valores estão entre –1,96 e +1,96, 99% deve estar entre –2,58 e +2,58 e quase todos (99,9%) devem situar-se entre –3,09 e +3,09. Portanto, resíduos padronizados com um valor absoluto maior que 3 são motivo de preocupação porque em uma amostra média é improvável que aconteça um valor tão alto por acaso (203). Na Saída, observa-se que os valores estão dentro de –3 e +3. Essa avaliação pode ser acompanhada de um gráfico diagnóstico (veja seção da regressão linear para maiores detalhes), usando a função plot() para o modelo mod2. plot (mod2, which = 5) Figura18.18: Gráfico diagnóstico dos resíduos e pontos de alavancagem. São produzidos vários gráficos com a função plot(), mas o foco é o gráfico 5 (Figura 18.18) que confirma os achados de não existirem valores atípicos que comprometam o ajuste do modelo. Para a análise dos pontos influentes, pode-se verificar os pontos de alavancagem (leverage) com a função hatvalues() do pacote stats, incluído no R base. hat &lt;- hatvalues(mod2) summary (hat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.001847 0.005862 0.013332 0.017857 0.023846 0.090653 Os valores de alavancagem podem estar entre 0 (indicando que o caso não tem qualquer influência) e 1 (indicando que o caso tem grande influência). Se nenhum caso exercer influência indevida sobre o modelo, se espera que todos os valores de alavancagem estivessem próximos do valor médio. Alguns autores (204), recomendam investigar casos com valores superiores a duas vezes a média (2 x 0,0179 = 0,0358) como ponto de corte para identificar casos com influência indevida. Alguns valores estão acima de duas vezes a média. Entretanto, o maior valor está bem longe do valor igual a 1. É interessante fazer a análise, observando junto a distância de Cook para ver se um ponto é um outlier significativo. cook &lt;- cooks.distance(mod2) summary (cook) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.220e-06 8.955e-05 5.703e-04 2.868e-03 2.908e-03 1.121e-01 Se a distância de Cook é &lt; 1, não há necessidade real de excluir esse ponto, uma vez que não tem um grande efeito na análise de regressão (205). Na Figura 18.18, verifica-se que os casos mais distantes não alcançam a distância de Cook. 18.8.7.5 Matriz de confusão É uma representação tabular de valores observados versus valores previstos. Ajuda a quantificar precisão (ou acurácia) do modelo. Agora será realizada uma comparação dos valores observados de “diabetes” com os valores previstos. Inicialmente, será criada uma variável correspondente aos valores previstos (mod2$fitted.values) classificando como “pos” se o valor ajustado exceder 0,5, caso contrário, “neg”. dados$predito &lt;- ifelse(mod2$fitted.values &gt;0.5,&quot;pos&quot;,&quot;neg&quot;) Pode-se avaliar o desempenho do modelo (acurácia) com a comparação tabular entre os valores observados e os previstos: tabDiabetes &lt;- table(dados$diabetes, dados$predito) rownames(tabDiabetes) &lt;- c(&quot;Obs.neg&quot;, &quot;Obs.pos&quot;) colnames(tabDiabetes) &lt;- c(&quot;Pred.neg&quot;, &quot;Pred.pos&quot;) tabDiabetes ## ## Pred.neg Pred.pos ## Obs.neg 237 25 ## Obs.pos 48 82 A acurácia é obtida pela soma dos valores das caselas na diagonal dividido pelo total. acuracia &lt;- sum(diag(tabDiabetes))/sum(tabDiabetes) acuracia ## [1] 0.8137755 De acordo com a matriz de confusão, a acurácia do modelo é de 81,4%. 18.8.7.6 Avaliação do modelo com a Curva ROC A curva ROC permite explicar o desempenho do modelo avaliando a sensibilidade versus especificidade. Para se obter a curva ROC será usada a função roc() do pacote pROC (veja Estatísticas Diagnósticas). Os comandos abaixo exibem em seus resultados (Figura 18.19) a curva ROC e a área sob a curva (AUC) e seus IC95%. Quanto maior a área sob a curva melhor é o poder de predição do modelo (206). roc (dados$diabetes, mod2$fitted.values, plot=TRUE, legacy.axes=FALSE, print.auc=TRUE, print.auc.y = 0.2, ci = TRUE, ylab=&quot;Sensibilidade&quot;, xlab=&quot;1 - Especificdade&quot;, col=&quot;steelblue&quot;, lwd=2) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases Figura18.19: Curva ROC do modelo de regressão. ## ## Call: ## roc.default(response = dados$diabetes, predictor = mod2$fitted.values, ci = TRUE, plot = TRUE, legacy.axes = FALSE, print.auc = TRUE, print.auc.y = 0.2, ylab = &quot;Sensibilidade&quot;, xlab = &quot;1 - Especificdade&quot;, col = &quot;steelblue&quot;, lwd = 2) ## ## Data: mod2$fitted.values in 262 controls (dados$diabetes 0) &lt; 130 cases (dados$diabetes 1). ## Area under the curve: 0.8631 ## 95% CI: 0.8254-0.9007 (DeLong) A Figura 18.19 exibe uma a curva ROC do mod2 onde se observa que AUC = 0,863. Isto significa que a performance do modelo como preditor é muito boa (ver Tabela 18.1). Existem duas maneiras de descrever uma estimativa de odds: ou como um número isolado, por exemplo, 0,25, subentendendo que expressa uma razão, 0,25: 1,0, ou de forma clara como uma razão 1:4. Ou seja, para cada indivíduo com o fator existem quatro sem o fator. Tradicional e comumente usados no mundo das apostas em corridas de cavalos.↩︎ Foi mantido o nome das variáveis em inglês, pois no banco de dados oswego elas estão nessa língua.↩︎ em inglês, built-in bias↩︎ Observem que todo o intervalo de confiança de 95% encontra-se abaixo de 1, indicando que existe significância estatística.↩︎ Aproveite para revisar como construir matriz↩︎ Em inglês, NNH (number needed to harm).↩︎ Se houver uma justificativa, esta referência pode ser modificada, usando a função relevel() do pacote stats. Por exemplo, para mudar a referência da variável idadeCateg para 31-40, deve-se executar: dados$idadeCateg &lt;- relevel (dados$idadeCateg, ref = \"31-40\").↩︎ https://www.scribbr.com/statistics/akaike-information-criterion/↩︎ "],["referências.html", "Referências", " Referências "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
